[["index.html", "Methods for Network Analysis About this Course", " Methods for Network Analysis Mark Hoffman Summer 2021 About this Course This 4-5 credit hour seminar is intended as a theoretical and methodological introduction to social network analysis. Though network analysis is an interdisciplinary endeavor, its roots can be found in classical anthropology and sociology. Network analysis focuses on patterns of relations between actors. Both relations and actors can be defined in many ways, depending on the substantive area of inquiry. For example, network analysis has been used to study the structure of affective links between persons, flows of commodities between organizations, shared members between social movement organizations, and shared needles between drug users. What is common across these domains is an emphasis on the structure of relations, which serves to link micro- and macro-level processes. Methods, in traditional social science conception, are largely technical affairs and distinct from theory. But in the history of social network analysis, the distinction between theory and method was never strong. Network methods operationalize core theoretical constructs and in developing them, scholars produce theoretical statements of their own, or amend those of others. As such, the development of network methods – influenced by linear algebra, statistics, and graph theory – has played a key role in the conceptual development of the field of social network analysis. This class could just as fittingly be called Social Network Theory. The class is structured with the unity of theory and method in mind. Each week covers a core set of interrelated ideas from the history of social network analysis. On Mondays, we discuss seminal papers which introduce, evaluate, and debate those core ideas. Wednesdays’s class entails a technical lab, where we learn how to apply the ideas discussed on Monday to a network of your choosing in R (a statistical programming language). By the end of the course, you should be able to read and understand the basic mathematics underlying network analysis as well as implement network methods in R, but no prior knowledge of these skills is needed. "],["reading-list.html", "1 Reading List", " 1 Reading List 1.0.1 Week 1 (beg. September 20th): Introductions and Introduction to R Required for Wednesday: Borgatti, S. P., Mehra, A., Brass, D. J., &amp; Labianca, G. (2009). Network analysis in the social sciences. science, 323(5916), 892-895. Bott, E. (2017). Urban families: conjugal roles and social networks. In Man in Adaptation(pp. 76-104). Routledge. Sewell, William Jr. (1992.) A Theory of Structure: Duality, Agency, and Transformation. American Journal of Sociology 98, 1:1-29. Hinde, R.A. (1976.) Interactions, Relationships and Social Structure. Man 11, 1: 1-17. Before class, install R and RStudio, and work through this introductory tutorial on Datacamp: https://campus.datacamp.com/courses/free-introduction-to-r Optional: Wasserman and Faust, Chapter 1 1.0.2 Week 2 (beg. September 27th): Types of networks For Monday: Local networks: McPherson, Miller, Lynn Smith-Lovin, and Matthew E. Brashears. 2006. “Social isolation in America: Changes in Core Discussion Networks over Two Decades (Links to an external site.).” American Sociological Review 71:353-375. Salganik, Matthew. and Douglas Heckathorn (2004). Sampling and estimation in hidden populations using respondent-driven sampling. Sociological Methodology 34, 193–239. One-mode networks: A. Christakis and J. H. Fowler (2007) “The Spread of Obesity in a Large Network” New England Journal of Medicine 357:370-379. Beckfield, Jason. 2010. “The Social Structure of the World Polity” American Journal of Sociology. 115:1018-1068. Two-mode networks (and beyond!): Breiger, R. L. 1974. “The Duality of Persons and Groups.” Social Forces 53:181- 90. Fararo, T. J., &amp; Doreian, P. (1984). Tripartite structural analysis: Generalizing the Breiger-Wilson formalism. Social Networks, 6(2), 141-175. Required for Wednesday: Complete and bring to class lab assignment from week 1 Optional: Wasserman and Faust, Chapters 2 &amp; 4 (skim to get the gist of the jargon and notation) Further Readings: Granovetter, Mark. (1976). Network Sampling: Some First Steps. American Journal of Sociology 81(6), 1287–1303. Moody, James. 2004. “The Structure of a Social Science Collaboration Network” American Sociological Review 69:213-264. Abbott, Andrew and E. Barman. 1997. “Sequence Comparison via Alignment and Gibbs Sampling.” Sociological Methodology 27:47-87. Blau, Peter M. and Joseph E. Schwartz. 1997. Crosscutting Social Circles: Testing A Macrostructural Theory of Intergroup Relations. New Brunswick, N.J: Transaction. Fischer, Claude S. 2009. “Comment: The 2004 GSS Finding of Shrunken Social Networks: An Artifact? (Links to an external site.).” American Sociological Review 74:657-669. McPherson, Miller, Lynn Smith-Lovin, and Matthew E. Brashears. 2009. “Reply: Models and Marginals: Using Survey Evidence to Study Social Networks.” American Sociological Review 74:670-681. Lee, Byungkyu, and Peter Bearman. 2017. “Important Matters in Political Context.” Sociological Science 4: 1–30. Anthony, Kenneth Sanchagrin 2013. “Social Isolation in America: An Artifact” American Sociological Review 2013 78:339-360 Small, Mario Luis (2013). Weak ties and the core discussion network: Why people regularly discuss important matters with unimportant alters. Social networks, 35(3), 470-483. Frank, Kenneth, et. al “The Social Dynamics of Mathematics Course taking in High School” American Journal of Sociology. McFarland, D. A., &amp; Thomas, R. J. (2006). Bowling young: How youth voluntary associations influence adult political participation. American sociological review, 71(3), 401-425. Chu, Johan SG, and Gerald F. Davis. “Who killed the inner circle? The decline of the American corporate interlock network.” American Journal of Sociology3 (2016): 714-754. 1.0.3 Week 3 (beg. October 4th): Triads, Balance and Hierarchy Required for Monday: Simmel, George. 1950. “The Triad” in Kurt Wolf (ed.), The Sociology of Georg Simmel, Free Press, 145‐169. Gould, Roger. 2002. “The Origins of Status Hierarchies.” American Journal of Sociology 107: 1143:1178. Cartwright, D., &amp; Harary, F. (1956). Structural balance: a generalization of Heider’s theory. Psychological review, 63(5), 277. Chase, I. D. (1982). Dynamics of Hierarchy Formation: the Sequential Development of Dominance Relationships. Behaviour 80(3-4), 218–239. Johnson, Eugene. 1985. “Network Macrostructure Models for the Davis-Leinhardt set of empirical sociomatricies.” Social Networks Required for Wednesday: Complete and bring to class lab assignment from week 2 Optional: Wasserman and Faust, Chapters 6 &amp; 14 Further Readings: Moody, James and Douglas R. White. 2003. “Social Cohesion and Embeddedness.” American Sociological Review 68:103-127. Moody, James (1998). Matrix methods for calculating the triad census. Social Networks, 20(4), 291-299. 1.0.4 Week 4 (beg. October 11th): Centrality, Power, and Inequality Required for Monday: Bonacich, Phillip. 1987. “Power and Centrality: A Family of Measures (Links to an external site.)” American Journal of Sociology 92:1170-118. Feld, Scott. 1991. “Why Your Friends Have More Friends Than You Do.” American Journal of Sociology 96:1464-77. Zerubavel, Noam et al. (2015.) Neural Mechanisms Tracking Popularity in Real-World Social Networks. Proceedings of the National Academy of Sciences. Grossman, G., &amp; Baldassarri, D. (2012). The impact of elections on cooperation: Evidence from a lab‐in‐the‐field experiment in Uganda. American journal of political science, 56(4), 964-985. DiMaggio, Paul, &amp; Garip, Feliz. (2012). Network effects and social inequality. Annual review of sociology, 38, 93-118. Required for Wednesday: Submit project research question and names of group members, if any (up to 3) Further Readings: Wasserman and Faust, Chapter 5 Zerubavel, N., et al. (2018). Neural precursors of future liking and affective reciprocity. Proceedings of the National Academy of Sciences, 115(17), 4375-4380. Friedkin, N. E. 1991. “Theoretical Foundations for Centrality Measures.” American Journal of Sociolgy 96:1478-504. Liljeros, F., Edling, C. R., Amaral, L. A. N., Stanley, H. E., &amp; Åberg, Y. (2001). The web of human sexual contacts. Nature, 411(6840), 907.. Albert, Réka, Hawoong Jeong, and Albert-László Barabási. 2000. “Error and attack tolerance of complex networks.” Nature 6794: 378. Freeman, L. C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40:35-41. Albert-László Barabási and Réka Albert. “Emergence of scaling in random networks.”Science, 286(5439), 509-512. Cook, K. S., Emerson, R. M., Gillmore, M. R., &amp; Yamagishi, T. (1983). The distribution of power in exchange networks: Theory and experimental results. American journal of sociology, 89(2), 275-305. 1.0.5 Week 5 (beg. October 18th): Connectivity and the Small World Problem Required for Monday: Travers, J. and S. Milgram. “An experimental study of the small world Problem” Sociometry 32:425-443 Watts, Duncan J. (1999) “Networks, Dynamics, and the Small-World Phenomenon” American Journal of Sociology. v. 105:493-527. Granovetter, Mark S. 1973. “The Strength of Weak Ties.” American Journal of Sociology 78:1360-1380. Lee, Nancy H. (1969).The Search for an Abortionist (Links to an external site.): Preface, Chapter 1. (Available on Canvas). Burt, R. S. (2004). Structural holes and good ideas. American journal of sociology, 110(2), 349-399. Park, Patrick, Joshua Blumenstock, and Michael Macy. 2018. “The Strength of Long-Range Ties in Population-Scale Social Networks.” Science 362(6421):1410-1413. Required for Wednesday: Complete and bring to class lab assignment from week 4 Further Readings: Schnettler, S. 2009. “A structured overview of 50 years of small-world research” Social Networks Bailey, M., Cao, R., Kuchler, T., Stroebel, J., &amp; Wong, A. (2018). Social connectedness: measurement, determinants, and effects. Journal of Economic Perspectives, 32(3), 259-80. Uzzi, Brian and Jarrett Spiro. 2005. “Collaboration and Creativity: The Small World Problem.” American Journal of Sociology 111:2, 447-504 Dodds et al. (2003). “An Experimental Study of Search in Social Networks” Granovetter (2003). “Ignorance, Knowledge, and Outcomes in a Small World” Granovetter, Mark.Getting a job: A study of contacts and careers. University of Chicago press, 2018. Stovel, K., &amp; Shaw, L. (2012). Brokerage. Annual Review of Sociology, 38, 139-158. 1.0.6 Week 6 (beg. October 25th): Groups, Communities, and Homophily Required for Monday: Wimmer, Andreas and Kevin Lewis. 2010. “Beyond and Below Racial Homophily: ERG Models of a Friendship Network Documented on Facebook” American Journal of Sociology 116:583-642 Goodreau, S. M., Kitts, J. A., &amp; Morris, M. (2009). Birds of a feather, or friend of a friend? Using exponential random graph models to investigate adolescent social networks.Demography, 46(1), 103-125. McPherson, Miller, Lynn Smith-Lovin, and James M. Cook. 2001. ‘‘Birds of a Feather: Homophily in Social Networks.’’ Annual Review of Sociology 27:415-44 McFarland, D. A., Moody, J., Diehl, D., Smith, J. A., &amp; Thomas, R. J. (2014). Network ecology and adolescent social structure. American sociological review, 79(6), 1088-1121. Shwed, Uri and Peter Bearman. “The Temporal Structure of Scientific Consensus Formation”. American Sociological Review. Required for Wednesday: Newman, Mark E. J. 2006. “Modularity and Community Structure in Networks.” Proceedings of the National Academy of Sciences 103 (23): 8577–8582. Robins, G., Pattison, P., Kalish, Y., &amp; Lusher, D. (2007). An introduction to exponential random graph (p*) models for social networks.Social networks, 29(2), 173-191. Complete and bring to class lab assignment from week 5 Optional: Wasserman and Faust, Chapter 7 (skim) Further Readings: Moody, James. 2001. ``Race, School Integration, and Friendship Segregation in America.\" American Journal of Sociology 107(3) 679:716 Centola, Damon. 2011. “An experimental study of homophily in the adoption of health behavior.”Science 606: 1269-1272. Kandel, Denise B. 1978. “Homophily, selection, and socialization in adolescent friendships.”American journal of Sociology 2: 427-436. Rytina, Steve, and David L. Morgan 1982. ``The Arithmetic of Social Relations: The Interplay of Category and Network\" American Journal of Sociology 88(1): 88‐113. Boutyline, A., &amp; Willer, R. (2017). The social structure of political echo chambers: Variation in ideological homophily in online networks. Political Psychology, 38(3), 551-569. 1.0.7 Week 7 (beg. November 1st): Categories and Positions Required for Monday: Nadel, A Theory of Social Structure Chapter 4. White, H., S. Boorman, and R. Breiger. 1976. “Social Structure from multiple networks. I. Blockmodels of roles and positions.” American Journal of Sociology 81:730-779. Padgett, J. F. and C. K. Ansell. 1993. “Robust Action and the Rise of the Medici, 1400-1434.” American Journal of Sociology 98:1259-319. Bearman, P. 1997. “Generalized Exchange.” American Journal of Sociology 102:1383-1415. Grover, A., &amp; Leskovec, J. 2016. “node2vec: Scalable feature learning for networks.” InProceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. 855-864. Required for Wednesday: Complete and bring to class lab assignment from week 6 Optional: Wasserman and Faust. Chapters 9 and 10. Further Readings: Munson, J. and M. Macri (2009). Sociopolitical network interactions: A case study of the Classic Maya. Journal of Anthropological Archaeology 28(4), 424–438. Brieger, Ronald L. 1976. Career Attributes and Network Structure: A Blockmodel Study of a Biomedical Research Specialty American Sociological Review, Vol. 41: 117-135. Doreian, Patrick, Vladimir Batagelj, and Anuška Ferligoj. 2004. “Generalized blockmodeling of two-mode network data.”Social networks 26: 29-53. Hillman, Henning. 2007. “Mediation in Multiple Networks: Elite Mobilization before the English Civil War” American Sociological Review. Faust, Katherine. 1988. “Comparison of Methods for Positional Analysis: Structural and General Equivalences.” Social Networks 10: 313-341. 1.0.8 Week 8 (beg. November 8th): Networks from Culture and Culture from Networks Required for Monday: De Saussure, F. (1959). Course in general linguistics. Columbia University Press. Pages 1-37. https://monoskop.org/images/0/0b/Saussure_Ferdinand_de_Course_in_General_Linguistics_1959.pdf Lee, Monica, &amp; Martin, J. L. 2018. “Doorway to the Dharma of Duality.” Poetics, 68, 18-30. Mohr, John W. 1998. “Measuring Meaning Structures.” Annual Review of Sociology 24: 345-370. Latour, B. (1992). 10 ‘‘Where Are the Missing Masses? The Sociology of a Few Mundane Artifacts’’. Hoffman, M. A. (2019). The Materiality of Ideology: Cultural Consumption and Political Thought after the American Revolution. American Journal of Sociology, 125(1), 1-62. Goldberg, Amir. 2011. “Mapping shared understandings using relational class analysis: The case of the cultural omnivore reexamined.”American Journal of Sociology 5: 1397-1436. Required for Wednesday: Complete and bring to class lab assignment from week 7 Optional: Wasserman and Faust, Chapter 8 (skim) Further Readings: Schultz, J., &amp; Breiger, R. L. (2010). The strength of weak culture. Poetics, 38(6), 610-624. Mische, Ann, and Philippa Pattison. 2000. “Composing a civic arena: Publics, projects, and social settings.” Poetics2-3: 163-194. DiMaggio, Paul, et al. 2018. “Culture out of attitudes: Relationality, population heterogeneity and attitudes toward science and religion in the US.”Poetics 68 (2018): 31-51. Lizardo, O. (2006). How cultural tastes shape personal networks. American sociological review, 71(5), 778-807. Boutyline, Andrei, and Stephen Vaisey. “Belief network analysis: A relational approach to understanding the structure of attitudes.”American Journal of Sociology 5 (2017): 1371-1447. Baldassarri, D., &amp; Goldberg, A. (2014). Neither ideologues nor agnostics: Alternative voters’ belief system in an age of partisan politics. American Journal of Sociology, 120(1), 45-95. De Vaan, M., Stark, D., &amp; Vedres, B. (2015). Game changer: The topology of creativity. American Journal of Sociology, 120(4), 1144-1194. Hoffman, M. A., Cointet, J. P., Brandt, P., Key, N., &amp; Bearman, P. (2018). The (Protestant) Bible, the (printed) sermon, and the word (s): The semantic structure of the Conformist and Dissenting Bible, 1660–1780. Poetics, 68, 89-103. Roth, C., &amp; Cointet, J. P. (2010). Social and semantic coevolution in knowledge networks. Social Networks, 32(1), 16-29. Rule, A., Cointet, J. P., &amp; Bearman, P. S. (2015). Lexical shifts, substantive changes, and continuity in State of the Union discourse, 1790–2014. Proceedings of the National Academy of Sciences, 112(35), 10837-10844. Martin, J. L. (2000). What do animals do all day?: The division of labor, class bodies, and totemic thinking in the popular imagination. Poetics, 27(2-3), 195-231. Erikson, E. (2013). Formalist and relationalist theory in social network analysis. Sociological Theory, 31(3), 219-242. Martin, J. L. (2002).“Power, Authority, and the Constraint of Belief Systems.” American Journal of Sociology 107: 861-904. Bail, C. A. (2016). Combining natural language processing and network analysis to examine how advocacy organizations stimulate conversation on social media. Proceedings of the National Academy of Sciences, 113(42), 11823-11828. 1.0.9 Week 9 (beg. November 15th): Dynamics and Diffusion Required for Monday: Granovetter, M. (1978). Threshold models of collective behavior. American journal of sociology, 83(6), 1420-1443. Morris, M., &amp; Kretzschmar, M. (1997). Concurrent partnerships and the spread of HIV. Aids, 11(5), 641-648. Centola, D., &amp; Macy, M. (2007). Complex contagions and the weakness of long ties. American journal of Sociology, 113(3), 702-734. Buskens, V &amp; van de Rijt, Arnout “Dynamics of Networks if Everyone Strives for Structural Holes” American Journal of Sociology Paluck, Elizabeth Levy, Hana Shepherd, and Peter M. Aronow. “Changing climates of conflict: A social network experiment in 56 schools.” Proceedings of the National Academy of Sciences3 (2016): 566-571. Goldberg, A., &amp; Stein, S. K. (2018). Beyond social contagion: Associative diffusion and the emergence of cultural variation. American Sociological Review, 83(5), 897-932. Required for Wednesday: Snijders, T.A.B. , van de Bunt, G.Gb, Steglich, C.E.G. 2010.”Introduction to stochastic actor-based models for network dynamics” Social Networks. 32: 44-60 Moody, James, Daniel A. McFarland and Skye Bender-DeMoll. 2005. “Dynamic Network Visualization: Methods for Meaning with Longitudinal Network Movies.” American Journal of Sociology 110:1206-1241 Further Readings: Coleman, James, Elihu Katz, and Herbert Menzel. 1957. “The diffusion of an innovation among physicians.” Sociometry 20:253-270. Banerjee, Abhijit, Arun G. Chandrasekhar, Esther Duflo, and Matthew O. Jackson. “The diffusion of microfinance.” Science 341, no. 6144 (2013): 1236498. Burt, Ronald S. 1987. “Social Contagion and Innovation: Cohesion Versus Structural Equivalence.” American Journal of Sociology 92: 1287-1335. Sotoudeh, Ramina, Kathleen Mullan Harris, and Dalton Conley. “Effects of the peer metagenomic environment on smoking behavior.”Proceedings of the National Academy of Sciences 33 (2019): 16302-16307. Stark, David and Vedres, Balázs, 2006. “Social Times of Network Spaces: Network Sequences and Foreign Investment in Hungary.” American Journal of Sociology 111(5):1367-1411. Wang, Dan J.; Sarah A. Soule. 2012. “Social Movement Organizational Collaboration: Networks of Learning and the Diffusion of Protest Tactics, 1960–1995” American Journal of Sociology. 117:1674-1722 DellaPosta, D., Shi, Y., &amp; Macy, M. (2015). Why do liberals drink lattes? American Journal of Sociology, 120(5), 1473-1511. Centola, Damon. 2010. “The Spread of Behavior in an Online Social Network Experiment” Science 2010: 1194-1197. 1.0.10 Week 9 and 3/4: Thanksgiving! 1.0.11 Week 10 (beg. November 29th): Presentations of Research 1.0.12 December 11th: Final papers are due. "],["installing-r-and-rstudio.html", "2 Installing R and RStudio 2.1 Downloading and Installing R 2.2 Downloading and Installing RStudio", " 2 Installing R and RStudio In this section, we will learn how to install R and RStudio. Both are freely available online. 2.1 Downloading and Installing R Description R is an open source programming language designed for statistical computing and visualization. Scientists and data analysts worldwide use it for purposes ranging from regression analysis, to natural language processing, to biological simulation, to social network analysis - the topic of this class. Being open source, users from around the world add new functions to its repositories on a daily basis. This means that the possible tools you can use and analyses you can perform with R are expanding constantly, making it an increasingly powerful environment for statistical analysis. We will show you just a glimpse of this power, but hopefully we can provide enough of a basis for you to go out on your own and learn more. Steps Navigate to https://www.r-project.org/ Click on the blue, bolded “download R” in the first paragraph. Choose a mirror (in other words, a website hosting current and past R distributions) located somewhere near to you. I am based in New York so I chose one based out of Carnegie Mellon in Pennsylvania. Download R by clicking on one of the “Download R for” links. Choose the link that accords with your operating system. I am using Mac, so I clicked: “Download R for (Mac) OS X” Next click the first link underneath the “Files” heading. This should begin the download. Follow the instructions on the installer that begins when you click on the downloaded file. Once you are finished, R should be installed on your system. 2.2 Downloading and Installing RStudio Description Next we need to install RStudio. RStudio is a user interface for R, which greatly improves the experience of working in R. As stated on its website, some of its features include: Customizable workbench with all of the tools required to work with R in one place (console, source, plots, workspace, help, history, etc.). Syntax highlighting editor with code completion. Execute code directly from the source editor (line, selection, or file). Full support for authoring Sweave and TeX documents. Runs on all major platforms (Windows, Mac, and Linux) and can also be run as a server, enabling multiple users to access the RStudio IDE using a web browser. Steps To download RStudio, navigate to https://www.rstudio.com/ and click the download RStudio button: Scroll down and click on the green “Download” button in the RStudio Desktop column. RStudio is free! This should cause your browser to scroll down to the bottom of the page where you will see a series of blue installers. Click the installer (not Zip/Tarball!) according to your operating system. This should prompt a download. Double click on the downloaded file, which will begin the installation process. If you are on Mac OS X, drag RStudio to your Applications folder. Now you are all set to go for the tutorial! Find RStudio wherever you saved it (Applications folder if you are on Mac), and open it. In the next chapter, we will learn what to do once it is up and running. "],["tour-rstudio-with-udacity.html", "3 Tour RStudio with Udacity", " 3 Tour RStudio with Udacity Now that RStudio is open watch the Udacity video below to learn more about the interface and how to use it. Once you finish, we will begin programming in R. "],["r-basics.html", "4 R Basics 4.1 Vectors, matrices and data.frames 4.2 Indexing and Subsetting 4.3 Loading Packages", " 4 R Basics This initial tutorial for R has two primary learning objectives. The first is to become affiliated with the R environment and the second is to learn how to extend the basic set of R functions to make it suitable for your own research purposes. The lessons we learn in this tutorial will serve as a strong basis for the those that follow, which focus on the actual analysis of networks using R. Like most programming languages, R can serve as a calculator. We will use many of these basic mathematical operations when working with network data. 2+2 ## [1] 4 ((2+2)*3)/6 ## [1] 2 2^2 ## [1] 4 We use the assignment operator “&lt;-” to save the results in a vector for later. four &lt;- 2+2 sixteen &lt;- (2+2)^2 If we type the name of the vector, it will return its values. four ## [1] 4 sixteen ## [1] 16 Functions in R also have names. Later on, we will learn to write our own functions. For now, we can make use of the large body of default functions that exist within R. The most basic function is print. We can use it to output text in the console. print(&quot;Hello world!&quot;) ## [1] &quot;Hello world!&quot; log() is another useful function and it has two arguments, x and base. When you call the function log() you have to specify x (the input value), while base has a default of exp(1). log82 &lt;- log(x = 8, base = 2) If you don’t specify the arguments in their correct order, you must use argument=value form or else you will get a different result. log1 &lt;- log(8, base = 2) log2 &lt;- log(x = 8, base = 2) log3 &lt;- log(8, 2) log4 &lt;- log(base = 2, x = 8) log5 &lt;- log(2, 8) The cat function concatenates the R objects and prints them. cat(log1, log2, log3, log4, log5) ## 3 3 3 3 0.3333333 As you can see, the fifth specification of the logarithm returned different results. 4.1 Vectors, matrices and data.frames Vectors are the most basic object in R. They contain ordered elements of the same type. Vectors of size &gt; 1 are created using the “c” function. v &lt;- c(0,1,2,3,4,5,6,7,8,9) print(v) ## [1] 0 1 2 3 4 5 6 7 8 9 Computations on vectors are performed element-wise. v &lt;- v * 3 print(v) ## [1] 0 3 6 9 12 15 18 21 24 27 Matrices can be created using the matrix function. I specify what I want in each cell of the matrix and the dimensions of the matrix. This will make a square 5 by 5 matrix with all zeroes. If the argument “byrow” is TRUE, the matrix will be filled row by row (horizontally). Otherwise, it will be filled by column (vertically). zero &lt;- rep(0,5) # the rep function replicates the values in x a given number of times one &lt;- rep(1,5) matrix_data &lt;- matrix(c(zero,one,zero,one,zero), nrow = 5, ncol = 5, byrow = TRUE) matrix_data ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 1 1 1 1 1 ## [3,] 0 0 0 0 0 ## [4,] 1 1 1 1 1 ## [5,] 0 0 0 0 0 matrix_data &lt;- matrix(c(zero,one,zero,one,zero), nrow = 5, ncol = 5, byrow = FALSE) matrix_data ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 1 0 1 0 ## [2,] 0 1 0 1 0 ## [3,] 0 1 0 1 0 ## [4,] 0 1 0 1 0 ## [5,] 0 1 0 1 0 Using the sample function, we can fill each cell randomly with either 0 or 1. This simulates an adjacency matrix for a network where connections between people are random. matrix_data2 &lt;- matrix(sample(c(1,0), 25, replace = TRUE, prob=c(.5,.5)), nrow=5, ncol=5, byrow =TRUE) matrix_data2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 1 1 1 0 ## [2,] 0 1 0 0 1 ## [3,] 0 0 0 1 1 ## [4,] 1 0 1 1 1 ## [5,] 1 0 1 0 0 Finally, using dimnames, we can name each dimension, which makes sense given we are working with “social” networks. matrix_data3 &lt;- matrix(sample(c(1,0), 25, replace = TRUE, prob=c(.5,.5)), nrow=5, ncol=5, byrow =TRUE, dimnames = list(c(&quot;Thea&quot;, &quot;Pravin&quot;, &quot;Troy&quot;, &quot;Albin&quot;, &quot;Clementine&quot;), c(&quot;Thea&quot;, &quot;Pravin&quot;, &quot;Troy&quot;, &quot;Albin&quot;, &quot;Clementine&quot;))) matrix_data3 ## Thea Pravin Troy Albin Clementine ## Thea 0 1 1 1 0 ## Pravin 0 0 1 1 0 ## Troy 0 1 0 1 0 ## Albin 0 0 1 0 1 ## Clementine 1 1 0 1 1 We can convert a matrix into a data.frame and vice versa. The class() function tells us what type of object we are working with. matrix_data3 &lt;- as.matrix(matrix_data3) class(matrix_data3) ## [1] &quot;matrix&quot; &quot;array&quot; matrix_data3 &lt;- as.data.frame(matrix_data3) class(matrix_data3) ## [1] &quot;data.frame&quot; Data frames behave like a Stata dataset, for those that are familiar. However, they are inefficient in R and are increasingly being replaced by user-created data classes, such as data.table. In this class, we will deal primarily with matrices, but if you use regression analysis it will be worth your time to explore data.frames. 4.2 Indexing and Subsetting When we are working with a matrix or data.frame, we might want to access or manipulate a single row or column at a time. To do so, we need to index a row or column. This can be done in two ways. If we are working with a data.frame, we can use the $ operator. The name of the column we wish to access follows the dollar sign. matrix_data3$Thea will return the column values for Thea. matrix_data3 ## Thea Pravin Troy Albin Clementine ## Thea 0 1 1 1 0 ## Pravin 0 0 1 1 0 ## Troy 0 1 0 1 0 ## Albin 0 0 1 0 1 ## Clementine 1 1 0 1 1 matrix_data3$Thea ## [1] 0 0 0 0 1 We can then manipulate the column directly matrix_data3$Thea &lt;- 1 matrix_data3 ## Thea Pravin Troy Albin Clementine ## Thea 1 1 1 1 0 ## Pravin 1 0 1 1 0 ## Troy 1 1 0 1 0 ## Albin 1 0 1 0 1 ## Clementine 1 1 0 1 1 We can also use subscripting. For example, matrix_data3[,1] tells R to return the first column while matrix_data3[1,] tells R to return the first row. Finally, matrix_data3[1,1] is the cell located in the first row of the first column. This is more flexible than $, but requires you to know the locations of the intended data in the dataset or matrix. matrix_data3[,1] ## [1] 1 1 1 1 1 matrix_data3[1,] ## Thea Pravin Troy Albin Clementine ## Thea 1 1 1 1 0 matrix_data3[1,1] ## [1] 1 Vectors are also indexed, but they only have one dimension, so no comma is needed. trial_vector &lt;- c(1,2,3,4,5,6) trial_vector[1] ## [1] 1 Finally, we may wish to remove columns in a data.frame, matrix or vector. We can use the subset function to do this. trial_vector &lt;- subset( trial_vector, trial_vector &gt; 2) trial_vector ## [1] 3 4 5 6 We can perform this same operation with subscripts trial_vector &lt;- c(1,2,3,4,5,6) trial_vector &lt;- trial_vector[trial_vector &gt; 2] In effect, they both say - take a subset of trial_vector in which the value in the vector is greater than 2 Finally we can use the subscript index method to change values if they meet a certain criteria trial_vector &lt;- c(1,2,3,4,5,6) trial_vector[trial_vector &gt; 2] &lt;- 10 trial_vector ## [1] 1 2 10 10 10 10 We won’t use subset with matrices, but we will likely index them. Subsetting is more commonly used with data.frames 4.3 Loading Packages Packages are collections of R functions, data, and compiled code. They are built by members of the R community to add functionality to base R. Generally, if you wish you could do something with R, someone has built a package to do it already! We will use a few packages, some of which are built into R. We will need to install the others. For now, we just need to install igraph, which is the most developed network analysis package for R. To do so, we use the install.packages() function. # install.packages(&quot;igraph&quot;) The library function tells R to add the package to the current R session library(igraph) ## ## Attaching package: &#39;igraph&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## decompose, spectrum ## The following object is masked from &#39;package:base&#39;: ## ## union We will use the library() function every time we start a new R session. If R cannot find a function that you are sure is in a package you use, it normally means the package isn’t loaded or that you somehow misspelled the function name. "],["understanding-network-data-structures.html", "5 Understanding network data structures 5.1 Edge lists 5.2 Adjacency matrices", " 5 Understanding network data structures Underlying every network visualization is data about relationships. These relationships can be observed or simulated (that is, hypothetical). When analyzing a set of relationships, we will generally use one of two different data structures: edge lists or adjacency matrices. 5.1 Edge lists One simple way to represent a graph is to list the edges, which we will refer to as an edge list. For each edge, we just list who that edge is incident on. Edge lists are therefore two column matrices that directly tell the computer which actors are tied for each edge. In a directed graph, the actors in column A are the sources of edges, and the actors in Column B receive the tie. In an undirected graph, order doesn’t matter. In R, we can create an example edge list using vectors and data.frames. I specify each column of the edge list with vectors and then assign them as the columns of a data.frame. We can use this to visualize what an edge list should look like. personA &lt;- c(&quot;Mark&quot;, &quot;Mark&quot;, &quot;Peter&quot;, &quot;Peter&quot;, &quot;Bob&quot;, &quot;Jill&quot;) personB &lt;- c(&quot;Peter&quot;, &quot;Jill&quot;, &quot;Bob&quot;, &quot;Aaron&quot;, &quot;Jill&quot;, &quot;Aaron&quot;) edgelist &lt;- data.frame(PersonA = personA, PersonB = personB, stringsAsFactors = F) print(edgelist) ## PersonA PersonB ## 1 Mark Peter ## 2 Mark Jill ## 3 Peter Bob ## 4 Peter Aaron ## 5 Bob Jill ## 6 Jill Aaron What are the upsides of using the edge list format? As you can see, in an edge list, the number of rows accords to the number of edges in the network since each row details the actors in a specific tie. It is therefore really simple format for recording network data in an excel file or csv file. What are the downsides? The first is practical - it is impossible to represent isolates using an edge list since it details relations. There are ways to get around this problem in R, however. The second is technical - edge lists are not suitable for data formats for performing linear algebraic techniques. As a result, we will almost always convert and edge list into either an adjacency matrix, or into a network object. 5.2 Adjacency matrices Adjacency matrices have one row and one column for each actor in the network. The elements of the matrix can be any number but in most networks, will be either 0 or 1. A matrix element of 1 (or greater) signals that the respective column actor and row actor should be tied in the network. Zero signals that they are not tied. We can use what we learned in the last tutorial to create such a matrix. An example might look like this: adjacency &lt;- matrix(c(0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0), nrow = 5, ncol = 5, dimnames = list(c(&quot;Mark&quot;, &quot;Peter&quot;, &quot;Bob&quot;, &quot;Jill&quot;, &quot;Aaron&quot;), c(&quot;Mark&quot;, &quot;Peter&quot;, &quot;Bob&quot;, &quot;Jill&quot;, &quot;Aaron&quot;))) print(adjacency) ## Mark Peter Bob Jill Aaron ## Mark 0 1 0 1 0 ## Peter 1 0 1 0 1 ## Bob 0 1 0 1 0 ## Jill 1 0 1 0 1 ## Aaron 0 1 0 1 0 What are the upsides of using the adjacency matrix format? Adjacency matrices are the most fundamental network analysis data format. They undergird all of the analytical techniques we will show you later on. They are also much more efficient than edge lists. For example, imagine searching for whether Peter and Jill are friends in an adjacency matrix as opposed to an edge list. In the adjacency matrix, we would go to Peter’s row and Jill’s column and we would find either a 1 or a 0, giving us our answer. In an edge list, we would have to search through each edge, which might seem simple in a dataset with only 5 people, but as the edge list grows, it will scale linearly with the number of edges. What are the downsides? It is really difficult to record network data with an adjacency matrix. They are better suited for computers than people. "],["your-first-network.html", "6 Your First Network 6.1 Creating a project 6.2 Creating data in Excel 6.3 Saving Excel data as a .csv file 6.4 Loading data into R 6.5 From data to networks 6.6 Exploring your network", " 6 Your First Network In this chapter, we will create our own network data in Excel, learn to load the data into R, and the turn the data into a network object using the igraph package. You can use these steps to begin analyzing data from your own projects. 6.1 Creating a project First let’s create a new R project. RStudio projects allow you to keep your various data analysis projects separate from one another. Each project has its own working directory, workspace, history, and source documents. When you load a project, you will therefore see the work, history and files associated with that project, helping your organize your work. Go to the “File” menu at the top of the screen and click on “New Project…” Then start a new directory. Directories are folders where you keep your R Project and all of the data, files, code etc. that you create and use in the project. Now start an empty project. Projects are created by RStudio and they maintain your R workspace for you, so that you can load it exactly as it was before you closed RStudio. Finally name your project (I named mine “SNA_Tutorial”) and browse to find the location on your computer where you would like it to be saved. We will use this directory to store all of the files and information for the class, so make sure to choose a name you can remember. Now if you navigate to the folder of your new R Project (mine is on my Desktop in the folder titled “SNA Tutorial”, for example), you will see the R Project there, symbolized by a blue glass cube with an R in it. If you ever want to re-open this project, you can double click on the cube. Alternatively, in RStudio you can go “File -&gt; Open Project”, navigate to the project’s directory and open it from there. This folder is where you should keep all of your data and R scripts for this project. Since we just created the project that we will use for the remainder of the tutorials, it is the best place for you to put all future class materials. 6.2 Creating data in Excel Imagine you want to collect some data on your local social network. You might go to each of your friends and ask them to nominate up to five people who are their friends. You then want to load this data into R and graph it as a network. How could you go about doing this? The first step is to record the data in some machine-readable way, either using Excel or Numbers or even in plain text files. Many people, before coming to R, have experience using Excel or some program like it to manage data, whether financial, academic, or otherwise. An Excel worksheet is organized into rows and columns with columns generally containing information of a single kind (say First Names) and rows generally containing the data for a single observation. You can freely input data into any of the cells. An empty Excel spreadsheet The first step in any network analysis project is to create a dataset with the relationships between the people in your study. The easiest way to record relationships in Excel is as an edge list, which, as we discussed in the last section, is a two column matrix that lists the pairs of actors in a relationship. Let’s build an edge list together in Excel. Imagine we want to track patterns of monetary provision in a household (i.e. whether person A gives person B money). We construct an edge list, where column A is the Ego (the money provider) and column B is the Alter (the money taker). We then fill in instances, or relationships, of monetary provision from Ego to Alter. Below is an example: Money providing edge list (Hoffman family) Try recording something similar for your family! 6.3 Saving Excel data as a .csv file Great! Now we need to save this data in a format that R can easily read. I generally use the .csv format, which separates values by commas, because R has a default function for reading .csv files. To save an Excel sheet as a .csv file, in Excel go to “File -&gt; Save As…” Then give your file a name (I chose “money_edgelist”), choose the place where you want to save it (inside of the directory you made for class), and finally (the most important part!), click on the “File Format” dropdown menu and choose “Comma Separated Values (.csv)”. Click “Save”! It will ask will warn you that the workbook contains features that will not work or may be removed if you save it in the selected file format and ask if you want to continue. Click “Continue”. Great! Now, inside of your class directory, you should see a new file titled “money_edgelist” (or whatever you chose to name it). In the following section, we will learn how to read that edge list into R. 6.4 Loading data into R Open up RStudio, make sure you are in your project for class (if not, go to “File -&gt; Open Project”, and selection your project), and open a new R Script by going “File -&gt; New File -&gt; R Script”. Save the empty script and name it “loading_data.R”. Your RStudio should look like this: Now we will write our first line of code together. R has a simple function for loading .csv files: read.csv() To use read.csv, we just tell R the name of the .csv file that we want it to read. It will then look inside the directory for that file. If it can’t find the file - either because you typed the wrong name or you never dragged it to the directory - it will return something like the following error: Error in file(file, “rt”) : cannot open the connection In addition: Warning message: In file(file, “rt”) : cannot open file ‘money_edgelist.csv’: No such file or directory So be sure money_edgelist.csv is in your directory or the following lines of code won’t work! Of course, you can load in any edgelists here and name it anything you like. money_edgelist = read.csv(&quot;data/money_edgelist.csv&quot;) Now you should see your data fle in your Environment pane under the “Data” heading. That means you successfully loaded the data into R. You can click on it to see its contents or else run View(your_data_here) in the console. Cool! We now have our data loaded into R and ready to be turned into a network. 6.5 From data to networks In this class, we will primarily use igraph, a user-maintained package in R, to analyze networks. Installing igraph gives us a bunch of new tools for graphing, analyzing and manipulating networks, that don’t come with base R. The first step then is to install igraph. To install a new package, we use the install.packages() function. It takes a character argument that is the name of the package you wish to install. install.packages(&quot;igraph&quot;, repos=&#39;http://cran.us.r-project.org&#39;) If you leave out the “repos” argument, it will cause a window to pop up with a list of CRAN mirrors. Then you can choose the repository nearest to you by double clicking on it. Now that igraph is installed, we need to use the library() function to load it into R. You will have to do this every time you open RStudio or switch projects if you wish to make use of igraph’s functions. library(&quot;igraph&quot;) This will allow us to use all of igraph’s functions. To analyze networks, igraph uses an object class called: “igraph”. We therefore have to convert our edge list, freshly loaded into R, into an igraph object. igraph only takes matrices, so we then have to convert our data.frame (the default class of objects returned by read.csv()) to a matrix. money_edgelist &lt;- as.matrix(money_edgelist) We can now turn the money_edgelist edge list into a network object. The required function is graph.edgelist() and it takes two arguments, the network data (an edge list) and whether the edges are directed or undirected. In this case, because giving money is not necessarily a reciprocal relationship (i.e. just because I give you money, doesn’t mean you necessarily give it back… in fact, the opposite is almost always the case!), the network should be directed. moneyNetwork &lt;- graph.edgelist(money_edgelist, directed=TRUE) A Note on Function Documentation If you want to see more about graph.edgelist() or if you want to see other ways to graph data type, you can type ?graph.edgelist. Entering ? before any function will cause R to bring up documentation on that function. Now we have two objects in our Environment, the money_edgelist and a new networked, called moneyNetwork. Both contain the same information at the moment, but igraph can only make use of the latter. What if my data was in adjacency matrix format? If your data was in adjacency matrix format, then you would use the graph.adjacency() function instead of the graph.edgelist() function. More about the graph.adjacency() function can be read in the function’s help section accessed by typing ?graph.adjacency 6.6 Exploring your network We finally have a network in R! So.. what next? Well we can read a summary of it by typing its name into R. moneyNetwork ## IGRAPH f5811ef DN-- 6 11 -- ## + attr: name (v/c) ## + edges from f5811ef (vertex names): ## [1] Greg -&gt;Maria Greg -&gt;Mark Greg -&gt;Lexi Greg -&gt;Grace Greg -&gt;Nick ## [6] Maria-&gt;Mark Maria-&gt;Lexi Maria-&gt;Grace Maria-&gt;Nick Mark -&gt;Nick ## [11] Lexi -&gt;Nick The first line - which begins with IGRAPH DN - tells us moneyNetwork is an igraph object and a directed network (DN), with N nodes and E edges. The next line tells us some attributes of the nodes and edges network. At the moment, it only has the attribute “name” for the vertices (v/c). We can look at the values of the “name” attribute with the V()$ function. V(moneyNetwork)$name ## [1] &quot;Greg&quot; &quot;Maria&quot; &quot;Mark&quot; &quot;Lexi&quot; &quot;Grace&quot; &quot;Nick&quot; Finally, the last part gives us a snapshot of the edges present in the network, most of which are omitted. We can visualize the network using the plot() function. plot(moneyNetwork) The default visualization is pretty ugly… In the next section, we will learn how to improve the aesthetics of our network visualizations. "],["network-visualization-and-aesthetics.html", "7 Network Visualization and Aesthetics 7.1 The Basics 7.2 Layouts 7.3 Adding attributes to a network object 7.4 Plotting based on attributes", " 7 Network Visualization and Aesthetics As social scientists, we want to tell convincing stories about the structure and dynamics of the networks we study. Visualization and statistics are the primary tools at our disposable for conveying these stories. In this tutorial we will learn how to change the aesthetics of our network visualizations. When visualizing networks, there are a number of different elements we can adjust. First, we can change the color, size, shapes and labels of nodes. Second, we can change the color, width, curviture and appearance of edges. We can highlight the location of different groups in the network. Finally, we can manipulate the overall layout of the network. 7.1 The Basics Let’s start by adjusting the basic visualization. First, load in the data we created in the last tutorial and graph it as a network. The code below should look familiar. The only difference is that I converted money_edgelist into a matrix in the same line as graphing it as an edgelist. This is called nesting functions. There is no limit to the amount of functions you can have nested, though often times it is easier to read if you break them up into multiple lines. library(igraph) money_edgelist &lt;- read.csv(&quot;data/money_edgelist.csv&quot;, stringsAsFactors = F) moneyNetwork &lt;- graph.edgelist(as.matrix(money_edgelist), directed=TRUE) Visualizing the network is as simple as typing plot(moneyNetwork). But as I mentioned earlier, the default plot is really horrible (and it used to be worse…). We can adjust the settings of the plot so that the resulting visualization is more aesthetically pleasing. There are many settings for the igraph plot() function, and we will go over the bare minimum you need to start making decent visualizations on your own. plot(moneyNetwork) The most basic things we can change are the sizes and colors of nodes. When your network is large, often the nodes will appear too large and clumped together. The argument vertex.size allows you to adjust node size (vertex is the graph-theory term for node!). plot(moneyNetwork, vertex.size = 10) This is the basic way you change the settings of the plot function in igraph - you put a comma next to the next to the network object, type the name of the setting you want to change, and set it to a new value. Here we set vertex.size to 10. When you don’t change any settings, R will automatically use the default settings. You can find them in the help section for that function (i.e. by typing ?plot.igraph, for example). All you have to do is remember the names of the various settings, or look them up at: http://igraph.org/r/ The nodes have an ugly light orange color… We can use vertex.color to change their color to something nicer. We can also remove the ugly black frames by changing the vertex.frame.color setting to NA. Useful Link You can find a list of all the named colors in R at http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf plot(moneyNetwork, vertex.size = 10, vertex.color = &quot;tomato&quot;, vertex.frame.color = NA) The labels are too large and blue. We can adjust label size with vertex.label.cex. We can adjust the color with vertex.label.color plot(moneyNetwork, vertex.size = 10, vertex.color = &quot;tomato&quot;, vertex.frame.color = NA, vertex.label.cex = .7, vertex.label.color = &quot;black&quot;) Alternatively, if we want to get rid of the labels, we can just set vertex.label to NA. plot(moneyNetwork, vertex.size = 10, vertex.color = &quot;tomato&quot;, vertex.frame.color = NA, vertex.label = NA) Finally we can make the edges smaller and curved to give them a nicer aesthetic plot(moneyNetwork, vertex.size = 10, vertex.color = &quot;tomato&quot;, vertex.frame.color = NA, vertex.label.cex = .7, vertex.label = NA, edge.curved = .1, edge.arrow.size = .3, edge.width = .7) But don’t go too crazy! If you set edge.curved to be greater than .1, it will start to look like spaghetti. plot(moneyNetwork, vertex.size = 10, vertex.color = &quot;tomato&quot;, vertex.frame.color = NA, vertex.label.cex = .7, vertex.label = NA, edge.curved = 1.7, edge.arrow.size = .3, edge.width = .7) 7.2 Layouts An essential part of a network visualization is its layout, which determines the nodes’ positions in the plot. There are a wide range of layouts that have been developed for social network analysis. They all try to minimize the number of edges that cross, but use different algorithms for achieving this goal. Generally, I use either the Kamada Kawai algorithm or the Fruchterman Reingold algorithm. igraph has set of layout functions which, when passed a network object, return an array of coordinates that can then used when plotting that network. These coordinates should be saved to a separate R object, which is then called within the plot function. They all have the format: layout DOT algorithm name. For example, layout.kamada.kawai() or layout.fruchterman.reingold() Kamada Kawai # first we run the layout function on our graph kamadaLayout &lt;- layout.kamada.kawai(moneyNetwork) # and then we change the default layout setting to equal the layout we generated above plot(moneyNetwork, layout = kamadaLayout, vertex.size = 10, vertex.color = &quot;tomato&quot;, vertex.frame.color = NA, vertex.label.cex = .7, vertex.label = NA, edge.curved = .1, edge.arrow.size = .3, edge.width = .7) Frucherman-Reingold # first we run the layout function on our graph fruchtermanLayout &lt;- layout.fruchterman.reingold(moneyNetwork) # and then we change the default layout setting to equal the layout we generated above plot(moneyNetwork, layout = fruchtermanLayout, vertex.size = 10, vertex.color = &quot;tomato&quot;, vertex.frame.color = NA, vertex.label.cex = .7, vertex.label = NA, edge.curved = .1, edge.arrow.size = .3, edge.width = .7) You can see ?layout_ for more options and details. 7.3 Adding attributes to a network object We can affect any of these aesthetics arbitrarily. You might prefer the way networks look when the edges are curved as opposed to straight for example. However, we often make decisions informed by theories we have about our empirical setting. If we think that race is important to the structure of the network, we might color nodes according to race. If we think both race and gender are important, we could color nodes by race and change the shape of nodes by gender. We could also size nodes according to wealth. We are generally limited to visualizing three attributes of nodes at once (using color, shape and size), though I find that anything more than two (color and size) is difficult to interpret. It follows that, to visualize how attributes are distributed over the nodes of the network, most network projects need two separate datasets. In addition to creating and loading a dataset of relations, which we covered in the previous tutorial, we need a second dataset that details actors’ attributes and we need to add the data to our network. Deciding wich attributes are relevant will depend on the domain and your research question; if you are studying gangs in the southside Chicago, you might record actors’ gang affiliations and residence; if you are interested in the business relationships between Saudi elites, you might record their age and lineage. Let’s return to Excel and build an attribute dataset for our network. This kind of dataset is much more traditional: each row is a person (otherwise known as an observation) and each column accords with some attribute you measured for that person. Here is a picture of an example attribute dataset I made for my family’s money provision network. Hoffman family attributes I saved this attribute dataset as a .csv file inside of my R directory. I will load it into R, just as I did the edgelist in the previous tutorial (all of this should be practiced and familiar! if not, revisit the previous tutorial). attributes &lt;- read.csv(&quot;data/attribute_df.csv&quot;, stringsAsFactors = F) head(attributes) ## Name Age Gender Role ## 1 Greg 53 Male Father ## 2 Maria 52 Female Mother ## 3 Mark 25 Male Son ## 4 Lexi 23 Female Daughter ## 5 Grace 19 Female Daughter ## 6 Nick 14 Male Son This gives us a data.frame, where each row corresponds to a person and each column corresponds to one attribute of the people in our network, but to use these attributes in igraph, we have to assign them to the nodes in the igraph object (moneyNetwork, that is). Attributes in igraph are assigned to nodes and edges separately. As mentioned in the previous tutorial, vertices are accessed using the V() function while edges are accessed with the E() function. Attributes are then accessed with the dollar sign operator followed by the attribute name. For example, as we showed earlier, V(countrysideNetwork)$name will tell us the names of all of the nodes in the network. To change an attribute, we can just use the equals sign to set them equal to something else. So for example, here I change the names of the nodes in my network # Change the names V(moneyNetwork)$name = c(&quot;Bob&quot;, &quot;Linda&quot;, &quot;Elias&quot;, &quot;Catherine&quot;, &quot;Eloise&quot;, &quot;Pumpkin&quot;) # Print the node names to see that it worked! print(V(moneyNetwork)$name) ## [1] &quot;Bob&quot; &quot;Linda&quot; &quot;Elias&quot; &quot;Catherine&quot; &quot;Eloise&quot; &quot;Pumpkin&quot; At the moment, our network doesn’t have any attributes other than name. If we try to look at the sex of our nodes, we will get NULL as a result. V(moneyNetwork)$gender[1:6] ## NULL We therefore need to attach the attributes from our attribute file to our network. The method for doing can be a bit complicated. If we have an edge list, we can use the _from_data_frame() function instead of graph.edgelist(), and include the attributes file as the vertices argument. For whatever reason, in this case, the edge list needs to be a data.frame, so no need to convert it to a matrix. Also, it is important to note that igraph assumes that the first column is the name column, so make sure that is the case! # Load in the edge list again money_edgelist &lt;- read.csv(&quot;data/money_edgelist.csv&quot;, stringsAsFactors = F) # Load in the attributes again attributes &lt;- read.csv(&quot;data/attribute_df.csv&quot;, stringsAsFactors = F) # Put them both in the network. moneyNetwork &lt;- graph_from_data_frame(money_edgelist, directed = T, vertices = attributes) Now, if we look at our network, we will see that there are many more next to attr. moneyNetwork ## IGRAPH 5a66b2e DN-- 6 11 -- ## + attr: name (v/c), Age (v/n), Gender (v/c), Role (v/c) ## + edges from 5a66b2e (vertex names): ## [1] Greg -&gt;Maria Greg -&gt;Mark Greg -&gt;Lexi Greg -&gt;Grace Greg -&gt;Nick ## [6] Maria-&gt;Mark Maria-&gt;Lexi Maria-&gt;Grace Maria-&gt;Nick Mark -&gt;Nick ## [11] Lexi -&gt;Nick We can look at each one with the V()$ function. For example, here is gender. V(moneyNetwork)$Gender ## [1] &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; Awesome! Now the attributes are in the network. 7.4 Plotting based on attributes In the last section, we imported attributal data into our network object, which will allow us to manipulate our network according to nodal attributes. Let’s start by manipulating color according to gender To do this we have to assign colors to people according to their sex. We use the ifelse function. The ifelse() function takes three arguments. The first is a test (something that evaluates to TRUE or FALSE), the second is what to return if the test is TRUE and third is what to return if the test is FALSE. We therefore set up an ifelse function, which tests whether a node’s gender is male, assigning it the color “blue” if TRUE (i.e. if they are a male) and “green” if FALSE (i.e. otherwise/they are a female). Remember that R is case sensitive, so if your gender variable contains “Males” and “Females”, then make sure you put Male or Female (capitalized) in the ifelse statement. V(moneyNetwork)$color &lt;- ifelse(V(moneyNetwork)$Gender == &quot;Male&quot;, &quot;dodgerblue3&quot;,&quot;seagreen&quot;) Now we can replot the network. This time a node’s color will be green or blue depending on their gender. Notice that I didn’t set the vertex.colors inside the plot function this time, since doing so would override the colors we just gave the nodes. plot(moneyNetwork, vertex.size = 10, vertex.frame.color = &quot;black&quot;, vertex.label.cex = .7, vertex.label = NA, edge.curved = .1, edge.arrow.size = .3) Now let’s try role. First, I reset color. There are four roles (Father, Mother, Son, Daughter), so we need a few more ifelse statements to code for all of them. V(moneyNetwork)$color &lt;- NA V(moneyNetwork)$color &lt;- ifelse(V(moneyNetwork)$Role == &quot;Father&quot;, &quot;burlywood1&quot;,&quot;tomato&quot;) V(moneyNetwork)$color &lt;- ifelse(V(moneyNetwork)$Role == &quot;Mother&quot;, &quot;seagreen&quot;, V(moneyNetwork)$color) V(moneyNetwork)$color &lt;- ifelse(V(moneyNetwork)$Role == &quot;Son&quot;, &quot;grey70&quot;, V(moneyNetwork)$color) plot(moneyNetwork,vertex.size = 10, vertex.label.cex = .7, vertex.label = NA, edge.curved = .1, vertex.frame.color = &quot;black&quot;, edge.arrow.size = .3, edge.width = .7, edge.color = &quot;grey30&quot;) Last but not least, let’s adjust the sizes of the nodes so that they reflect differences in age. We can set node size to be 1/5th of the node’s age with the code below. Simple, but effective. It looks like the oldest nodes give to the most people. V(moneyNetwork)$size = V(moneyNetwork)$Age/5 plot(moneyNetwork, vertex.label.cex = .7, vertex.label = NA, edge.curved = .1, vertex.frame.color = &quot;black&quot;, edge.arrow.size = .3, edge.width = .7, edge.color = &quot;grey30&quot;) "],["ego-networks.html", "8 Ego Networks", " 8 Ego Networks In this section, we analyze ego networks from the GSS network module in 2004. We will use the GSS data to become acquainted with measures of network density and heterogeneity. It will also teach us how to analyzing many networks all at once. In some cases, you might have hundreds of complete networks - for example, data about high schools often has networks from many different high schools. Since the schools are separate, you have to analyze them separately, but doing so one by one is laborious. Here we will learn about ego networks as well as strategies for apply the same function to many networks at once. We begin by reading in the data from the GSS network module, which I have included in the “Data” section of the materials for this class. library(igraph) gss &lt;- read.csv(&quot;https://raw.githubusercontent.com/mahoffman/stanford_networks/main/data/gss_local_nets.csv&quot;, stringsAsFactors = TRUE) Let’s have a look at the data. You can either click on it in your environment, type View(gss) or: head(gss) ## sex race age partyid relig numgiven close12 close13 ## 1 female other 52 independent catholic 0 NA NA ## 2 female other 43 not str republican catholic 0 NA NA ## 3 male black 52 strong democrat protestant 4 1 2 ## 4 female other 34 ind,near dem catholic 4 2 0 ## 5 male other 22 ind,near dem moslem/islam 0 NA NA ## 6 male black 26 not str democrat protestant 6 0 2 ## close14 close15 close23 close24 close25 close34 close35 close45 sex1 sex2 ## 1 NA NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA ## 3 0 NA 2 2 NA 1 NA NA 1 1 ## 4 2 NA 2 2 NA 2 NA NA 1 0 ## 5 NA NA NA NA NA NA NA NA NA NA ## 6 1 1 1 1 1 2 2 2 1 1 ## sex3 sex4 sex5 race1 race2 race3 race4 race5 educ1 educ2 educ3 ## 1 NA NA NA NA NA NA NA NA NA &lt;NA&gt; &lt;NA&gt; ## 2 NA NA NA NA NA NA NA NA NA &lt;NA&gt; &lt;NA&gt; ## 3 0 0 NA 1 1 1 1 NA 1 h.s. grad Grad ## 4 1 1 NA 2 2 2 2 NA 1 h.s. grad Grad ## 5 NA NA NA NA NA NA NA NA NA &lt;NA&gt; &lt;NA&gt; ## 6 0 1 1 0 1 1 2 2 1 h.s. grad h.s. grad ## educ4 educ5 age1 age2 age3 age4 age5 relig1 relig2 ## 1 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA &lt;NA&gt; &lt;NA&gt; ## 3 Bachelors &lt;NA&gt; 56 40 58 59 NA protestant protestant ## 4 Grad &lt;NA&gt; 63 36 34 36 NA catholic catholic ## 5 &lt;NA&gt; &lt;NA&gt; NA NA NA NA NA &lt;NA&gt; &lt;NA&gt; ## 6 Some College Some College 25 25 39 33 30 other other ## relig3 relig4 relig5 ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 protestant protestant &lt;NA&gt; ## 4 catholic catholic &lt;NA&gt; ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 catholic catholic catholic There are 42 variables. The first five concern the attributes of a given respondent: their sex, age, race, partyid and religion. The next 36 make up the “network” part of the GSS Network Module. The structure can be a bit confusing, especially if you haven’t read any papers that use this data. The basic idea of the module was to ask people about up to five others with whom they discussed “important matters” in the past six months. The respondents reported the number of people whom they discussed “important matters”: which is the variable “numgiven” in our dataset. They were also asked to detail the relations between those five people: whether they were especially close, knew each other, or were total strangers. This accords to the close variables in the dataset, where, for example, close12 is the closeness of person 1 to person 2, for each respondent. Finally they were asked about the attributes of each of the up to five people in their ego network (sex, race, age). To see why these are called ego networks, let’s take a respondent and graph the relations of the up to five people they said they discussed “important matters” with. To do so, we have to first turn the variables close12 through close45 into an edge list, one for each respondent. This requires a tricky bit of code. First we use grepl to extract the columns we want. grep basically uses string matching, so it looks through the column names and identifies those with the word “close” in them (look here for more information: https://www.regular-expressions.info/rlanguage.html) ties &lt;- gss[,grepl(&quot;close&quot;, colnames(gss))] head(ties) ## close12 close13 close14 close15 close23 close24 close25 close34 close35 ## 1 NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 1 2 0 NA 2 2 NA 1 NA ## 4 2 0 2 NA 2 2 NA 2 NA ## 5 NA NA NA NA NA NA NA NA NA ## 6 0 2 1 1 1 1 1 2 2 ## close45 ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 NA ## 6 2 As an example of what we will do for each respondent, let’s first make a matrix, which we can fill in with the closeness values for a given respondent. mat = matrix(nrow = 5, ncol = 5) As it turns out, we can assign a person’s close values directly to the lower triangle of the matrix. Here we do it for respondent 3. mat[lower.tri(mat)] &lt;- as.numeric(ties[3,]) And we can symmetrize the matrix since the relation here (closeness) is mutual (i.e. the relation is undirected). mat[upper.tri(mat)] = t(mat)[upper.tri(mat)] mat ## [,1] [,2] [,3] [,4] [,5] ## [1,] NA 1 2 0 NA ## [2,] 1 NA 2 2 NA ## [3,] 2 2 NA 1 NA ## [4,] 0 2 1 NA NA ## [5,] NA NA NA NA NA Nice! Now let’s drop any of the respondents who are missing. na_vals &lt;- is.na(mat) non_missing_rows &lt;- rowSums(na_vals) &lt; nrow(mat) mat &lt;- mat[non_missing_rows,non_missing_rows] And set the diagonal to zero, since NAs give igraph trouble diag(mat) &lt;- 0 How does it look? Perfectly symmetrical, like all undirected graphs should be! mat ## [,1] [,2] [,3] [,4] ## [1,] 0 1 2 0 ## [2,] 1 0 2 2 ## [3,] 2 2 0 1 ## [4,] 0 2 1 0 Great! We can use this matrix to creat a network for a single respondent, like we did in the last tutorial but this time using the graph.adjacency function since our input data is a matrix. We will specify that we want it to be undirected and weighted. ego_net &lt;- graph.adjacency(mat, mode = &quot;undirected&quot;, weighted = T) How does it look? plot(ego_net, vertex.size = 30, vertex.label.color = &quot;black&quot;, vertex.label.cex = 1) Cool.. the only problem is that we have to do this for every row in the dataset… what should we do? One option is to create a function, which uses the code above to turn any row in the ties data set into an ego network, and then apply that function to every row in the data. Below is such a function! make_ego_nets &lt;- function(tie){ # make the matrix mat = matrix(nrow = 5, ncol = 5) # assign the tie values to the lower triangle mat[lower.tri(mat)] &lt;- as.numeric(tie) # symmetrize mat[upper.tri(mat)] = t(mat)[upper.tri(mat)] # identify missing values na_vals &lt;- is.na(mat) # identify rows where all values are missing non_missing_rows &lt;- rowSums(na_vals) &lt; nrow(mat) # if any rows if(sum(!non_missing_rows) &gt; 0){ mat &lt;- mat[non_missing_rows,non_missing_rows] } diag(mat) &lt;- 0 ego_net &lt;- graph.adjacency(mat, mode = &quot;undirected&quot;, weighted = T) return(ego_net) } Now we can use lapply to loop through all of the rows in the data and apply the above function to each row. It will return a list of size nrow(ties), in which every item is an ego net of one of the respondents in the data. ego_nets &lt;- lapply(1:nrow(ties), FUN = function(x) make_ego_nets(ties[x,])) head(ego_nets) ## [[1]] ## IGRAPH 04f2d8f U--- 0 0 -- ## + edges from 04f2d8f: ## ## [[2]] ## IGRAPH dee1515 U--- 0 0 -- ## + edges from dee1515: ## ## [[3]] ## IGRAPH ad1e1d8 U-W- 4 5 -- ## + attr: weight (e/n) ## + edges from ad1e1d8: ## [1] 1--2 1--3 2--3 2--4 3--4 ## ## [[4]] ## IGRAPH 5ec7ac1 U-W- 4 5 -- ## + attr: weight (e/n) ## + edges from 5ec7ac1: ## [1] 1--2 1--4 2--3 2--4 3--4 ## ## [[5]] ## IGRAPH 35e2c04 U--- 0 0 -- ## + edges from 35e2c04: ## ## [[6]] ## IGRAPH d95e552 U-W- 5 9 -- ## + attr: weight (e/n) ## + edges from d95e552: ## [1] 1--3 1--4 1--5 2--3 2--4 2--5 3--4 3--5 4--5 Awesome! We have a whole list of networks. Let’s take a look at a random network, say, the 1001st ego net. random_ego_net &lt;- ego_nets[[1021]] plot(random_ego_net) "],["calculating-network-size-and-density.html", "9 Calculating Network Size and Density", " 9 Calculating Network Size and Density Now that we have a list of networks, we can apply the same function to each network using a single line of code, again with the help of lapply. Network size is the number of nodes in a network. To find this, we use the vcount() function. We can also find the number of edges using ecount() network_sizes &lt;- lapply(ego_nets, vcount) network_edge_counts &lt;- lapply(ego_nets, ecount) head(network_sizes) ## [[1]] ## [1] 0 ## ## [[2]] ## [1] 0 ## ## [[3]] ## [1] 4 ## ## [[4]] ## [1] 4 ## ## [[5]] ## [1] 0 ## ## [[6]] ## [1] 5 We can take the mean of one of these results simply by turning the list into a vector and using the mean function on the resulting vector. network_sizes &lt;- unlist(network_sizes) mean(network_sizes, na.rm = T) ## [1] 1.796634 The average network has a little over one and a half people in it. We could similarly plot the distribution. hist(network_sizes, main = &quot;Histogram of Ego Network Sizes&quot;, xlab = &quot;Network Size&quot;) And, naturally, we can do the same for edges. network_edge_counts &lt;- unlist(network_edge_counts) hist(network_edge_counts, main = &quot;Histogram of Ego Network Edge Counts&quot;, xlab = &quot;# of Edges&quot;) Finally, let’s try density. Density captures how many edges there are in a network divided by the total possible number of edges. In an undirected network of size N, there will be (N * (N-1))/2 possible edges. If you think back to the matrix underlying each network, N * N-1 refers to the number of rows (respondents) times the number of columns (respondents again) minus 1 so that the diagonal (i.e. ties to oneself) are excluded. We divide that number by 2 in the case of an undirected network only to account for that fact that the network is symmetrical. We could calculate this on our own for the random ego network from before as follows. ecount(random_ego_net)/((vcount(random_ego_net) * (vcount(random_ego_net) - 1))/2) ## [1] 0.6 igraph has its own function - graph.density which we can again apply to every ego network in the data. densities &lt;- lapply(ego_nets, graph.density) densities &lt;- unlist(densities) To end the tutorial, let’s plot the distribution of density across the different ego networks. hist(densities) "],["affiliation-data.html", "10 Affiliation Data 10.1 Tripartite network analysis?", " 10 Affiliation Data This portion of the tutorial focuses on affiliation data. Individuals can be directly linked to one another by affections or interactions. We have spent the tutorial so far working with direct, one-mode networks. That said, individuals can also be linked through “affiliations”, that is, shared associations to groups or objects. As an example, people might be tied by the classes they have taken together. Such data might look like: Person, Classes Leo, Biostatistics, Chemistry, Linear Algebra Clement, Islamic Civilization, The Modern World-System, Exile and Diaspora Paula, Calc 1, Calc 2, Linear Algebra, Filippo, Linear Algebra, Social Networks, The Modern World-System We can create a network with two types of nodes - one set of nodes will be people, the other classes. People, in this network, cannot be directly tied to each other. Rather they are co-affiliated with a class, which serves as the basis of their connection. Therefore, all ties will be between nodes of different types. To create this network, we need to turn the above data into an edgelist, convert it to a matrix, and plot it in igraph. Let’s start with the data. library(igraph) classes_data &lt;- data.frame(name = c(&quot;Leo&quot;, &quot;Clement&quot;, &quot;Palla&quot;, &quot;Filippo&quot;), class1 = c(&quot;Biostatistics&quot;,&quot;Islamic Civ&quot;, &quot;Calc 1&quot;, &quot;Linear Algebra&quot;), class2 = c(&quot;Chemistry&quot;, &quot;The Modern World-System&quot;, &quot;Calc 2&quot;, &quot;Social Networks&quot;), class3 = c(&quot;Linear Algebra&quot;, &quot;Exile and Diaspora&quot;, &quot;Linear Algebra&quot;, &quot;The Modern World-System&quot;), stringsAsFactors = FALSE) classes_data ## name class1 class2 class3 ## 1 Leo Biostatistics Chemistry Linear Algebra ## 2 Clement Islamic Civ The Modern World-System Exile and Diaspora ## 3 Palla Calc 1 Calc 2 Linear Algebra ## 4 Filippo Linear Algebra Social Networks The Modern World-System The reshape packages will let us convert this type of data into an edgelist. # install.packages(&quot;reshape2&quot;) library(reshape2) classes_data &lt;- melt(classes_data, measure.vars = c(&quot;class1&quot;, &quot;class2&quot;,&quot;class3&quot;), value.name = &quot;classes&quot;, variable.name = &quot;order&quot;) The ?melt function turns so called “short form data” into “long form”. It takes the class variables and combines them into a single variable “classes”. We only need two columns, name and classes, so we use the subset function to select them. If we look at the data now, it is basically an edge list, in which people are on the left side and classes they are affiliated with on the right. classes_data &lt;- subset(classes_data, select = c(&quot;name&quot;, &quot;classes&quot;)) Once we have such an edge list, we can then use the table function to turn it into an incidence matrix, which is what igraph needs to turn affiliation data into an igraph object. classesMatrix = table(classes_data) class(classesMatrix) &lt;- &quot;matrix&quot; # And we convert it from a table to a matrix # View(classesMatrix) In an incidence matrix, the rows are of one class of node, while columns are of another. The rows are generally people who are affiliated with groups in the columns. Using the get.incidence() function will turn our matrix into a bipartite network. classesNet &lt;- graph.incidence(classesMatrix, mode = c(&quot;all&quot;)) plot(classesNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;) We can change the shape of nodes to highlight their type. V(classesNet)$shape &lt;- ifelse(V(classesNet)$type == FALSE, &quot;circle&quot;, &quot;square&quot;) plot(classesNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;) 10.0.1 Unipartite Projection Bipartite networks can be represented (or “projected”) as unipartite networks. In this case, either people will be the only nodes, and they will be connected if they share an affiliation (i.e. they are in the same group) OR groups willbe the only nodes and they will be connected if they share an affiliation to a person. We can make the projection two ways - using the bipartite.projection() function in igraph, or by multiplying the incidence matrix by its transpose (or vise versa). The mathematical operation to make a person-to-person projection is to multiply the initial matrix by its transpose. In R that looks like: personMatrix = classesMatrix %*% t(classesMatrix) # View(personMatrix) where the t() function transposes the matrix that is passed to it and %*% performs matrix multiplication. The diagonal of this new matrix tells us the number of groups each person is affiliated with, but we set it to 0 using the ?diag function. number_of_classes_taken = diag(personMatrix) diag(personMatrix) &lt;- 0 # View(personMatrix) personNet &lt;- graph.adjacency(personMatrix, mode = &quot;undirected&quot;) plot(personNet, vertex.size = 8, vertex.label.cex = .8, vertex.label.color = &quot;black&quot;) To get the group-to-group matrix, we multiply the transpose by the initial matrix (reverse!) groupMatrix = t(classesMatrix) %*% classesMatrix # View(groupMatrix) # The diagonal details the number of people in each class number_of_students &lt;- diag(groupMatrix) diag(groupMatrix) &lt;- 0 # we again set it to 0 Both of these operations turn our rectangular incidence matrix into a square adjacency matrix. Order matters. Now that we have adjacency matrices can use the graph.adjacency() function to turn them into network objects. personNet &lt;- graph.adjacency(personMatrix, mode = &quot;undirected&quot;) groupNet &lt;- graph.adjacency(groupMatrix, mode = &quot;undirected&quot;) plot(personNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;) plot(groupNet, vertex.size = betweenness(groupNet)/max(betweenness(groupNet)) * 10, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;) We can analyze these networks just like we would any other network with a single node type. 10.1 Tripartite network analysis? What if we wanted to analyze data from a third mode or level? For example, classes are not run independently, rather they nested in departments and schools, which govern curricula and student enrollment. Adding additional modes can allow us to trace multilevel linkages. Here is a quick example that we will build on in later classes. First, we will build a classes to departments matrix. classes_to_departments &lt;- data.frame(class = c(&quot;Biostatistics&quot;,&quot;Islamic Civ&quot;, &quot;Calc 1&quot;, &quot;Linear Algebra&quot;, &quot;Chemistry&quot;, &quot;The Modern World-System&quot;, &quot;Calc 2&quot;, &quot;Social Networks&quot;, &quot;Exile and Diaspora&quot;), department = c(&quot;Math&quot;, &quot;History&quot;, &quot;Math&quot;, &quot;Math&quot;, &quot;Chemistry&quot;, &quot;Sociology&quot;, &quot;Math&quot;, &quot;Sociology&quot;, &quot;History&quot;), stringsAsFactors = F) classes_to_departments_matrix &lt;- table(classes_to_departments) class(classes_to_departments_matrix) &lt;- &quot;matrix&quot; Now following the paper on tripartite structural analysis, we can multiply the transpose of this matrix by the transpose of classesMatrix to trace links between people and departments! people_to_departments &lt;- t(classes_to_departments_matrix) %*% t(classesMatrix) We can graph this matrix and analyze it like a bipartite graph. people_to_departments_net &lt;- graph.incidence(people_to_departments) plot(people_to_departments_net, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;) 10.1.1 Lab For this week’s lab, install Ben Davies’ (a PhD student in economics here at Stanford!) package containing data on all the papers published in the NBER Working Paper Series. Take the bipartite edge list that it provides, subset it to a reasonable size (below, I show how to subset it to papers published since 2016) and use it to construct a bipartite network using the steps outlined above. Plot the resulting bipartite network, representing papers as squares and authors as circles. Identify the 10 authors who have published the most papers using the degree() function. Next, project the network to a one-mode network in which authors are connected to other authors and identify the 10 authors with the most collaborators, again using degree(). Below install.packages(&#39;nberwp&#39;) library(nberwp) data(paper_authors) data(authors) data(papers) papers &lt;- subset(papers, year &gt;= 2016) paper_authors &lt;- subset(paper_authors, paper_authors$paper %in% papers$paper) paper_authors$author &lt;- authors$user_nber[match(paper_authors$author , authors$author)] head(paper_authors) ## paper author ## 45988 w21840 timothy_kehoe ## 45989 w21840 kim_ruhl ## 45990 w21840 jose_asturias ## 45991 w21840 sewon_hur ## 45992 w21841 robert_topel ## 45993 w21841 kevin_murphy "],["transitivity-structural-balance-and-hierarchy.html", "11 Transitivity, structural balance, and hierarchy 11.1 The Dyad 11.2 Generating a random graph for comparison 11.3 The Triad 11.4 Calculating a triad census 11.5 Random graphs galore! 11.6 Producing a tau statistic 11.7 Banning triads", " 11 Transitivity, structural balance, and hierarchy The main goal of this tutorial is to delve more deeply into the microfoundations of networks, such as dyads and tryads. We will learn the basic functions for measuring reciprocity, transitivity, the triad census, and for identifying cliques. We will also learn how to “ban” triads from a graph, and begin to think about whether our graphs deviate, in a statistical sense, from what we might expect by chance, one way of evaluating whether our results are “meaningful”. Through this tutorial, we will rely on igraph to analyze the comm59 Add Health network that we made use of last class. # read in the edge list from our github el &lt;- read.table(&quot;https://raw.githubusercontent.com/mahoffman/stanford_networks/main/data/comm59.dat.txt&quot;, header = T) # Read in attributes from our github attributes &lt;- read.table(&quot;https://raw.githubusercontent.com/mahoffman/stanford_networks/main/data/comm59_att.dat.txt&quot;, header = T) # add an ID column attributes$ID &lt;- 1:nrow(attributes) Next let’s graph it as a network, ignoring the ranking of friendships for now. # First read in igraph library(igraph) # Indexing data so that you only put in certain columns el_no_weight &lt;- el[,1:2] # We will ignore the ranking variable for now. el_no_weight &lt;- as.matrix(el_no_weight) # igraph requires a matrix # convert ids to characters so they are preserved as names el_no_weight[,1] &lt;- as.character(el_no_weight[,1]) el_no_weight[,2] &lt;- as.character(el_no_weight[,2]) # Graph the network net59 &lt;- graph.edgelist(el_no_weight, directed = T) # Finally, add attributes # First link vertex names to their place in the attribute dataset linked_ids &lt;- match(V(net59)$name, attributes$ID) # Then we can use that to assign a variable to each user in the network V(net59)$race &lt;- attributes$race[linked_ids] V(net59)$sex &lt;- attributes$sex[linked_ids] V(net59)$grade &lt;- attributes$grade[linked_ids] V(net59)$school &lt;- attributes$school[linked_ids] net59 # Great! ## IGRAPH c726991 DN-- 975 4160 -- ## + attr: name (v/c), race (v/n), sex (v/n), grade (v/n), school (v/n) ## + edges from c726991 (vertex names): ## [1] 1 -&gt;191 1 -&gt;245 1 -&gt;272 1 -&gt;413 1 -&gt;447 3 -&gt;21 3 -&gt;221 3 -&gt;480 3 -&gt;495 ## [10] 3 -&gt;574 5 -&gt;96 5 -&gt;258 5 -&gt;335 6 -&gt;271 6 -&gt;374 6 -&gt;400 6 -&gt;489 6 -&gt;491 ## [19] 6 -&gt;573 6 -&gt;586 7 -&gt;134 7 -&gt;159 7 -&gt;464 7 -&gt;478 8 -&gt;221 8 -&gt;284 8 -&gt;378 ## [28] 8 -&gt;557 9 -&gt;137 9 -&gt;442 9 -&gt;473 9 -&gt;498 10-&gt;20 10-&gt;22 10-&gt;64 10-&gt;75 ## [37] 10-&gt;89 10-&gt;219 10-&gt;272 10-&gt;276 11-&gt;101 11-&gt;155 11-&gt;190 11-&gt;273 11-&gt;337 ## [46] 11-&gt;339 11-&gt;353 11-&gt;616 12-&gt;188 12-&gt;475 14-&gt;20 14-&gt;151 14-&gt;597 15-&gt;106 ## [55] 15-&gt;189 15-&gt;233 15-&gt;325 15-&gt;333 15-&gt;449 15-&gt;491 15-&gt;552 15-&gt;624 15-&gt;627 ## [64] 16-&gt;30 16-&gt;201 16-&gt;217 16-&gt;267 16-&gt;268 16-&gt;466 16-&gt;569 17-&gt;625 19-&gt;45 ## + ... omitted several edges 11.1 The Dyad We can break large social networks down into their constituent parts. These constituent parts are referred to as “motifs”. The most basic motif consists of two nodes and is called a dyad. Edges in a network signify the presence or absence of dyadic relations. It follows that a dyad in an undirected network can have two unique configurations: connected or disconnected; and three unique configurations in a directed network (mutual, assymetric, and null) Density captures, at the macro-level, the proportion of dyads that are present over the possible total number of dyads in the network. We are simply re-framing what we discussed last class, except we are focusing on the configuration of nodes as opposed to edges. graph.density(net59) ## [1] 0.004380561 A related concept is that of reciprocity, a measure which pertains only to directed graphs. Reciprocity is the tendency with which affect, or network ties, sent out by egos are returned by alters. Edges are reciprocal when ego and alter both send each other ties; reciprocity is the graph-level analogue, evaluating the tendency for edges to be reciprocal across the whole network. reciprocity(net59) ## [1] 0.39375 So our graph has a reciprocity score of 0.39. Is that high or low? It depends on your expectation. If you are from a society with a strong taboo against unrequited affect, then it might seem low. If you come from an individualistic society, it might seem high. 11.2 Generating a random graph for comparison One way network scholars evaluate whether a given descriptive statistic is high or low is to compare it to the value that obtains under a random network of similar density. In random graphs, the chance that any two dyads are in a relation is determined by chance (i.e. the flip of a coin). This means that the likelihood of observing a given tie is independent from observing a tie between any other dyad. There are a lot of reasons that this is unrealistic.. For example, if A and B are friends and A and C are friends, then we would expect the probability that B and C are friends to be higher. That is, you are more likely to be friends with your friends friends than with strangers. This is a basic feature of most social networks and one that was emphasized in our readings this week on transitivity. Even in dyads, if someone shows you affection you are more likely to return it. We therefore might want to factor in this human tendency towards triadic and dyadic closure into our null model - later in the class we will discuss ways of doing this. That said, random graphs have many properties that are mathematically and heuristically useful, which is why they are commonly used as null models. igraph has a fast and easy function for generating random graphs. ?erdos.renyi.game In an erdos.renyi.graph, each edge has the same probability of being created. We determine the probability and it returns a random graph with a density that equals (in expectation) this probability. First, we need to calculate the density and number of nodes in our graph. net59_n &lt;- vcount(net59) net59_density &lt;- graph.density(net59) Then we can input those into the erdos.renyi.game function provided by igraph to generate a network of the same size and density, but with edges that are random re-arranged. random_graph &lt;- erdos.renyi.game(n = net59_n, p.or.m = net59_density, directed = TRUE) # where n is the number of nodes, p.or.m is the probability of drawing an edge, directed is whether the network is directed or not Let’s take a look at the graph. plot(random_graph, vertex.size = 2, vertex.label = NA, edge.curved = .1, vertex.color = &quot;tomato&quot;, edge.arrow.size = .1, edge.width = .5, edge.color = &quot;grey60&quot;) It looks like a bowl of spaghetti - that is how you know you indeed have a random graph on your hands. What is its reciprocity? reciprocity(random_graph) ## [1] 0.005187456 0.003! So our network exhibits far more reciprocity than we would expect if people were affiliating randomly. 11.3 The Triad Triads consist of three nodes and are therefore more complex than dyads, with more possible arrangements. This becomes clear when you realize an undirected triad consists of three dyads: A and B, B and C, and A and C. In an undirected graph, there are eight possible triads (2^3… sort of obvious given that each dyad can be present or absent and there are three dyads). Of those eight possible triads, four are isomorphic, so that there are four unique triads. In an directed graph, there will be sixteen unique triads (the unique number of motifs in directed graphs is not easily reduced to a functional form). These sixteen unique triads give rise to the MAN framework as well as the triad census, which we discussed in class. The triad census calculates how many triads there are of each type (which, as I just mentioned, in a directed network amounts to 16). If we see a network with very few complete (003) triads, then we know something about the macro-level structure, just by looking at the frequencies of its constituent parts at the micro-level. By extension, if the whole distribution of triads is very different than the distribution that obtains under a random network, then we hopefully learn something about the macro-level structure that we couldn’t observe just by looking at a visualization of the network. 11.4 Calculating a triad census igraph has a built in function for the triad census: triad.census() It takes a network object as an argument and returns the number of each type of triad in the network. triad.census(net59) ## [1] 150776065 2416731 785249 3621 4325 5165 4028 ## [8] 3619 450 23 1019 294 320 137 ## [15] 385 144 As you can see, it returns 16 different numbers. It uses the M-A-N classification structure: M stands for the number of Mutual ties, A stands for the number of Asymmetric ties, and N stands for the number of Null ties. Mutual means that ego and alter (say A and B) have a mutual relation with each other (A likes B and B likes A). A means that ego and alter have an asymmetric relation with one another, i.e. A likes B but B doesn’t like A… the relation is not reciprocated. Finally, N means that A and B do not have any relation. The image below visualizes the different types of triads possible in a directed graph. ?triad.census also describes these possible types, telling you the order that the triad types are presented in the triad.census() output. 003 for example means there are 0 mutual relation, 0 asymmetric relations and 3 null relations. This triad contains no relations. 201 would mean there are two mutual relations, zero asymmetric relations, and 1 null relation. Of three dyads in the triad, two dyads are in a relation, and one dyad is not. For now let’s look at the triad census of our random graph. triad.census(random_graph) ## [1] 149922316 4032304 10523 9164 8976 18013 94 ## [8] 86 79 20 0 0 0 0 ## [15] 0 0 Most triads are null. This is because edges only have a 5% chance of being drawn. It follows that 95% of the total possible edges are missing. There are not many cases of transitive triads, or even in which all three dyads have a relationship. Thus, the majority of triads are concentrated in the left side of the triad distribution Let’s compare it our Add Health network. triad.census(net59) ## [1] 150776065 2416731 785249 3621 4325 5165 4028 ## [8] 3619 450 23 1019 294 320 137 ## [15] 385 144 Which triads are common in our network, but not in the random graph? What might this tell us? 11.5 Random graphs galore! One problem is that we are basing our analysis on a single random graph. Because edges are drawn randomly, there is a lot of variation in the structure of random graphs, especially when the number of nodes in the graph is small (less than one thousand). What we really want is a distribution of random graphs and their triad censuses, against which our own could be compared. So let’s generate one hundred random graphs, and create a distribution of random graph triad censuses and see where our graph lies on that distribution trial &lt;- vector(&quot;list&quot;, 100) # this creates a list with 100 spaces to store things. We will store each result here. for ( i in 1:length(trial) ){ random_graph &lt;- erdos.renyi.game(n = net59_n, p.or.m = net59_density, directed = TRUE) trial[[i]] &lt;- triad.census(random_graph) # We assign to the ith space the result. So for the first iteration, it will assign the result to the first space in the list } trial_df &lt;- do.call(&quot;rbind&quot;, trial) # We can use the do.call and &quot;rbind&quot; functions together to combine all of the results into a matrix, where each row is one of our trials colnames(trial_df) &lt;- c(&quot;003&quot;, &quot;012&quot;, &quot;102&quot;, &quot;021D&quot;, &quot;021U&quot;, &quot;021C&quot;, &quot;111D&quot;, &quot;111U&quot;, &quot;030T&quot;, &quot;030C&quot;, &quot;201&quot;, &quot;120D&quot;, &quot;120U&quot;, &quot;120C&quot;, &quot;210&quot;, &quot;300&quot;) # It is worth naming the columns too. trial_df_w_observed &lt;- rbind(trial_df, as.numeric(triad.census(net59))) # add in the observed results Now we have this 100 row dataset of simulation results with the observed results tacked on at the end (of course, we could have done 1,000 or 10,000 iterations!) Let’s produce, for each column, some simple statistics, like a mean and a confidence interval. # First, standardize all of the columns by dividing each of their values by the largest value in that column, so that each will be on a similar scale (0 to 1), we can visualize them meaningfully trial_df_w_observed &lt;- as.data.frame(trial_df_w_observed) trial_df_w_observed[,1:ncol(trial_df_w_observed)] &lt;- sapply(trial_df_w_observed[,1:length(trial_df_w_observed)], function(x) x/max(x)) # Then split the observed from the simulation results trial_df &lt;- as.data.frame(trial_df_w_observed[1:100,]) observed &lt;- as.numeric(trial_df_w_observed[101,]) # Summarize the simulation results and add the observed data set back in for comparison summarized_stats &lt;- data.frame(TriadType = colnames(trial_df), Means = sapply(trial_df, mean), LowerCI = sapply(trial_df, function(x) quantile(x, 0.05)), UpperCI = sapply(trial_df, function(x) quantile(x, 0.95)), Observed = observed) summarized_stats ## TriadType Means LowerCI UpperCI Observed ## 003 003 0.9948112794 0.994230237 0.9953703278 1.0000000 ## 012 012 0.9693179524 0.949169986 0.9900488583 0.5908888 ## 102 102 0.0103958999 0.004860879 0.0169992576 1.0000000 ## 021D 021D 0.9485477043 0.903033757 0.9921903832 0.3930316 ## 021U 021U 0.9345077055 0.891732663 0.9837596318 0.4628639 ## 021C 021C 0.9424454761 0.905085295 0.9888388037 0.2788275 ## 111D 111D 0.0181429990 0.007696127 0.0285129096 1.0000000 ## 111U 111U 0.0199281569 0.009381044 0.0317905499 1.0000000 ## 030T 030T 0.1721555556 0.135444444 0.2044444444 1.0000000 ## 030C 030C 0.6571794872 0.485897436 0.8987179487 0.5897436 ## 201 201 0.0001570167 0.000000000 0.0009813543 1.0000000 ## 120D 120D 0.0005102041 0.000000000 0.0034013605 1.0000000 ## 120U 120U 0.0002812500 0.000000000 0.0031250000 1.0000000 ## 120C 120C 0.0021167883 0.000000000 0.0072992701 1.0000000 ## 210 210 0.0000000000 0.000000000 0.0000000000 1.0000000 ## 300 300 0.0000000000 0.000000000 0.0000000000 1.0000000 Now that we have this dataset, we can use ggplot to plot it. library(ggplot2) ggplot(summarized_stats) + geom_point(aes(x=TriadType, y=Means, colour=TriadType)) + geom_errorbar(aes(x = TriadType, ymin=LowerCI, ymax=UpperCI, colour=TriadType), width=.1) + geom_point(aes(x=TriadType, y=Observed, colour=&quot;Observed&quot;)) + coord_flip() Beautiful! What do you see? Which triads are more or less common in our graph than in a random graph? What might this tell us about the macro-structure? 11.6 Producing a tau statistic Given a weighting scheme, which values some triads and not others, we can evaluate whether our network fits a macrolevel model of our choice. For example, below is a weighting scheme for the ranked-clustering weighting scheme, drawn from Daniel A. McFarland, et al. “Network Ecology and Adolescent Social Structure”. We can compare this to that produced by a random graph to see if our tau count is more or less than we should expect by chance. weighting_scheme &lt;- c(0,0,0,1,1,-1,0,0,1,0,0,1,1,0,0,0) sum(triad.census(net59) * weighting_scheme) ## [1] 3845 11.7 Banning triads What if we want to ban some triads, but allow others? How might we go about building a simulation that does that? Below is an example. The basic idea is to sample nodes, draw an edge randomly with some probability (i.e. to control density), and then choose to keep or delete that edge, depending on if the triangles that it produces are allowed or not. # A basic function which prevents the formation of specified triads in a random graph simulation banning_triads_game = function(n = 100, porm = .05, banned = c(2), sim_max = 1000000, probrecip = .5){ require(igraph) # Ensures igraph is loaded if(any(c(1) %in% banned)){ stop(&quot;Can&#39;t ban 003s&quot;) # Stops the simulation if the user tried to bad 003 or 012 triads } num_edges = round(n*(n-1)*porm, 2) # calculates the desired number of edges according to the N and Porm parameters net = make_empty_graph(n = n, directed = TRUE) # initializes an empty network edge_count = 0 sim_count = 0 while(edge_count &lt; num_edges){ # Begins a loop, which ends once the requisite number of edges is reached. # This part samples two nodes, checks whether the two sampled nodes are the same node, and whether an edge is already present in the network between these nodes uniq = TRUE edge_present = TRUE while(uniq == TRUE | edge_present == TRUE){ edge_id = sample(1:n, 2, replace = T) uniq = edge_id[1] == edge_id[2] reciprocated = sample(c(FALSE, TRUE), 1, prob = c(1-probrecip, probrecip)) edge_present_1 = are.connected(net, edge_id[1], edge_id[2]) if (reciprocated){ edge_present_2 = are.connected(net, edge_id[2], edge_id[1]) edge_present = edge_present_1|edge_present_2 } else { edge_present = edge_present_1 } } # Calculates the traid census for the network before adding an edge before = triad.census(net) net_new = net + edge(edge_id) # Adds in the edge if(reciprocated){ edge_id_rev = edge_id[2:1] net_new = net_new + edge(edge_id_rev) # Adds in the edge } after = triad.census(net_new) # Calculates the triad census again triad_diff = after - before # Checks to see how much the triad census changed if(all(triad_diff[banned] == 0)){ net = net_new # If the banned triads still aren&#39;t observed, then the new network is accepted. } edge_count = ecount(net) # number of edges updated sim_count = sim_count + 1 # Simulation count updated if(sim_count &gt; sim_max){ print(&quot;Warning: Failed to converge, banned triads may be incompatible&quot;) # exits simulation if simulation max count is exceeded return(net) } } return(net) # Returns the simulated network } Cool - let’s try it. Here, we ban three triad types. no_cycles = banning_triads_game(banned = c(4,5,7)) triad_census(no_cycles) ## [1] 137110 3721 19259 0 0 17 0 147 0 1 ## [11] 1382 0 2 1 9 51 plot(no_cycles, vertex.size = 2, vertex.label = NA, vertex.color = &quot;tomato&quot;, edge.arrow.size = .2) "],["centrality.html", "12 Centrality 12.1 Eigenvector Centrality 12.2 Bonacich Centrality 12.3 Page Rank 12.4 Distance weighted reach", " 12 Centrality In this tutorial, we look at measures of network centrality, which we use to identify structurally important actors. We also discuss possible ideas for identifying important edges. Centrality originally referred to how central actors are in a network’s structure. It has become abstracted as a term from its topological origins and now refers very generally to how important actors are to a network. Topological centrality has a clear definition, but many operationalizations. Network “importance” on the other hand has many definitions and many operationalizations. We will explore the possible meanings and operationalizations of centrality here. There are four well-known centrality measures: degree, betweenness, closeness and eigenvector - each with its own strengths and weaknesses. The main point we want to make is that the analytical usefulness of each depends heavily on the context of the network, the type of relation being analyzed and the underlying network morphology. We don’t want to leave you with the impression that one is better than another - only that one might serve your research goals better than another. Every node-level measure has its graph-level analogue. Centralization measures the extent to which the ties of a given network are concentrated on a single actor or group of actors. We can also look at the degree distribution. It is a simple histogram of degree, which tells you whether the network is highly unequal or not. 12.0.1 Loading the example network As always, we need to load igraph. We will load in tidyverse too in case we need to do some data munging or plotting. library(igraph) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ── ## ✓ tibble 3.1.3 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 2.0.0 ✓ forcats 0.5.1 ## ✓ purrr 0.3.4 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::as_data_frame() masks tibble::as_data_frame(), igraph::as_data_frame() ## x purrr::compose() masks igraph::compose() ## x tidyr::crossing() masks igraph::crossing() ## x dplyr::filter() masks stats::filter() ## x dplyr::groups() masks igraph::groups() ## x dplyr::lag() masks stats::lag() ## x purrr::simplify() masks igraph::simplify() library(reshape2) We will use John Padgett’s Florentine Families dataset. It is part of a famous historical datset about the relationships of prominent Florentine families in 15th century Italy. The historical puzzle is how the Medici, an upstart family, managed to accumulate political power during this period. Padgett’s goal was to explain their rise. He looked at many relations. On the github, we have access to marriage, credit, and business partnership ties, but we will focus on marriage for now. Marriage was a tool in diplomacy, central to political alliances. A tie is drawn between families if the daughter of one family was sent to marry the son of another. Ron Breiger, who analyzed these data in a famous paper on local role analysis, has argued these edges should be directed, tracing where daughters were sent or where finances flowed. As always, we first load and prepare the dataset. We will do so directly from GitHub again. # prepare the marriage adjacency matrix florentine_edj &lt;- read.csv(&quot;https://raw.githubusercontent.com/mahoffman/stanford_networks/main/data/florentine_marriage_edgelist.csv&quot;) florentine_edj &lt;- florentine_edj[,2:3] # prepare the attributes file florentine_attributes &lt;- read.csv(&quot;https://raw.githubusercontent.com/mahoffman/stanford_networks/main/data/florentine_attributes.csv&quot;) # graph the marriage network marriageNet &lt;- graph.edgelist(as.matrix(florentine_edj), directed = T) V(marriageNet)$Wealth &lt;- florentine_attributes$Gwealth[match(V(marriageNet)$name, florentine_attributes$Family)] # Gross wealth (Florins), for 87 (92) families # simple mean imputation of wealth (alternatively, we might think that those with NA were too poor to show up in historical records?) V(marriageNet)$Wealth &lt;- ifelse(is.na(V(marriageNet)$Wealth), mean(V(marriageNet)$Wealth, na.rm = T), V(marriageNet)$Wealth) # Number of Priors, The Priorate (or city council), first created in 1282, was Florence&#39;s governing body. Count of how many seats a family had on that city council from 1282-1344 # measure of the aggregate political influence of the family over a long period of time V(marriageNet)$Priorates &lt;- florentine_attributes$Npriors[match(V(marriageNet)$name, florentine_attributes$Family)] Let’s see how it looks. plot(marriageNet, vertex.size = 8, vertex.label.cex = .4, vertex.label.color = &quot;black&quot;, vertex.color = &quot;tomato&quot;, edge.arrow.size = 0.4) Based on this plot, which family do you expect is most central? 12.0.2 Degree Centrality The simplest measure of centrality is degree centrality. It counts how many edges each node has - the most degree central actor is the one with the most ties. Note: In a directed network, you will need to specify if in or out ties should be counted. These will be referred to as in or out degree respectively. If both are counted, then it is just called degree Degree centrality is calculated using the degree function in R. It returns how many edges each node has. degree(marriageNet) ## Acciaiuoli Guicciardini Medici Adimari Arrigucci ## 4 14 40 6 2 ## Barbadori Strozzi Albizzi Altoviti Della_Casa ## 14 50 28 12 4 ## Corsi Davanzati Frescobaldi Ginori Guadagni ## 2 6 12 8 20 ## Guasconi Nerli Del_Palagio Panciatichi Scolari ## 24 2 4 14 6 ## Aldobrandini Alessandri Tanagli Bencivenni Gianfigliazzi ## 4 2 4 2 20 ## Spini Dall&#39;Antella Martelli Rondinelli Ardinghelli ## 8 6 4 10 10 ## Peruzzi Rossi Arrighi Baldovinetti Ciampegli ## 30 4 2 8 2 ## Manelli Carducci Castellani Ricasoli Bardi ## 4 4 14 20 14 ## Bucelli Serragli Da_Uzzano Baroncelli Raugi ## 4 4 4 2 2 ## Baronci Manovelli Bartoli Solosmei Bartolini ## 2 4 2 2 2 ## Belfradelli Del_Benino Donati Benizzi Bischeri ## 2 2 4 2 8 ## Brancacci Capponi Nasi(?) Sacchetti Vettori ## 4 8 2 6 2 ## Doffi Pepi Cavalcanti Ciai Corbinelli ## 2 2 6 2 2 ## Lapi Orlandini Dietisalvi Valori Federighi ## 2 4 2 8 2 ## Dello_Scarfa Fioravanti Salviati Giugni Pandolfini ## 2 4 6 6 4 ## Lamberteschi Tornabuoni Mancini Di_Ser_Segna Macinghi ## 2 8 2 2 2 ## Masi Pecori Pitti Popoleschi Portinari ## 2 2 4 4 2 ## Ridolfi Serristori Vecchietti Minerbetti Pucci ## 8 2 2 2 2 ## Da_Panzano Parenti Pazzi Rucellai Scambrilla ## 4 2 4 2 2 ## Soderini ## 2 Who is the most degree central? We can assign the output to a variable in the network and size the nodes according to degree. V(marriageNet)$degree &lt;- degree(marriageNet) # assignment plot(marriageNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, vertex.size = V(marriageNet)$degree, vertex.label.cex = .2) # sized by degree The problem is that the degree values are a little small to plot well. We can use a scalar to increase the value of the degree but maintain the ratio. plot(marriageNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, vertex.size = V(marriageNet)$degree*3) 12.0.3 Betweenness Centrality Betweenness centrality captures which nodes are important in the flow of the network. It makes use of the shortest paths in the network. A path is a series of adjacent nodes. For any two nodes we can find the shortest path between them, that is, the path with the least amount of total steps (or edges). If a node C is on a shortest path between A and B, then it means C is important to the efficient flow of goods between A and B. Without C, flows would have to take a longer route to get from A to B. Thus, betweenness effectively counts how many shortest paths each node is on. The higher a node’s betweenness, the more important they are for the efficient flow of goods in a network. In igraph, betweenness() computes betweenness in the network betweenness(marriageNet, directed = FALSE) ## Acciaiuoli Guicciardini Medici Adimari Arrigucci ## 0.000000 327.850305 1029.609288 93.009524 0.000000 ## Barbadori Strozzi Albizzi Altoviti Della_Casa ## 162.697344 1369.979110 856.436111 125.620147 11.577398 ## Corsi Davanzati Frescobaldi Ginori Guadagni ## 0.000000 260.000000 145.532681 180.571429 277.059921 ## Guasconi Nerli Del_Palagio Panciatichi Scolari ## 583.679251 0.000000 0.000000 167.994891 19.801010 ## Aldobrandini Alessandri Tanagli Bencivenni Gianfigliazzi ## 23.351190 0.000000 88.000000 0.000000 187.915043 ## Spini Dall&#39;Antella Martelli Rondinelli Ardinghelli ## 89.500000 174.000000 88.000000 43.186597 58.278211 ## Peruzzi Rossi Arrighi Baldovinetti Ciampegli ## 604.369691 0.000000 0.000000 95.613889 0.000000 ## Manelli Carducci Castellani Ricasoli Bardi ## 0.000000 5.009524 194.199423 205.097092 280.248232 ## Bucelli Serragli Da_Uzzano Baroncelli Raugi ## 1.066667 81.000000 3.666667 0.000000 0.000000 ## Baronci Manovelli Bartoli Solosmei Bartolini ## 0.000000 88.000000 0.000000 0.000000 0.000000 ## Belfradelli Del_Benino Donati Benizzi Bischeri ## 0.000000 0.000000 88.000000 0.000000 63.995238 ## Brancacci Capponi Nasi(?) Sacchetti Vettori ## 0.000000 177.000000 0.000000 197.651515 0.000000 ## Doffi Pepi Cavalcanti Ciai Corbinelli ## 0.000000 0.000000 125.467749 0.000000 0.000000 ## Lapi Orlandini Dietisalvi Valori Federighi ## 0.000000 88.000000 0.000000 202.285859 0.000000 ## Dello_Scarfa Fioravanti Salviati Giugni Pandolfini ## 0.000000 5.951190 35.571429 96.664141 0.000000 ## Lamberteschi Tornabuoni Mancini Di_Ser_Segna Macinghi ## 0.000000 23.831746 0.000000 0.000000 0.000000 ## Masi Pecori Pitti Popoleschi Portinari ## 0.000000 0.000000 19.610606 0.000000 0.000000 ## Ridolfi Serristori Vecchietti Minerbetti Pucci ## 213.727670 0.000000 0.000000 0.000000 0.000000 ## Da_Panzano Parenti Pazzi Rucellai Scambrilla ## 16.961111 0.000000 4.361111 0.000000 0.000000 ## Soderini ## 0.000000 We can again assign the output of betweenness() to a variable in the network and size the nodes according to it. V(marriageNet)$betweenness &lt;- betweenness(marriageNet, directed = F) # assignment plot(marriageNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, vertex.size = V(marriageNet)$betweenness) # sized by betweenness Betweenness centrality can be very large. It is often helpful to normalize it by dividing by the maximum and multiplying by some scalar when plotting. plot(marriageNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, vertex.size = V(marriageNet)$betweenness/max(V(marriageNet)$betweenness) * 20) 12.0.4 Closeness Centrality With closeness centrality we again make use of the shortest paths between nodes. We measure the distance between two nodes as the length of the shortest path between them. Farness, for a given node, is the average distance from that node to all other nodes. Closeness is then the reciprocal of farness (1/farness). closeness(marriageNet) ## Warning in closeness(marriageNet): At centrality.c:2784 :closeness centrality is ## not well-defined for disconnected graphs ## Acciaiuoli Guicciardini Medici Adimari Arrigucci ## 0.0011723329 0.0012468828 0.0012836970 0.0011961722 0.0010822511 ## Barbadori Strozzi Albizzi Altoviti Della_Casa ## 0.0012210012 0.0012970169 0.0012658228 0.0012165450 0.0011792453 ## Corsi Davanzati Frescobaldi Ginori Guadagni ## 0.0011389522 0.0011467890 0.0012180268 0.0011737089 0.0012254902 ## Guasconi Nerli Del_Palagio Panciatichi Scolari ## 0.0012903226 0.0011389522 0.0011834320 0.0012562814 0.0011834320 ## Aldobrandini Alessandri Tanagli Bencivenni Gianfigliazzi ## 0.0011834320 0.0010141988 0.0011135857 0.0010989011 0.0012690355 ## Spini Dall&#39;Antella Martelli Rondinelli Ardinghelli ## 0.0011792453 0.0011709602 0.0010638298 0.0012195122 0.0012330456 ## Peruzzi Rossi Arrighi Baldovinetti Ciampegli ## 0.0012722646 0.0011534025 0.0011061947 0.0012091898 0.0010928962 ## Manelli Carducci Castellani Ricasoli Bardi ## 0.0011668611 0.0011933174 0.0012254902 0.0012269939 0.0012531328 ## Bucelli Serragli Da_Uzzano Baroncelli Raugi ## 0.0011627907 0.0011389522 0.0011737089 0.0001108033 0.0001108033 ## Baronci Manovelli Bartoli Solosmei Bartolini ## 0.0010416667 0.0011467890 0.0001108033 0.0001108033 0.0009727626 ## Belfradelli Del_Benino Donati Benizzi Bischeri ## 0.0011001100 0.0009469697 0.0010330579 0.0011441648 0.0012360939 ## Brancacci Capponi Nasi(?) Sacchetti Vettori ## 0.0012048193 0.0010976948 0.0010010010 0.0011806375 0.0010010010 ## Doffi Pepi Cavalcanti Ciai Corbinelli ## 0.0011061947 0.0011061947 0.0012285012 0.0010638298 0.0011534025 ## Lapi Orlandini Dietisalvi Valori Federighi ## 0.0010416667 0.0010438413 0.0010638298 0.0011337868 0.0001108033 ## Dello_Scarfa Fioravanti Salviati Giugni Pandolfini ## 0.0001108033 0.0011682243 0.0011723329 0.0011641444 0.0010718114 ## Lamberteschi Tornabuoni Mancini Di_Ser_Segna Macinghi ## 0.0011061947 0.0011890606 0.0011587486 0.0011587486 0.0011641444 ## Masi Pecori Pitti Popoleschi Portinari ## 0.0011441648 0.0011534025 0.0012195122 0.0011587486 0.0011534025 ## Ridolfi Serristori Vecchietti Minerbetti Pucci ## 0.0012315271 0.0011534025 0.0011534025 0.0011641444 0.0009560229 ## Da_Panzano Parenti Pazzi Rucellai Scambrilla ## 0.0011389522 0.0011641444 0.0011337868 0.0011641444 0.0010683761 ## Soderini ## 0.0011641444 We assign it to a node variable and plot the network, adjusting node size by closeness. V(marriageNet)$closeness &lt;- closeness(marriageNet) ## Warning in closeness(marriageNet): At centrality.c:2784 :closeness centrality is ## not well-defined for disconnected graphs plot(marriageNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, vertex.size = V(marriageNet)$closeness/max(V(marriageNet)$closeness) * 20) 12.1 Eigenvector Centrality Degree centrality only takes into account the number of edges for each node, but it leaves out information about ego’s alters. However, we might think that power comes from being tied to powerful people. If A and B have the same degree centrality, but A is tied to all high degree people and B is tied to all low degree people, then intuitively we want to see A with a higher score than B. Eigenvector centrality takes into account alters’ power. It is calculated a little bit differently in igraph. It produces a list object and we need to extract only the vector of centrality values. evcent(marriageNet)$vector ## Acciaiuoli Guicciardini Medici Adimari Arrigucci ## 1.399699e-01 3.830752e-01 6.707160e-01 1.831812e-01 2.433106e-02 ## Barbadori Strozzi Albizzi Altoviti Della_Casa ## 3.547849e-01 1.000000e+00 5.323660e-01 3.372469e-01 1.414314e-01 ## Corsi Davanzati Frescobaldi Ginori Guadagni ## 7.071156e-02 7.332195e-02 3.436670e-01 1.057862e-01 3.460880e-01 ## Guasconi Nerli Del_Palagio Panciatichi Scolari ## 6.811084e-01 7.071156e-02 1.383317e-01 5.090917e-01 1.367356e-01 ## Aldobrandini Alessandri Tanagli Bencivenni Gianfigliazzi ## 1.509870e-01 4.503879e-03 3.390835e-02 4.479485e-02 7.162939e-01 ## Spini Dall&#39;Antella Martelli Rondinelli Ardinghelli ## 2.096823e-01 1.893404e-01 2.560082e-02 3.998861e-01 4.000982e-01 ## Peruzzi Rossi Arrighi Baldovinetti Ciampegli ## 7.915554e-01 1.582815e-01 4.596917e-02 2.497482e-01 3.317283e-02 ## Manelli Carducci Castellani Ricasoli Bardi ## 1.659979e-01 1.799494e-01 4.238792e-01 5.324287e-01 4.080706e-01 ## Bucelli Serragli Da_Uzzano Baroncelli Raugi ## 1.218222e-01 5.798025e-02 1.593404e-01 1.757355e-17 2.919646e-18 ## Baronci Manovelli Bartoli Solosmei Bartolini ## 1.421582e-02 1.070266e-01 4.206220e-18 4.255001e-19 3.400431e-03 ## Belfradelli Del_Benino Donati Benizzi Bischeri ## 4.564759e-02 1.231075e-03 9.268390e-03 1.051384e-01 3.348147e-01 ## Brancacci Capponi Nasi(?) Sacchetti Vettori ## 2.232934e-01 2.844521e-02 3.778238e-03 1.486187e-01 3.778238e-03 ## Doffi Pepi Cavalcanti Ciai Corbinelli ## 5.630179e-02 5.630179e-02 2.359640e-01 1.405106e-02 8.908790e-02 ## Lapi Orlandini Dietisalvi Valori Federighi ## 9.738994e-03 9.913900e-03 1.405106e-02 6.854784e-02 1.875726e-17 ## Dello_Scarfa Fioravanti Salviati Giugni Pandolfini ## 8.288370e-18 1.350571e-01 1.467173e-01 1.011873e-01 2.254509e-02 ## Lamberteschi Tornabuoni Mancini Di_Ser_Segna Macinghi ## 4.596917e-02 1.834366e-01 9.046827e-02 9.046827e-02 1.328251e-01 ## Masi Pecori Pitti Popoleschi Portinari ## 1.051384e-01 8.908790e-02 2.219130e-01 1.134529e-01 8.908790e-02 ## Ridolfi Serristori Vecchietti Minerbetti Pucci ## 2.507818e-01 8.908790e-02 8.908790e-02 1.328251e-01 1.316815e-03 ## Da_Panzano Parenti Pazzi Rucellai Scambrilla ## 9.046017e-02 1.328251e-01 9.020762e-02 1.328251e-01 2.785107e-02 ## Soderini ## 1.328251e-01 Then we can assign that vector to our network and plot it. V(marriageNet)$eigenvector &lt;- evcent(marriageNet)$vector plot(marriageNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, vertex.size = V(marriageNet)$eigenvector/max(V(marriageNet)$eigenvector) * 20) 12.2 Bonacich Centrality Perhaps marrying your daughters off to weaker families is a good way to ensure their loyalty? We could evaluate this using bonacich centrality. From igraph: “Interpretively, the Bonacich power measure corresponds to the notion that the power of a vertex is recursively defined by the sum of the power of its alters. The nature of the recursion involved is then controlled by the power exponent: positive values imply that vertices become more powerful as their alters become more powerful (as occurs in cooperative relations), while negative values imply that vertices become more powerful only as their alters become weaker (as occurs in competitive or antagonistic relations).” V(marriageNet)$bonacich &lt;- power_centrality(marriageNet, exponent = -2, rescale = T) V(marriageNet)$bonacich &lt;- ifelse(V(marriageNet)$bonacich &lt; 0, 0, V(marriageNet)$bonacich) plot(marriageNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, vertex.size = V(marriageNet)$bonacich/max(V(marriageNet)$bonacich) * 20) 12.3 Page Rank Here is Google’s page rank measure. It uses random walks to identify individuals who are commonly encountered along such walks. Those individuals are viewed as central. V(marriageNet)$page_rank &lt;- page_rank(marriageNet, directed = TRUE)$vector plot(marriageNet, vertex.label.cex = .6, vertex.label.color = &quot;black&quot;, vertex.size = V(marriageNet)$page_rank/max(V(marriageNet)$page_rank) * 20) 12.3.1 Measure Correlations Most of these measures are highly correlated, meaning they don’t necessarily capture unique aspects of pwoer. However, the amount of correlation depends on the network structure. Let’s see how the correlations between centrality measures looks in the Florentine Family network. cor.test(x,y) performs a simple correlation test between two vectors. # extract all the vertex attributes all_atts &lt;- lapply(list.vertex.attributes(marriageNet),function(x) get.vertex.attribute(marriageNet,x)) # bind them into a matrix all_atts &lt;- do.call(&quot;cbind&quot;, all_atts) # add column nams colnames(all_atts) &lt;- list.vertex.attributes(marriageNet) # drop the family variable all_atts &lt;- data.frame(all_atts[,2:ncol(all_atts)]) # convert all to numeric all_atts &lt;- sapply(all_atts, as.numeric) # produce a correlation matrix cormat &lt;- cor(all_atts) # melt it using reshape to function melt() to prepare it for ggplot which requires long form data melted_cormat &lt;- melt(cormat) ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + geom_tile() + scale_fill_distiller(palette = &quot;Spectral&quot;, direction=-2) + xlab(&quot;&quot;) + ylab(&quot;&quot;) What do we learn? 12.3.2 Centralization and Degree Distributions To understand the measures further, we can look at their distributions. This will tell us roughly how many nodes have centralities of a given value. # fitting a degree distribution on the log-log scale alter_hist = table(degree(marriageNet)) vals = as.numeric(names(alter_hist)) vals = vals[2:length(vals)] alter_hist = alter_hist[2:length(alter_hist)] df = data.frame(Vals = log(vals), Hist = log(as.numeric(alter_hist)), stringsAsFactors = F) # plot log-log degree distribution plot(Hist ~ Vals, data = df) # regression line abline(lm(Hist ~ Vals, data = df)) Do their marriage partners have more marriage partners than they do? # degrees of your friends neighbor_degrees &lt;- knn(marriageNet)$knn degrees &lt;- degree(marriageNet) mean(neighbor_degrees, na.rm = T) ## [1] 18.72979 mean(degrees) ## [1] 6.541667 # plot neighbor degrees vs. ego degress hist(neighbor_degrees) hist(degrees) We can see that most nodes in the marriage network have low betweenness centrality, and only one node has more than 40 betweenness. Degree distributions tend to be right-skewed; that is, only a few nodes in most networks have most of the ties. Evenly distributed degree is much rarer. Finally centralization measures the extent to which a network is centered around a single node. The closer a network gets to looking like a star, the higher the centralization score will be. degcent &lt;- centralization.degree(marriageNet)$centralization centralization.betweenness(marriageNet)$centralization ## [1] 0.2881759 centralization.evcent(marriageNet)$centralization ## [1] 0.8503616 centralization.closeness(marriageNet)$centralization ## Warning in centralization.closeness(marriageNet): At ## centrality.c:2784 :closeness centrality is not well-defined for disconnected ## graphs ## [1] 0.02030026 Can we compare our centralization scores against some baseline? Here is an example with Barabasi-Albert model, which simulates a network in which there is preferential attachment with respect to degree, the amount of which is controlled by the power parameter. N &lt;- vcount(marriageNet) degcent &lt;- centralization.degree(marriageNet)$centralization centralizations = c() powers &lt;- seq(from = 0, to = 3, by = 0.1) for(e in powers){ net &lt;- barabasi.game(N, directed = F, power=e) centralizations &lt;- c(centralizations, centralization.degree(net)$centralization) } power_df &lt;- data.frame(Centralization = centralizations, Power = powers) ggplot(power_df, aes(x = Power, y = Centralization)) + geom_point() + geom_hline(yintercept = degcent, linetype=&quot;dashed&quot;, color = &quot;red&quot;) + theme_bw() ## Reach N What proportion of nodes can any node reach at N steps? reach_n =function(x, n = 2){ r=vector(length=vcount(x)) for (i in 1:vcount(x)){ neighb =neighborhood(x, n, nodes=i) ni=unlist(neighb) l=length(ni) r[i]=(l)/vcount(x) } return(r) } two_reach = reach_n(marriageNet, 2) plot(marriageNet, vertex.size = two_reach * 10, vertex.label.cex = .4, vertex.label.color = &quot;black&quot;, vertex.color = &quot;tomato&quot;) three_reach = reach_n(marriageNet, 3) plot(marriageNet, vertex.size = three_reach * 10, vertex.label.cex = .4, vertex.label.color = &quot;black&quot;, vertex.color = &quot;tomato&quot;) four_reach = reach_n(marriageNet, 4) plot(marriageNet, vertex.size = four_reach * 10, vertex.label.cex = .4, vertex.label.color = &quot;black&quot;, vertex.color = &quot;tomato&quot;) five_reach = reach_n(marriageNet, 5) plot(marriageNet, vertex.size = five_reach * 10, vertex.label.cex = .4, vertex.label.color = &quot;black&quot;, vertex.color = &quot;tomato&quot;) 12.4 Distance weighted reach distance_weighted_reach=function(x){ distances=shortest.paths(x) #create matrix of geodesic distances diag(distances)=1 # replace the diagonal with 1s weights=1/distances # take the reciprocal of distances return(apply(weights,1,sum)) # sum for each node (row) } dw_reach = distance_weighted_reach(marriageNet) dw_reach = dw_reach/max(dw_reach) plot(marriageNet, vertex.size = dw_reach * 10, vertex.label.cex = .4, vertex.label.color = &quot;black&quot;, vertex.color = &quot;tomato&quot;) "],["bridges-holes-the-small-world-problem-and-simulation.html", "13 Bridges, Holes, the Small World Problem, and Simulation 13.1 It’s a small world after all. 13.2 Measuring connectivity of networks", " 13 Bridges, Holes, the Small World Problem, and Simulation Following our discussion, this week’s lab will focus on connectivity, bridging ties, wormholes, structural holes, and the small world problem. First, we will replicate Watts and Strogatz’s small world simulation. We will then analyze simulated small world graphs using igraph’s functions for measuring connectivity and constraint, and identifying bridging ties and articulation points (nodes whose removal would reduce the connectivity of a graph). We will conclude by looking at a couple of other simulations available in igraph. 13.1 It’s a small world after all. Real-world social networks tend to be small worlds. In a small world, people are clustered in groups, but despite this, are still, on average, socially proximate. For example, you might think that you are socially (and spatially) distant from a random villager in India, but find that through a series of steps, you could reach that villager. The villager lives in her own small world and you live in yours, and yet you are mutually reachable. This is referred to as “the Small-World Phenomenon”. Duncan Watts in his landmark paper explains this phenomenon. He begins with most clustered (and yet connected graph) imaginable - a “caveman” structure. There are groups of people clustered together and connected by only one or two connections to other groups. Sadly, igraph doesn’t have a function for simulating caveman structures, so we will make use of one I wrote. In this caveman structure, all of the groups will be the same size, so the number of people must be evenly divisible by the size of groups. The basic idea is to generate a bunch of fully connected groups and then connect them by an edge or two so that they are arrayed around a circle. simulate_caveman &lt;- function(n = 25, clique_size = 5){ require(igraph) # Groups are all the same size, so I check whether N is divisible by the size of groups if ( ((n%/%clique_size) * clique_size) != n){ stop(&quot;n is not evenly divisible by clique_size&quot;) } groups = n/clique_size # this determines the number of groups el &lt;- data.frame(PersonA = 1:n, Group = NA) # I create a dataframe which has people and the groups they are in # I treat it like a person to group edgelist group_vector = c() for (i in 1:groups){ group_vector &lt;- c(group_vector, rep(i, clique_size)) } el$Group &lt;- group_vector inc &lt;- table(el) # I use the table function to turn the person to group edgelist into an incidence matrix adj &lt;- inc %*% t(inc) # And I use matrix multiplication with the transpose to turn the person to group incidence matrix # into a person to person adjacency matrix diag(adj) &lt;- 0 g &lt;- graph.adjacency(adj, mode = &quot;undirected&quot;) # I graph this matrix group_connect &lt;- seq(from = 1, to = n, by = clique_size) # I determine the points of connection using a sequence function for( i in 1:(length(group_connect)-1)){ p1 &lt;- group_connect[i] + 1 p2 &lt;- group_connect[i+1] g &lt;- add.edges(g, c(p1,p2)) # And I connect the points of connection using add.edges } g &lt;- add.edges(g, c(group_connect[1],(group_connect[groups]+1))) # finally I connect the ends of the structure so that it forms a circle return(g) } You don’t have to understand every part of this function in order to use it. All you need to do is run the function above so that it is in your R environment. You can then use it. It has two arguments - number of nodes and the size of the groups. You could change clique_size to 4 or 10. caveman_net &lt;- simulate_caveman(n = 100, clique_size = 5) par(mar = c(2,2,2,2)) plot(caveman_net, layout = layout.kamada.kawai(caveman_net), vertex.size = 2, vertex.label = NA, vertex.color = &quot;grey80&quot;) Now you can clearly see what a caveman structure is. Let’s analyze it. graph.density(caveman_net) ## [1] 0.04444444 transitivity(caveman_net) # transitivity() measures clustering coefficient, which essentially says, how clustered is the network overall ## [1] 0.7894737 average.path.length(caveman_net) ## [1] 10.70101 It has a pretty low density since most nodes only have connections within their clique or else one tie outwards. And as I mentioned above, caveman structures are extremely clustered, since most edges are within group. Path length is also high - basically it takes 10 steps, on average, to reach one node from a random other node. In the real world, there are way more than 100 people and more than 20 groups, so it should be even more surprising that the average degree of separation is roughly six or seven steps. It follows that the “caveman structure” is not a small-world. We can look at the diameter of the network to see this too. The diameter is the longest shortest path. nodes_diameter&lt;-get.diameter(caveman_net) edges_incident &lt;- get.edge.ids(caveman_net, nodes_diameter) V(caveman_net)$color&lt;-&quot;grey60&quot; # Set default color for nodes V(caveman_net)[nodes_diameter]$color&lt;-&quot;green&quot; # Set the nodes on the diameter to be green E(caveman_net)$color&lt;-&quot;grey70&quot; # Set default edge color E(caveman_net)[edges_incident]$color&lt;-&quot;green&quot; # Set the edges on the diameter to be green plot(caveman_net, layout = layout.kamada.kawai(caveman_net), vertex.size = 2, vertex.label = NA) Watts wants to get from this network structure, to one in which the average path length is much lower. He performs a simple exercise to do so (and one we have already experimented with). He randomly rewires the network so that it begins, slowly to approximate a random graph. Random graphs have low average path length; so this is a good idea. We end up with a caveman structure with some number of rewired edges that will have the tendency to cut across the network caveman_net_rewired &lt;- rewire(caveman_net, keeping_degseq(niter = 1000)) We can use the rewire function to rewire the network. keeping_degseq() ensures that the degree distribution does not change and niter = 20 is the number of iterations (rewirings). E(caveman_net_rewired)$color &lt;- &quot;grey80&quot; V(caveman_net)$color &lt;- &quot;grey60&quot; plot(caveman_net_rewired, layout = layout.kamada.kawai(caveman_net), vertex.size = 2, vertex.label=NA) Most of the rewirings cut across the network structure! plot(caveman_net_rewired, layout = layout.kamada.kawai(caveman_net_rewired), vertex.size = 2, vertex.label = NA) Let’s compare this to the caveman network. graph.density(caveman_net_rewired) ## [1] 0.04444444 transitivity(caveman_net_rewired) ## [1] 0.02763158 average.path.length(caveman_net_rewired) ## [1] 3.307879 Density is unchanged. Clustering coefficient is less than before, but still relatively high. And average.path.length was cut in half. Only 20 rewirings and look at the change! We can analyze the change as we perform more rewirings. caveman_net_rewired &lt;- simulate_caveman(n = 100, clique_size = 10) avgpathlength &lt;- average.path.length(caveman_net_rewired) # These are the first observation clusteringcoefficient &lt;- transitivity(caveman_net_rewired) iter = 100 for ( i in 2:iter){ caveman_net_rewired &lt;- caveman_net_rewired %&gt;% rewire(keeping_degseq(niter = 1)) avgpathlength &lt;- c(avgpathlength, average.path.length(caveman_net_rewired)) # We are just appending the result to a vector clusteringcoefficient &lt;- c(clusteringcoefficient, transitivity(caveman_net_rewired)) } plot(1:100, avgpathlength, xlab = &quot;Numer of Rewirings&quot;, ylab = &quot;Average Path Length&quot;, main = &quot;Caveman&quot;, type = &quot;l&quot;) lines(1:100, clusteringcoefficient) plot(1:100, clusteringcoefficient, xlab = &quot;Numer of Rewirings&quot;, ylab = &quot;Clustering Coefficient&quot;, main = &quot;Caveman&quot;, type = &quot;l&quot;, ylim = c(0,1)) 13.2 Measuring connectivity of networks Let’s say that one of the small-world graphs resulting from this simulation were a real network that we had collected with an online survey. caveman_net_rewired &lt;- rewire(caveman_net, keeping_degseq(niter = 10)) How might we go about analyzing it, identifying things like bridges and cut-points, and evaluating the degree of its connectivity? So far we have learned a few useful measures - like the diameter, the average shortest path length, and the clustering coefficient. But there are many others too. In what follows, we’ll explore some of the options that igraph provides. First, imagine that we wanted to highlight every bridge in the network, not just the diameter. Unfortunately, igraph doesn’t have a function for identifying bridges, so we will have to build one ourselves. The basic idea is that a bridge is the only tie that connects two otherwise distinct components in a network. We can use the decompose.graph, to decompose it into its constituitive components. Then we loop through the edges in the network, deleting them one at a time, and evaluating if the number of components changes as a result of the deletion. If there is a change, then we save the edge id as a bridge, otherwise we just continue on. Finally, we return all of the bridge ids. bridges &lt;- function(net){ bridges &lt;- c() # empty vector to store bridge names in number_components &lt;- length(decompose.graph(net)) # grab the number of components in the original raph for (i in 1:length(E(net))) { # begin a loop through all of the edges net_sub &lt;- delete.edges(net, i) # delete the edge in question if(length(decompose.graph(net_sub) ) &gt; number_components){ # if the number of components has increased bridges &lt;- c(i, bridges) # save this edge as a bridge } } return(bridges) # return the set of bridges } Let’s try it on the caveman network. bridges(caveman_net_rewired) ## NULL Hmm, there are no bridges in our graph. I guess that makes sense - since there is a lot of redundancy in the small world graphs we saw previously So perhaps we need a different measure, which following Park et al., measures a tie’s range, rather than just whether it is a bridge or not. igraph doesn’t have that either, but we can make it ourselves. The basic idea is to again loop through the edges in a network, evaluate which vertices are incident to those edges, delete the edge in question, and evaluate how the distance between the vertices changed as a result. tie_range &lt;- function(net){ tie_ranges &lt;- c() # empty vector to save ranges for (i in 1:length(E(net))) { # loop through edges incident_vertices &lt;- ends(net, i) # which nodes are incident to the edge in quetion net_sub &lt;- delete.edges(net, i) # delete the edge updated_distance &lt;- distances(net_sub, v = incident_vertices[1,1], to = incident_vertices[1,2], mode = &quot;all&quot;) # evaluate the distance for the previously connected nodes tie_ranges &lt;- c(tie_ranges, updated_distance) # save the result } return(tie_ranges) # return the resulting tie ranges } tie_range(caveman_net_rewired) ## [1] 2 2 2 2 2 2 2 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 5 ## [26] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [51] 2 2 4 2 2 2 2 2 2 2 5 5 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [76] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [101] 2 2 2 5 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 4 2 2 2 2 2 ## [126] 2 2 2 2 2 5 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [151] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [176] 2 2 2 2 2 2 2 2 4 2 2 2 2 2 2 2 2 2 2 2 2 2 5 2 2 ## [201] 10 3 3 10 10 4 11 11 11 11 10 5 5 17 17 17 17 17 2 17 Most of the edges have ranges of around 2 but a few have a range of 11 or more! Let’s plot the graph, adjusting the width of edges according to E(caveman_net_rewired)$color &lt;- &quot;grey80&quot; V(caveman_net)$color &lt;- &quot;grey60&quot; E(caveman_net_rewired)$range &lt;- tie_range(caveman_net_rewired) plot(caveman_net_rewired, layout = layout.kamada.kawai(caveman_net_rewired), vertex.size = 2, vertex.label=NA, edge.width = E(caveman_net_rewired)$range/2) Cool! It feels like we have been doing a lot of the heavy lifting here.. What, in relation to connectivity, does igraph have? Well it has a couple of interesting connectivity measures that we didn’t encounter in the literature: edge connectivity and vertex connectivity. As igraph describes, “the vertex connectivity of two vertices in a graph is the minimum number of vertices needed to remove from the graph to eliminate all paths from source to target.” The vertex connectivity of a graph is the minimum vertex connectivity of all pairs of vertices in the graph. It signifies how many vertices would need to delete, on average, to sever the connectivity between any two nodes in the network. This highlights the redundancy of the caveman graph and explains why we didn’t find any bridges. vertex_connectivity(caveman_net_rewired) ## [1] 2 Naturally, edge connectivity of two vertices is the number of edges needed to remove from the graph to eliminate all paths between them. edge_connectivity(caveman_net_rewired) ## [1] 2 It also has Burt’s measure of network constraint! constraint(caveman_net_rewired) ## 1 2 3 4 5 6 7 8 ## 0.4463000 0.4463000 0.4478125 0.4478125 0.6062500 0.5093000 0.5093000 0.7015625 ## 9 10 11 12 13 14 15 16 ## 0.7015625 0.7015625 0.4512000 0.3382000 0.4659375 0.6135937 0.6135937 0.5093000 ## 17 18 19 20 21 22 23 24 ## 0.5093000 0.7015625 0.7015625 0.7015625 0.5093000 0.5093000 0.7015625 0.7015625 ## 25 26 27 28 29 30 31 32 ## 0.7015625 0.3382000 0.4512000 0.6135937 0.4659375 0.6135937 0.2450000 0.3500000 ## 33 34 35 36 37 38 39 40 ## 0.4659375 0.5398437 0.5398437 0.5093000 0.5093000 0.7015625 0.7015625 0.7015625 ## 41 42 43 44 45 46 47 48 ## 0.5093000 0.5093000 0.7015625 0.7015625 0.7015625 0.5093000 0.5093000 0.7015625 ## 49 50 51 52 53 54 55 56 ## 0.7015625 0.7015625 0.3382000 0.4512000 0.6135937 0.6135937 0.4659375 0.2950000 ## 57 58 59 60 61 62 63 64 ## 0.2950000 0.5262500 0.3803125 0.3803125 0.5093000 0.5093000 0.7015625 0.7015625 ## 65 66 67 68 69 70 71 72 ## 0.7015625 0.3500000 0.3500000 0.6215625 0.6215625 0.6215625 0.5093000 0.5093000 ## 73 74 75 76 77 78 79 80 ## 0.7015625 0.7015625 0.7015625 0.5093000 0.5093000 0.7015625 0.7015625 0.7015625 ## 81 82 83 84 85 86 87 88 ## 0.5093000 0.5093000 0.7015625 0.7015625 0.7015625 0.5093000 0.5093000 0.7015625 ## 89 90 91 92 93 94 95 96 ## 0.7015625 0.7015625 0.3382000 0.4993000 0.6135937 0.6135937 0.5334375 0.3783000 ## 97 98 99 100 ## 0.3922000 0.3126562 0.4478125 0.5245312 How correlated is it with betweenness centrality? cor.test(constraint(caveman_net_rewired), betweenness(caveman_net_rewired)) ## ## Pearson&#39;s product-moment correlation ## ## data: constraint(caveman_net_rewired) and betweenness(caveman_net_rewired) ## t = -10.832, df = 98, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8162472 -0.6336310 ## sample estimates: ## cor ## -0.738178 Pretty negatively correlated, which makes perfect sense. 13.2.1 One last thing… After this laborious lab, you might be annoyed to hear that igraph has its own small world simulation. It starts from a lattice structure, though, instead of a caveman. Watts and Strogatz mention in their AJS paper that the same results hold whether a lattice or caveman structure is used as the starting point of the simulation, but the caveman verion is much prettier than the lattice. Still, if you want to simulate quickly many small world graphs of size N and don’t want to deal with the limitations of the caveman simulation, this may be a good option. sample_smallworld(dim, size, nei, p, loops = FALSE, multiple = FALSE) There are some other options for simulating graphs too. Barbasi game refers to a simulation of a scale-free network, one with an unequal degree distribution (rich get richer). Last week we saw the erdos.renyi.game function, which simulates an Erdos-Renyi random graph. scale_free_net &lt;- barabasi.game(100, 1, 5) # Generate a scale-free network random_net &lt;- erdos.renyi.game(100, 0.05, directed = TRUE) # generate a Erdos-Renyi random graph An alternative to the caveman network is the “interconnected islands” game: Create a number of Erdos-Renyi random graphs with identical parameters, and connect them with the specified number of edges. islands.n is the number of islands, islands.size is the number of people in each island, islands.pin is the within-island density, and n.inter is the number of edges between islands. plot(sample_islands(islands.n = 5, islands.size = 5, islands.pin = 1, n.inter = 1)) Here, game is referring to a network simulation but igraph can also simulate games on top of graphs to see how network structure affects diffusion outcomes (https://igraph.org/c/doc/igraph-Spatial-Games.html) "],["finding-groups-in-networks.html", "14 Finding Groups in Networks 14.1 Cohesive Blocking 14.2 Group Detection", " 14 Finding Groups in Networks Groups are one of the many tools we have for analyzing network structure. Group detection focuses on the presence of ties - in attempt to identify densly connected groups of actors. There are many different tools for doing this - we will cover four: component analysis, k-cliques, modularity, and cohesive blocking. Each method has its own uses and theoretical underpinnings, so I put citations of famous papers that use each method at the bottom of the script. 14.0.1 Component analysis The most basic form of network group is a component. In a connected component, every node is reachable via some path by every other node. Most network datasets have only a single large connected component with a few isolates - however, some unique datasets might have three or four large, distinct components. In a directed graph, components can be weakly or strongly connected. If node i can reach j via a directed path and j can reach i via a directed path, for all i and j nodes in the component, then we say the component is strongly connected. If all nodes are only reachable from a single direction, (i.e. i can reach j via a directed path, but j can’t reach i), then we say the component is weakly connected. The ?decompose.graph function in igraph will take a network and decompose it into its connected components. We can then analyze each component separately. library(igraph) set.seed(1234) g1 &lt;- barabasi.game(20, 1, 5) # Generate a scale-free network V(g1)$name &lt;- as.character(1:20) g2 &lt;- erdos.renyi.game(20, graph.density(g1), directed = TRUE) V(g2)$name &lt;- as.character(21:40) g3 &lt;- graph.union(g1, g2, byname = TRUE) # The ?graph.union function combines two networks. plot(g3, vertex.label = NA, vertex.size=2, edge.arrow.size = .1) # Two distinct networks in a single plot Now let’s use decompose to isolate each component. There will be two components. component_list &lt;- decompose.graph(g3, mode = &quot;weak&quot;) component_list ## [[1]] ## IGRAPH 3864a87 DN-- 20 85 -- ## + attr: name_1 (g/c), name_2 (g/c), power (g/n), m (g/n), zero.appeal ## | (g/n), algorithm (g/c), type (g/c), loops (g/l), p (g/n), name (v/c) ## + edges from 3864a87 (vertex names): ## [1] 2 -&gt;1 3 -&gt;1 3 -&gt;2 4 -&gt;1 4 -&gt;2 4 -&gt;3 5 -&gt;1 5 -&gt;2 5 -&gt;3 5 -&gt;4 ## [11] 6 -&gt;1 6 -&gt;2 6 -&gt;3 6 -&gt;4 6 -&gt;5 7 -&gt;1 7 -&gt;2 7 -&gt;3 7 -&gt;4 7 -&gt;6 ## [21] 8 -&gt;1 8 -&gt;2 8 -&gt;3 8 -&gt;4 8 -&gt;5 9 -&gt;1 9 -&gt;2 9 -&gt;3 9 -&gt;4 9 -&gt;7 ## [31] 10-&gt;1 10-&gt;2 10-&gt;3 10-&gt;4 10-&gt;5 11-&gt;1 11-&gt;2 11-&gt;3 11-&gt;4 11-&gt;5 ## [41] 12-&gt;1 12-&gt;3 12-&gt;5 12-&gt;6 12-&gt;8 13-&gt;1 13-&gt;2 13-&gt;3 13-&gt;4 13-&gt;5 ## [51] 14-&gt;1 14-&gt;3 14-&gt;5 14-&gt;8 14-&gt;13 15-&gt;1 15-&gt;2 15-&gt;3 15-&gt;4 15-&gt;5 ## [61] 16-&gt;1 16-&gt;2 16-&gt;3 16-&gt;5 16-&gt;10 17-&gt;1 17-&gt;2 17-&gt;3 17-&gt;5 17-&gt;7 ## + ... omitted several edges ## ## [[2]] ## IGRAPH fb4647d DN-- 20 94 -- ## + attr: name_1 (g/c), name_2 (g/c), power (g/n), m (g/n), zero.appeal ## | (g/n), algorithm (g/c), type (g/c), loops (g/l), p (g/n), name (v/c) ## + edges from fb4647d (vertex names): ## [1] 21-&gt;22 21-&gt;25 21-&gt;29 21-&gt;37 21-&gt;39 22-&gt;24 22-&gt;31 22-&gt;32 22-&gt;36 23-&gt;24 ## [11] 23-&gt;27 23-&gt;29 23-&gt;32 23-&gt;35 23-&gt;37 23-&gt;38 24-&gt;22 24-&gt;30 25-&gt;21 25-&gt;22 ## [21] 25-&gt;24 25-&gt;34 25-&gt;37 26-&gt;21 26-&gt;25 26-&gt;27 26-&gt;30 26-&gt;33 26-&gt;36 27-&gt;29 ## [31] 27-&gt;30 27-&gt;31 27-&gt;33 27-&gt;34 27-&gt;36 27-&gt;39 28-&gt;27 28-&gt;33 28-&gt;34 28-&gt;35 ## [41] 28-&gt;40 29-&gt;25 29-&gt;26 29-&gt;27 29-&gt;34 29-&gt;37 29-&gt;40 30-&gt;25 30-&gt;34 30-&gt;40 ## [51] 31-&gt;33 31-&gt;39 32-&gt;26 32-&gt;28 32-&gt;29 32-&gt;35 32-&gt;39 33-&gt;21 33-&gt;23 34-&gt;24 ## [61] 34-&gt;25 34-&gt;27 34-&gt;29 34-&gt;38 34-&gt;39 34-&gt;40 35-&gt;23 35-&gt;26 35-&gt;29 35-&gt;39 ## + ... omitted several edges It returns a list with two graphs in it - one for each component. par(mfrow = c(1, 2)) plot(component_list[[1]], main = &quot;The Scale-Free Graph&quot;, vertex.label = NA, vertex.size = 3) plot(component_list[[2]], main = &quot;The Random Graph&quot;, vertex.label = NA, vertex.size = 3) 14.0.2 Cliques Cliques are fully connected subgraphs within a network structure; they are like the caves in the caveman structure we learned about a few weeks ago. We often want to find all of the cliques in a network of various sizes. We could have a theory for example that people will dress or behave similarly or affect those in their cliques. That is, we might imagine cliques to be meaningful for the outcomes we are interested in study. We can do this with the clique function in igraph. clique_out &lt;- cliques(g1) ## Warning in cliques(g1): At igraph_cliquer.c:56 :Edge directions are ignored for ## clique calculations length(clique_out) ## [1] 467 Quite a few… imagine the number of cliques in a really large network If we want to look for cliques of a certain size we can use the min and max arguments. clique_four &lt;- cliques(g1, min = 4, max = 4) ## Warning in cliques(g1, min = 4, max = 4): At igraph_cliquer.c:56 :Edge ## directions are ignored for clique calculations length(clique_four)/length(clique_out) ## [1] 0.2955032 Cliques of size four make about 25% of total cliques in the network. 14.1 Cohesive Blocking Cohesive blocking builds on the idea of cliques. It starts at the level of the component and identifies large substructures nested within the component. It then moves to those large substructures and identifies smaller and smaller nested substructures, until it reaches cliques. It is therefore a useful way to operationalize network embeddedness. We will run it on a small world network size 20. g2 &lt;- as.undirected(g2) g4 &lt;- watts.strogatz.game(1, 20 , 5, .1) V(g4)$name &lt;- as.character(1:20) g5 &lt;- graph.union(g2, g4) par(mfrow = c(1, 1)) plot(g5, vertex.label = NA, vertex.size=2, edge.arrow.size = .1) # Two distinct networks in a single plot The function for cohesive blocking is ?cohesive.blocks. It is very inefficient so it will take awhile to run. Run time depends on the number of edges in the network and the degree of nestedness. Don’t even bother running it on very large networks. blocks &lt;- cohesive.blocks(g5) # The basic function for finding cohesive blocks blocks(blocks) # This tells us what the blocks are and which nodes are in them cohesion(blocks) # This gives a cohesion score for each block. Cohesion is the minimum number of vertices you must remove in order to make the block not strongly connected. plotHierarchy(blocks) # Finally, plotHierarchy shows the nestedness structure of the blocks. 14.2 Group Detection The previous methods for detecting groups require either that all individuals in a group are connected or that the groups are completely isolated from one another. We might wish to relax those rather stringent requirements. For example, if we randomize our g5 network, which was the union of a random graph and a watts.strogatz graph, enough times, it will connect the two separate networks with a handful of edges. g5 &lt;- g5 %&gt;% rewire(keeping_degseq(niter = vcount(g5) * 0.05)) plot(g5, vertex.label = NA, vertex.size=2, edge.arrow.size = .1) # Two distinct networks in a single plot Our component analysis would not reveal that there are two distinct groups - instead the whole graph would appear connected. But we know there are (at least) two groups in the network - we designed it that way - so how might we identify them? We could define groups as sets of nodes who have a higher proportion of edges going inwards rather than outwards - that is, solidarity is strongest within the group - and use that definition to search for groups. How does group detecion work? Remember the Girvan and Newman algorithm we covered in class? We can write a simple version of it in R. Here is one I wrote with comments. # first we create a function which defines what happens at each iteration of our algorithm edge_betweenness_step = function(mat){ # graph the matrix net &lt;- graph.adjacency(mat) # evaluate edge betweenness edgebets &lt;- edge_betweenness(net) # identify the edge that has the highest betweenness to_delete &lt;- which.max(edgebets) # delete that edge net_new &lt;- delete.edges(net, to_delete) # evaluate the number of components net_new &lt;- decompose(net_new) # if the number of components grew to be greater than 1, return the matrices of those two components if(length(net_new) &gt; 1){ return(lapply(net_new, get.adjacency)) } else { # otherwise return the matrix that we started with, minus the deleted edge return(get.adjacency(net_new[[1]])) } } # recursive function to measure depth of a list depth &lt;- function(this) ifelse(is.list(this), 1L + max(sapply(this, depth)), 0L) # next we will repeatedly run this step algorithm a certain number of times, at each step applying it to every component we have created # so at first, we start with the whole graph, we delete the highest between edges until the network is split into two components. # then we apply the above to each of the network subsets # we do this until we have made N effective cuts edge_betweenness_clustering &lt;- function(net, desired_depth = 15){ # required packages require(reshape2) # turn our net into a matrix net_mat &lt;- get.adjacency(net) # run the first iteration of edge_betweenness and save the result in a list net_temp &lt;- list(edge_betweenness_step(net_mat)) # while actual depth is &lt; desired_depth # depth signifies how many effective cuts have been made # where an effective cut is any cut that divides a component into at least two new components while(depth(net_temp) &lt; desired_depth){ # apply edge_betweenness recursively to every component in the list net_temp &lt;- rapply(net_temp, edge_betweenness_step, how = &quot;list&quot;) } #get the row.names of every matrix (i.e. the actors who are in each component/group at the end of the iterations) groups &lt;- lapply(unlist(net_temp), row.names) # name the groups according to their order names(groups) &lt;- as.character(1:length(groups)) # use melt to produce a person to group data.frame memberships &lt;- melt(groups) # convert value (i.e. id) to a character vector from factor memberships$value &lt;- as.character(memberships$value) # reorder the memberships data.frame so that it matches the order of vertices in the original network memberships &lt;- memberships[match(V(net)$name, memberships$value),] # construct the communities object using this helpful function provided by igraph output &lt;- make_clusters(net, membership = as.numeric(memberships$L1), algorithm = &quot;edge_betweenness_attempt&quot;, modularity = TRUE) # return the communities object return(output) } mod_comparison &lt;- c() for(i in 1:20){ mod_comparison &lt;- c(mod_comparison, edge_betweenness_clustering(g5, i)$modularity) } plot(1:20, mod_comparison, ylab = c(&quot;Modularity&quot;), xlab = c(&quot;Effective Cuts Made&quot;)) It looks like a simple 2 component solution performs best - modularity actually decreases as we increase the number of cuts. Let’s see how it looks on the network. plot(g5, vertex.label = NA, vertex.size=2, edge.arrow.size = .1, mark.groups = edge_betweenness_clustering(g5, 2)) It works! This was a fun exercise, but my code is quite slow and not that intelligent. Thankfully, there are a plethora of group detection algorithms in igraph written to be much faster than mine. The full list is: edge.betweenness.community (i.e. what I tried to make above), fastgreedy.community, label.propagation.community, leading.eigenvector.community, multilevel.community, optimal.community, spinglass.community, and walktrap.community. This webpage has a summary of their pros and cons for an older version of igraph: http://bommaritollc.com/2012/06/summary-community-detection-algorithms-igraph-0-6/ The main takeaway is that you should tailor your algorithm choice to your network. Certain algorithms are designed for directed or undirected graphs, and work better with small or large graphs. Each algorithm as its own igraph function. These functions produce lists with information about the algorithm results. Element 1 holds a vector that details which group each node is in, which I will refer to as the membership vector. Element 6 holds the modularity of the network given the group detection algorithm. For undirected graphs, you can use the optimal or multilevel algorithms. communityMulti &lt;- multilevel.community(g5) For directed graphs, edge betweenness is generally your best bet, though the walktrap algorithm performs well too. communityWalk &lt;- walktrap.community(g2) communityEB &lt;- edge.betweenness.community(g2) communityInfo &lt;- infomap.community(g2) We can use the mark.groups argument in plot to see the groups. plot(g5, vertex.size = 3, vertex.label = NA, mark.groups = communityMulti) Alternatively, we can just color the nodes by group membership. In this case, we can just use the membership function V(g5)$color &lt;- membership(communityMulti) plot(g5, vertex.size = 3, vertex.label = NA) This becomes less feasible as the number of groups increases! It is better in that case to use one of the coloring functions I sent along via Canvas a few weeks back. 14.2.1 Modularity Modularity takes a given group structure and calculates how separated the different groups are from each other. It therefore operationalizes our notion of groups above by calculating the proportion of ties that are within groups as opposed to between them. Networks with high modularity have dense connections between the nodes within modules but sparse connections between nodes in different modules. We can think of modularity as both a measure of how effective a given grouping algorithm is - i.e. higher modularity means the alogrithm is identifying distinct, sociall separate groups. But it can also be thought of as a measure of the saliency of groups to the network in general. The higher modularity the more that groups structure the network. Modularity measures the extent to which a network can be divided into distinct modules or groups. Getting a network’s modularity in igraph is easy! We can either access the modularity score directly communityMulti &lt;- multilevel.community(g5) communityMulti$modularity ## [1] 0.4270216 0.4716049 Or use the modularity() function. modularity(communityMulti) # We can use the modularity() function on a group detection output. ## [1] 0.4716049 "],["homophily-and-exponential-random-graphs-ergm.html", "15 Homophily and Exponential Random Graphs (ERGM) 15.1 Homophily 15.2 ERGMs", " 15 Homophily and Exponential Random Graphs (ERGM) 15.1 Homophily In this tutorial, we cover how to A) calculate homophily on a network and B) fit exponential random graphs to networks. First, we will need one of the Add Health data sets that we have been playing around with in previous tutorials. The code below downloads the data from Moreno’s website and converts it into an igraph object. We went over this in the Transitivy tutorial, so here, I just paste the code. Through this tutorial, we will rely on igraph to analyze the comm59 Add Health network that we made use of last class. # read in the edge list from our github el &lt;- read.table(&quot;https://raw.githubusercontent.com/mahoffman/stanford_networks/main/data/comm59.dat.txt&quot;, header = T) # Read in attributes from our github attributes &lt;- read.table(&quot;https://raw.githubusercontent.com/mahoffman/stanford_networks/main/data/comm59_att.dat.txt&quot;, header = T) # add an ID column attributes$ID &lt;- 1:nrow(attributes) # Indexing data so that you only put in certain columns el_no_weight &lt;- el[,1:2] # We will ignore the ranking variable for now. el_no_weight &lt;- as.matrix(el_no_weight) # igraph requires a matrix # convert ids to characters so they are preserved as names el_no_weight[,1] &lt;- as.character(el_no_weight[,1]) el_no_weight[,2] &lt;- as.character(el_no_weight[,2]) # Graph the network library(igraph) net59 &lt;- graph.edgelist(el_no_weight, directed = T) # Finally, add attributes # First link vertex names to their place in the attribute dataset linked_ids &lt;- match(V(net59)$name, attributes$ID) # Then we can use that to assign a variable to each user in the network V(net59)$race &lt;- attributes$race[linked_ids] V(net59)$sex &lt;- attributes$sex[linked_ids] V(net59)$grade &lt;- attributes$grade[linked_ids] V(net59)$school &lt;- attributes$school[linked_ids] net59 &lt;- delete.vertices(net59, which(is.na(V(net59)$sex) | V(net59)$sex == 0)) net59 &lt;- delete.vertices(net59, which(is.na(V(net59)$race) | V(net59)$race == 0)) net59 &lt;- delete.vertices(net59, which(is.na(V(net59)$grade) | V(net59)$grade == 0)) Great, now that we have the network, we can evaluate homophily. We can either use igraph’s built in function… assortativity(net59, types1 = as.numeric(V(net59)$sex)) ## [1] 0.2180534 Or do it ourselves. If you’ll remember from class on Tuesday, assortativity on variable is calculated by simply correlating the values of an attribute for every ego-alter pair in the network. We just grab the edgelist, match in ego and alter’s values for the variable of interest, and correlate with cor.test. df &lt;- data.frame(get.edgelist(net59), stringsAsFactors = F) df$sex1 &lt;- as.numeric(attributes$sex[match(df$X1, attributes$ID)]) df$sex2 &lt;- as.numeric(attributes$sex[match(df$X2, attributes$ID)]) cor.test(df$sex1, df$sex2) ## ## Pearson&#39;s product-moment correlation ## ## data: df$sex1 and df$sex2 ## t = 14.092, df = 3978, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1882596 0.2474463 ## sample estimates: ## cor ## 0.2180534 Race, however, is probably better conceptualized as a categorical variable. Assortativity_nominal can be used to evaluate assortativity for categorical variables. It requires a numeric vector, denoting the different categories, which starts at 1, so we add one to the race variable, which starts at 0. assortativity_nominal(net59, types = as.numeric(V(net59)$race) + 1) ## [1] 0.04103466 15.2 ERGMs Now imagine, like Wimmer and Lewis, we wanted to calculate homophily for race, but wanted to control for other network factors, such as transitivity, which might lead to a higher degree of same race friendships, but which don’t actually signal an in-group preference. Just like them, we can use exponential random graphs, which model networks as a function of network statistics. Specifically, ERGMs imagine the observed network to be just one instantiation of a set of possible networks with similar features, that is, as the outcome of a stochastic process, which is unknown and must therefore be inferred. The package that allows one to fit ergm models is part of the statnet (statistical networks) suite of packages. Much like tidyverse, which you might be familiar with, statnet includes a number of complementary packages for the statistical analysis of networks. Let’s install statnet and load it into R. install.packages(&quot;statnet&quot;) library(statnet) Great, now we can use statnet’s ergm() function to fit our first ERGM. The only problem? Our network is an igraph object rather than a statnet one. There is some good news though. People have built a package for converting igraph objects to statnet and vise versa - intergraph. Let’s install that and load it in too. install.packages(&quot;intergraph&quot;) library(intergraph) Now that we have intergraph installed and in our R environment, we can use the asNetwork function to convert it to a statnet object. I will also subset the data here, because ERGMs take forever to run and the network is somewhat large (~1000 nodes). set.seed(1234) sampled_students &lt;- sample(V(net59)$name, 350, replace = F) net59_for_ergm &lt;- igraph::delete.vertices(net59, !V(net59)$name %in% sampled_students) statnet59 &lt;- asNetwork(net59_for_ergm) statnet59 ## Network attributes: ## vertices = 350 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges= 547 ## missing edges= 0 ## non-missing edges= 547 ## ## Vertex attribute names: ## grade race school sex vertex.names ## ## No edge attributes It even ported over our node attributes! You can plot it to see if it looks the same as it did in igraph. plot(statnet59, vertex.col = &quot;tomato&quot;, vertex.cex = 1) With our data ready for analysis using statnet, let’s build our first ERGM. There are a number of potential ERG model terms that we could use in fitting our model. You can view a full list by looking at the documentation for: ?ergm.terms That said, it is common to build up from some basic terms first. The McFarland-Moody paper we read in class is a useful reference point as is the Wimmer-Lewis paper. Generally, the first term that people use is the edges term. It is a statistic which counts how many edges there are in the network. random_graph &lt;- ergm(statnet59 ~ edges, control = control.ergm(seed = 1234)) How do we interpret this coefficient? Coefficients in ERGMs represent the change in the (log-odds) likelihood of a tie for a unit change in a predictor. We can use a simple formula for converting log-odds to probability to understand them better. inv.logit &lt;- function(logit){ odds &lt;- exp(logit) prob &lt;- odds / (1 + odds) return(prob) } theta &lt;- coef(random_graph) inv.logit(theta) ## edges ## 0.004478101 So the probability of an edge being in the graph is roughly 0.02. The probability of an edge being drawn should in theory be the same as density - let’s check. network.density(statnet59) ## [1] 0.004478101 Nice. To put it all back into the theoretical underpinnings of ERGMs, we have modeled a stochastic generating process, and the only constraint we have put on the stochastic process is the probability with which edges are drawn, which was set equal to the network density of our observed graph. We can more closely examine the model fit using the summary() function, just like lm() or glm() in base R. summary(random_graph) ## Call: ## ergm(formula = statnet59 ~ edges, control = control.ergm(seed = 1234)) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -5.40407 0.04285 0 -126.1 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 169336 on 122150 degrees of freedom ## Residual Deviance: 7009 on 122149 degrees of freedom ## ## AIC: 7011 BIC: 7020 (Smaller is better. MC Std. Err. = 0) We can also simulate graphs using our ERGM fit. We did something similar to this when we simulated random graphs and set the probability of an edge being drawn to be equal to our network’s density. We just didn’t know it at the time! set.seed(1234) hundred_simulations &lt;- simulate(random_graph, coef = theta, nsim = 100, control = control.simulate.ergm(MCMC.burnin = 1000, MCMC.interval = 1000)) Let’s examine the first nine simulations. par(mfrow = c(3, 3)) sapply(hundred_simulations[1:9], plot, vertex.cex = 1, vertex.col = &quot;tomato&quot;) We can compare the number of edges our observed graph has to the average of the simulated networks. net_densities &lt;- unlist(lapply(hundred_simulations, network.density)) hist(net_densities, xlab = &quot;Density&quot;, main = &quot;&quot;, col = &quot;lightgray&quot;) abline(v = network.density(statnet59), col = &quot;red&quot;, lwd = 3, lty = 2) abline(v = mean(net_densities), col = &quot;blue&quot;, lwd = 3, lty = 1) Pretty close! Another way to evaluate our model is to use the built-in goodness of fit measures. Essentially, we will evaluate whether our network has similar structural features as the simulated graphs. ergm has a built-in function - gof() - which calculates the statistics for us. We just have to tell it how many simulations we want to use in the comparison set - the larger the number, the more accurate representation of the model. gof_stats &lt;- gof(random_graph) par(mfrow = c(2, 3)) plot(gof_stats, main = &#39;&#39;) On one measure, edgewise shared partners, our model looks okay. On the others, especially in and out degree, it looks awful. How to we improve our fit? By adding more terms to the model! First, let’s build a model with only dyad-independent terms. Just like with the random graph, we are essentially fitting a logistic regression. nodematch is the homophily term in ergm. We can specify the attribute we want to examine as well as the diff argument, which allows for differential levels of homophily for different groups. model1 &lt;- ergm(statnet59 ~ edges + nodematch(&quot;race&quot;) + nodematch(&quot;sex&quot;) + nodematch(&quot;grade&quot;)) summary(model1) ## Call: ## ergm(formula = statnet59 ~ edges + nodematch(&quot;race&quot;) + nodematch(&quot;sex&quot;) + ## nodematch(&quot;grade&quot;)) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -6.91859 0.12039 0 -57.468 &lt; 1e-04 *** ## nodematch.race 0.05272 0.09125 0 0.578 0.56341 ## nodematch.sex 0.26996 0.08717 0 3.097 0.00195 ** ## nodematch.grade 2.87780 0.10361 0 27.774 &lt; 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 169336 on 122150 degrees of freedom ## Residual Deviance: 6007 on 122146 degrees of freedom ## ## AIC: 6015 BIC: 6054 (Smaller is better. MC Std. Err. = 0) Every variable is significant! Grade and race have especially large coefficients and all three are positive. Let’s try it with diff set to T. We will limit our examination to only grade/race/sex categories represented by a large number of vertices in our network. You can examine this using the table function table(V(net59_for_ergm)$race) # 1 = white, 2 = black, 3 = hispanic, 4 = asian, and 5 = mixed/other table(V(net59_for_ergm)$sex) # 1 = male, 2 = female table(V(net59_for_ergm)$grade) model2 &lt;- ergm(statnet59 ~ edges + nodematch(&quot;race&quot;, diff = T, levels = c(&quot;1&quot;, &quot;2&quot;, &quot;5&quot;)) + nodematch(&quot;sex&quot;, diff = T, levels = c(&quot;1&quot;, &quot;2&quot;)) + nodematch(&quot;grade&quot;, diff = T, levels = as.character(c(7:12)))) summary(model2) ## Call: ## ergm(formula = statnet59 ~ edges + nodematch(&quot;race&quot;, diff = T, ## levels = c(&quot;1&quot;, &quot;2&quot;, &quot;5&quot;)) + nodematch(&quot;sex&quot;, diff = T, levels = c(&quot;1&quot;, ## &quot;2&quot;)) + nodematch(&quot;grade&quot;, diff = T, levels = as.character(c(7:12)))) ## ## Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -6.96247 0.12167 0 -57.226 &lt; 1e-04 *** ## nodematch.race.1 0.11676 0.09397 0 1.243 0.214032 ## nodematch.race.2 -Inf 0.00000 0 -Inf &lt; 1e-04 *** ## nodematch.race.5 -0.07078 0.28052 0 -0.252 0.800805 ## nodematch.sex.1 0.17007 0.11570 0 1.470 0.141582 ## nodematch.sex.2 0.34239 0.09813 0 3.489 0.000485 *** ## nodematch.grade.7 2.86262 0.14430 0 19.838 &lt; 1e-04 *** ## nodematch.grade.8 2.65692 0.16632 0 15.975 &lt; 1e-04 *** ## nodematch.grade.9 2.97460 0.12574 0 23.657 &lt; 1e-04 *** ## nodematch.grade.10 2.42069 0.19922 0 12.151 &lt; 1e-04 *** ## nodematch.grade.11 2.54508 0.21460 0 11.860 &lt; 1e-04 *** ## nodematch.grade.12 3.31829 0.14238 0 23.306 &lt; 1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 169336 on 122150 degrees of freedom ## Residual Deviance: 5976 on 122138 degrees of freedom ## ## AIC: 5998 BIC: 6105 (Smaller is better. MC Std. Err. = 0) ## ## Warning: The following terms have infinite coefficient estimates: ## nodematch.race.2 Interesting! Now let’s add some dyad-dependent terms. These can be a bit finnicky, especially transitivity ones. mutual is the term for reciprocity. model3 &lt;- ergm(statnet59 ~ edges + nodematch(&quot;race&quot;) + nodematch(&quot;sex&quot;) + nodematch(&quot;grade&quot;) + mutual) summary(model3) ## Call: ## ergm(formula = statnet59 ~ edges + nodematch(&quot;race&quot;) + nodematch(&quot;sex&quot;) + ## nodematch(&quot;grade&quot;) + mutual) ## ## Monte Carlo Maximum Likelihood Results: ## ## Estimate Std. Error MCMC % z value Pr(&gt;|z|) ## edges -6.95232 0.10987 0 -63.275 &lt;1e-04 *** ## nodematch.race 0.04004 0.07970 0 0.502 0.6154 ## nodematch.sex 0.19509 0.07657 0 2.548 0.0108 * ## nodematch.grade 2.32082 0.10386 0 22.345 &lt;1e-04 *** ## mutual 4.39947 0.15846 0 27.764 &lt;1e-04 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Null Deviance: 169336 on 122150 degrees of freedom ## Residual Deviance: 5418 on 122145 degrees of freedom ## ## AIC: 5428 BIC: 5477 (Smaller is better. MC Std. Err. = 1.117) Now let’s add a term for triadic closure. There are a few terms for triads - one of them, triangles, tends to lead to degeneracy. The gwesp term behaves better, though convergence is far from guaranteed. It may be a good time to use the bathroom. This will take a while… model4 &lt;- ergm(statnet59 ~ edges + nodematch(&quot;race&quot;) + nodematch(&quot;sex&quot;) + nodematch(&quot;grade&quot;) + mutual + gwesp(0.25, fixed = T), control=control.ergm(MCMLE.maxit= 40)) # you can change a number of other things about the MCMC algorithm - from its burn-in to its step and sample size # here we just up the maximum iterations we wait to see if it has converged summary(model4) Let’s take a look at the goodness-of-fit statistics for this most elaborate model. model4_gof &lt;- gof(model4) par(mfrow = c(3, 2)) plot(model4_gof, main = &#39;&#39;) Definitely an improvement over the random graph. Lab: Run ergm on a graph of interest to you and include balance (reciprocity and triadic closure) and homophily terms. Interpret the results. Simulate a graph from this model and compare it to your original graph. "],["positional-analysis-in-networks.html", "16 Positional analysis in networks 16.1 Toy data 16.2 Structural equivalence 16.3 Block Modeling with CONCOR 16.4 Isomorphic local graphs 16.5 Stochastic Block Models (SBMs) 16.6 Feature-based strategies 16.7 Lab: Try these different strategies on a real graph of your choosing (go with smaller networks) and compare the different results using correlation. Which methods perform similarly? Do the roles that they identify look meaningful? Plot the network coloring the nodes by role.", " 16 Positional analysis in networks This tutorial covers positional analysis in networks. Positional analysis seeks to identify actors or groups of actors who occupy similar positions or roles in a network. Nodes are therefore not grouped because they share more connections with each other than with other nodes in the network, as in modularity-based community or group detection, but rather because they share similar, or even precisely the same, patterns of relations. These positions might be though of as roles - in so far as people who occupy similar positions also perform similar functions for the graph - though this need not be the case. However, compared to many of the other strategies we have so far seen for grouping and comparing nodes, positional analysis is particularly fraught with difficulty. It relies on one being able to identify which nodes are structurally equivalent, or more specifically, which nodes have identical or similar sets of relations across all other actors in a graph. This would be easy if we just wanted to know if two nodes shared the same exact set of relations - we could just calculate how different their relations are over the other nodes in the network and identify those pairs of nodes who have precisely the relations - but this exact form of equivalence is, in the real world, quite rare. In practice, we are interested in a more generalized notion of structural equivalence, one in which nodes are seen as sharing a position or role not when they have relations with the exact same people, but rather when they share patterns of relations that are generally similar. There are a number of strategies for identifying roles or positions in a network. Here, we cover three such strategies: 1) CONCOR, which evaluates approximate structural equivalence to identify positions, 2) stochastic block modeling, which uses a more relaxed notion of stochastic equivalence to identify positions, and 3) feature-based strategies for learning roles in a network. Block modeling of this sort is not implemented in igraph, so for much of this tutorial, we will have to rely on our own code. 16.1 Toy data Let’s make a toy network, where structurally equivalent nodes are easy to identify by eye, against which we can compare the different methods. library(igraph) el &lt;- data.frame(Ego = c(&quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;D&quot;, &quot;D&quot;, &quot;D&quot;), Alter = c(&quot;P&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;)) net &lt;- graph.edgelist(as.matrix(el), directed = T) What does the network look like? Let’s plot it, and color nodes by their equivalence class. layout_save &lt;- layout_with_fr(net) plot(net, vertex.color = true_class, vertex.label.color = &quot;white&quot;, layout = layout_save) 16.2 Structural equivalence Let’s begin with an exact measure of structural equivalence - two nodes will be deemed structurally equivalent if they have precisely the same set of relations across all other nodes in the network. To evaluate this, we could simply identify which nodes have exactly the same set of neighbors, or using a matrix, take the absolute difference in row values for two nodes. We extract the adjacency matrix. adj_mat &lt;- as.matrix(as_adj(net)) Here is an example for nodes Q and A. a_row &lt;- adj_mat[&quot;Q&quot;, !colnames(adj_mat) %in% c(&quot;Q&quot;, &quot;A&quot;)] b_row &lt;- adj_mat[&quot;A&quot;, !colnames(adj_mat) %in% c(&quot;Q&quot;, &quot;A&quot;)] # equivalent? sum(abs(a_row-b_row)) ## [1] 3 The sum of their absolute differences greater than 0, so we would say they are not precisely equivalent. Let’s extrapolate to every pair of nodes in the network with a function. perfect_equivalence &lt;- function(mat){ # make an empty version of the matrix where we will store equivalence values matrix_vals &lt;- mat matrix_vals[] &lt;- 0 # loop over the actors in the network, comparing pair-wise their values for(i in 1:nrow(mat)){ for(j in 1:nrow(mat)){ a_row &lt;- mat[i, c(-i, -j)] b_row &lt;- mat[j, c(-i, -j)] abs_diff &lt;- sum(abs(a_row-b_row)) # take sum of absolute differences matrix_vals[i,j] &lt;- abs_diff } } # return results return(matrix_vals) } Now we apply the function to our network/matrix. What do you see? structurally_equivalent &lt;- perfect_equivalence(adj_mat) structurally_equivalent ## Q P A B C D E F G H I J ## Q 0 2 3 3 6 6 3 3 3 3 3 3 ## P 2 0 1 1 3 3 0 0 0 0 0 0 ## A 3 1 0 2 3 4 1 1 1 1 1 1 ## B 3 1 2 0 4 3 1 1 1 1 1 1 ## C 6 3 3 4 0 6 2 2 2 3 3 3 ## D 6 3 4 3 6 0 3 3 3 2 2 2 ## E 3 0 1 1 2 3 0 0 0 0 0 0 ## F 3 0 1 1 2 3 0 0 0 0 0 0 ## G 3 0 1 1 2 3 0 0 0 0 0 0 ## H 3 0 1 1 3 2 0 0 0 0 0 0 ## I 3 0 1 1 3 2 0 0 0 0 0 0 ## J 3 0 1 1 3 2 0 0 0 0 0 0 Let’s convert this distance matrix to a similarity matrix. structurally_equivalent_sim &lt;- 1-(structurally_equivalent/max(structurally_equivalent)) We can now cluster this similarity matrix to identify sets of similar actors using k-means, for example. If we set the number of clusters equal to 6, it will pull out the set of actors who have precisely the same relations. If we set it equal to 5 or 4, it will show us actors who are proximately similar. group_ids_6 &lt;- kmeans(structurally_equivalent_sim, centers = 6) group_ids_5 &lt;- kmeans(structurally_equivalent_sim, centers = 5) How close do we get? plot(net, vertex.color = group_ids_5$cluster, layout = layout_save) plot(net, vertex.color = group_ids_6$cluster, layout = layout_save) Close, but there is a problem. This strategy appears to treat C and D as occupying different positions. Why is that? Well, it has to do with our strict definition of structural equivalence - which relies on comparing the precise set of actors that each node is connected to. As a result, our method is confusing similarity with closeness, and as a result, it turns out, nodes that we deem structurally equivalent can never be more than two steps away from one another. 16.3 Block Modeling with CONCOR Before we deal with our definitional problem, let’s work through the CONCOR algorithm, introduced by Breiger et al. and made use of by White, Boorman, and Breiger in their seminal paper, Social Structure from Multiple Networks. I. Blockmodels of Roles and Positions. This method relies a similar definition of equivalence as the one outlined above - it identifies structurally equivalent nodes through correlating their relation sets - but because it uses correlation, and implements the stacking of matrices, it allows one to produce a block model of multiple relations simultaneously. CONCOR measures structural equivalence using correlation (that is, we measure the similarity between people in terms of the pattern of their relations). The cor function measures correlation. net_mat &lt;- as.matrix(get.adjacency(net)) net_cor = cor(net_mat) ## Warning in cor(net_mat): the standard deviation is zero net_cor[] &lt;- ifelse(is.na(net_cor[]), 0, net_cor[]) The key insight of CONCOR is that, by repeatedly running correlation on the results of this initial correlation, the data will eventually converge to only -1s and 1s. Let’s try 100 times and see how it goes. net_concor &lt;- net_cor for(i in 1:200){ net_concor &lt;- cor(net_concor) } How does it look? It appears to have converged! range(net_concor) ## [1] -1 1 Now what? First, we identify people who have 1s vs. -1s and group them together. These are our initial blocks, for a 2 block solution. group &lt;- net_concor[, 1] &gt; 0 We can split the original data into each of the respective groups. split_results &lt;- list(net_mat[, names(group[group])], net_mat[, names(group[!group])]) Now, if we want we can run the same thing above again, on each of the groups. cor_many_times &lt;- function(x, times = 1000){ for(i in 1:times){ x &lt;- cor(x) if(sd(x, na.rm = T) == 0){ return(x) } else { x[] &lt;- ifelse(is.na(x[]), 0, x[]) } } return(x) } split_results_corred &lt;- lapply(split_results, cor_many_times) ## Warning in cor(x): the standard deviation is zero groups_2 &lt;- lapply(split_results_corred, function(x) x[, 1] &gt; 0) split_results_again &lt;- lapply(groups_2, function(x) list(net_mat[, names(x[x])], net_mat[, names(x[!x])])) split_results_again &lt;- unlist(split_results_again, recursive = F) final_blocks &lt;- lapply(split_results_again, colnames) final_blocks ## [[1]] ## [1] &quot;Q&quot; &quot;C&quot; &quot;D&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; ## ## [[2]] ## [1] &quot;P&quot; &quot;A&quot; &quot;B&quot; ## ## [[3]] ## [1] &quot;E&quot; &quot;F&quot; &quot;G&quot; ## ## [[4]] ## NULL We can also plot the result using the blockmodel() function from the sna package (part of statnet). clusters &lt;- lapply(1:length(final_blocks), function(x) rep(x, length(final_blocks[[x]]))) clusters &lt;- unlist(clusters) names(clusters) = unlist(final_blocks) clusters &lt;- clusters[colnames(net_mat)] all_output = sna::blockmodel(net_mat, clusters, glabels = &quot;Feudal Network&quot;, plabels = colnames(net_mat)) plot(all_output) Here is the plot of the graph, where nodes are colored by position. plot(net, vertex.color = clusters) Not very good! 16.4 Isomorphic local graphs CONCOR allows us to apply our structural equivalence routine across many relations simultaneously, but it doesn’t quite solve the issue of how to identify actors who are precise structural equivalents. First, we could try to relax the condition that nodes be tied to precisely the same set of nodes by defining them as structurally equivalent as long as their local neighborhoods (set to include nodes n steps away) are automorphic. The underlying algorithm, bliss, essentially permutes the matrices of the two networks it is comparing to see if, under any of the different permutations, the two matrices are equivalent. Since the matrices are being permuted, we are ignoring node labels (i.e. node ids) and focusing instead on the structure of their relations. First, we select neighborhood size. For now, we set it to 2. steps = 2 Next, we extract each node’s 2-step neighborhood using igraph’s handy make_ego_graph() function. local_graphs &lt;- make_ego_graph(net, order = steps) Then, we loop through the different local neighborhoods and evaluate whether they are automorphic, saving the result in a matrix. We convert the matrix (which is logical) to a numeric matrix (so that TRUE = 1 and FALSE = 0), and set the diagonal to 0. iso_mat &lt;- matrix(FALSE, nrow = length(local_graphs), ncol = length(local_graphs)) for(i in 1:length(local_graphs)){ for(j in 1:length(local_graphs)){ iso_mat[i,j] &lt;- isomorphic(local_graphs[[i]], local_graphs[[j]]) } } iso_mat[] &lt;- as.numeric(iso_mat[]) diag(iso_mat) = 0 We then cluster the resulting matrix in order to identify nodes who share isomorphic neighborhoods, using a network clustering strategy. clusters_iso &lt;- cluster_louvain(graph.adjacency(iso_mat, mode = &quot;undirected&quot;)) Let’s plot the result. plot(net, vertex.color = membership(clusters_iso), layout = layout_save ) Wow, it worked! By relaxing the assumption that structurally equivalent nodes are tied to the same precise set of nodes, we get better results.. So why not just this strategy all the time? There are a couple of reasons. First, evaluating whether two neighborhoods are isomorphic is easy and quick when neighborhoods are small, but very soon becomes computationally costly (prohibitively so) as they grow. Also, this strategy is highly sensitive to missing data - if one of edge is missing or randomly rewired, then the two neighborhoods won’t be viewed as isomorphic. So.. let’s relax our assumptions even further. 16.5 Stochastic Block Models (SBMs) Now that we can identify blocks using featurs, CONCOR and a relatively strict measure of structural equivalence, let’s try to generalize our measure of equivalence even further. One such generalization is that rather than sharing the same set of nodes, actors that share a role or position will have the same probability of being attached to all other alters in the network. Under this operationalization, equivalence is not absolute, but probabilistic (and hence, stochastic). Stochastic block models are quite difficult to program, but thankfully, there is already an R package which can fit them to data, blockmodels. Let’s install it. install.packages(&quot;blockmodels&quot;) And load it into R. library(blockmodels) Here we run an SBM on just the marriage network. sbm_out &lt;- BM_bernoulli(&quot;SBM&quot;, net_mat, verbosity = 3, plotting = &quot;&quot;, exploration_factor = 5) # run a bernoulli block model on the feudal matrix. # bernoulli is for when your edge weights are binary # poisson is for when your edge weights are counts # gaussian (i.e. normal distribution) is for when your edge weights are continuous variables # estimate the result sbm_out$estimate() ## -&gt; Estimation for 1 groups ## -&gt; Computation of eigen decomposition used for initalizations ## ## -&gt; Pass 1 ## -&gt; With ascending number of groups ## -&gt; For 2 groups ## -&gt; For 3 groups ## -&gt; For 4 groups ## -&gt; For 5 groups ## -&gt; With descending number of groups ## -&gt; For 4 groups ## -&gt; For 3 groups ## -&gt; For 2 groups ## -&gt; Pass 2 ## -&gt; With ascending number of groups ## -&gt; For 2 groups ## -&gt; For 3 groups ## -&gt; For 4 groups ## -&gt; For 5 groups ## -&gt; With descending number of groups ## -&gt; For 4 groups ## -&gt; For 3 groups ## -&gt; For 2 groups ## -&gt; Pass 3 ## -&gt; With ascending number of groups ## -&gt; For 2 groups ## -&gt; For 3 groups ## -&gt; For 4 groups ## -&gt; For 5 groups ## -&gt; With descending number of groups ## -&gt; For 4 groups ## -&gt; For 3 groups ## -&gt; For 2 groups Now we can extract the role assignments (which are probabilistic). best_fit_assignments &lt;- sbm_out$memberships[[which.min(sbm_out$ICL)]] # extract the fit which is best according to ICL (Integrated Completed Likelihood), a measure for selecting the best model head(best_fit_assignments$Z) # probabilities of belonging to each group ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.008130081 0.008130081 0.008130081 0.008130081 0.967479675 ## [2,] 0.008180839 0.008180839 0.948836318 0.026621164 0.008180839 ## [3,] 0.008130081 0.008130081 0.967479675 0.008130081 0.008130081 ## [4,] 0.008130081 0.008130081 0.967479675 0.008130081 0.008130081 ## [5,] 0.967479675 0.008130081 0.008130081 0.008130081 0.008130081 ## [6,] 0.967479675 0.008130081 0.008130081 0.008130081 0.008130081 class_assignments &lt;- apply(best_fit_assignments$Z, 1, which.max) # identify which column as the highest value for each node (row) And we can plot the results, just like before, using the blockmodel function from sna sbm_output = sna::blockmodel(net_mat, class_assignments, glabels = &quot;Feudal&quot;, plabels = colnames(net_mat)) plot(sbm_output) plot(net, vertex.color = class_assignments) Pretty bad too… it seems to just be picking up degree (or something close to it). 16.6 Feature-based strategies Finally, an idea which is currently being used in computer science is to capture features for nodes to classify them into kinds of roles and positions depending on that feature set. For example, we could measure nodes’ centralities, the motifs in their local neighborhoods (i.e. triangles/MAN), their average distance from other nodes, etc. and then cluster them according to those features. In a sense, this assumes the least about the data and also is most feasible for large graphs (since such features are usually computationally trivial to measure). Here is a quick example: # this function limits the network to a given node&#39;s neighborhood (defined n-steps away) and then runs the triad census on that neighborhood. local_man &lt;- function(net, vertex_name, steps = 2){ n_steps &lt;- ego(net, order = steps, vertex_name, mode = &quot;out&quot;)[[1]] subnet &lt;- igraph::delete.vertices(net, !V(net) %in% n_steps) local_triad_count &lt;- igraph::triad.census(subnet) return(local_triad_count) } onestep_triad_counts &lt;- lapply(V(net)$name, FUN = function(x) local_man(net, x, steps = 1)) twostep_triad_counts &lt;- lapply(V(net)$name, FUN = function(x) local_man(net, x, steps = 2)) threestep_triad_counts &lt;- lapply(V(net)$name, FUN = function(x) local_man(net, x, steps = 3)) onestep_triad_counts &lt;- do.call(&quot;rbind&quot;, onestep_triad_counts) twostep_triad_counts &lt;- do.call(&quot;rbind&quot;, twostep_triad_counts) threestep_triad_counts &lt;- do.call(&quot;rbind&quot;, threestep_triad_counts) # different centrality measures indegrees &lt;- igraph::degree(net, mode = &quot;in&quot;) outdegrees &lt;- igraph::degree(net, mode = &quot;out&quot;) # average distance with other nodes average_distances &lt;- rowMeans(distances(net)) # put them all together graph_features &lt;- cbind(onestep_triad_counts, twostep_triad_counts, threestep_triad_counts, indegrees, outdegrees, average_distances) # run kmeans on the resulting dataset cluster_ids &lt;- kmeans(graph_features, 5)$cluster # plot the results plot(net, vertex.color = cluster_ids) Pretty good! Of course, we knew the number of roles (k) beforehand. There are algorithms for choosing k if it is not known. This strategy can be very dependent on the set of features you include. 16.7 Lab: Try these different strategies on a real graph of your choosing (go with smaller networks) and compare the different results using correlation. Which methods perform similarly? Do the roles that they identify look meaningful? Plot the network coloring the nodes by role. "],["culture-and-networks.html", "17 Culture and Networks 17.1 RCA 17.2 Semantic Network Analysis 17.3 From counts to a document-to-term matrix 17.4 Producing a skip-gram matrix for semantic network analysis and embedding models 17.5 Comparing texts with cosine", " 17 Culture and Networks This week we discussed a number of studies which employ network analysis for the study of culture. Most, if not all, build off Breiger’s seminal 1972 paper on duality. Duality has been a central concept in Western philosophy since at least Spinoza; Breiger provides a network theoretic use for it. This use, as we saw in Fararo and Doreian (1984), can be extended beyond its original bipartite formulation to multipartite cultural structures, thereby capturing the inherent complexity in socio-cultural relations. At the core of these strategies then are matrices which link people to things - whether words or survey items - which can in turn be connected via tertiary relations to other things. Latour shows that by tracing the linkages, between people to things to other things perhaps back to people, we can begin to understand the possibilities for and constraints on social action. In this tutorial, we cover some of the methods which have been used to study culture with networks. We begin with relational class analysis using the RCA package developed by Amir Goldberg and Sarah K. Stein. It implements the algorithm from Goldberg’s 2011 paper introducing RCA and links natively with igraph for visualization and downstream analyses. We will then move to semantic networks (Rule et al. 2015; Hoffman 2019) comparing and contrasting two different strategies for graphing texts as networks. The first focuses on a single text (or a related body of texts) and uses words as the unit of analysis to reveal the text’s internal structure. The second operates more akin to topic modeling with texts as the unit of analysis and ties representing semantic similarity. These representations, as we will see, are dual to each other. 17.1 RCA Goldberg and Stein have implemented RCA in R and provided the package on R CRAN. I have put a subset of the GSS data on music taste that he and others have used for validating these methods. The data link people to musical genres, identifying classes of respondents who construe musical genres in similar ways. So for example, I might think of music in a classed manner - such that highbrow musical genres like classical and jazz are opposed to lowbrow musical genres like hard rock and country. Alternatively, I might focus more on the progressiveness of genres - opposing traditional forms like folk and classical against newer genres like hip hop and rock. Depending on which construal I subscribe to, classical music might be associated with very different sets of genres, independent of whether a person likes or dislikes classical music. We might also infer that a person’s like or dislike of classical music in the first case has very different underlying reasoning than in the second (for example, they are elitist vs. they have conservative tastes). Let’s test these ideas by first loading in the GSS data gss_music_taste &lt;- readRDS(&quot;Data/gss_music_taste.RDS&quot;) Let’s take a quick peak at the data. View(gss_music_taste) The columns correspond to musical genres, the rows correspond to people, and the values in the data correspond to how much a respondent rerpoted liking the genere on a scale of 1 to 5. We could imagine just treating this as a sort of weighted bipartite network and analyzing it as such, just like we learned in the affiliation data tutorial. Instead, we will load in the RCA package and use that to identify clusters of genres. Let’s install and load in the RCA package. install.packages(&quot;RCA&quot;) library(RCA) ## Loading required package: igraph ## ## Attaching package: &#39;igraph&#39; ## The following objects are masked from &#39;package:sna&#39;: ## ## betweenness, bonpow, closeness, components, degree, dyad.census, ## evcent, hierarchy, is.connected, neighborhood, triad.census ## The following objects are masked from &#39;package:network&#39;: ## ## %c%, %s%, add.edges, add.vertices, delete.edges, delete.vertices, ## get.edge.attribute, get.edges, get.vertex.attribute, is.bipartite, ## is.directed, list.edge.attributes, list.vertex.attributes, ## set.edge.attribute, set.vertex.attribute ## The following objects are masked from &#39;package:dplyr&#39;: ## ## as_data_frame, groups, union ## The following objects are masked from &#39;package:purrr&#39;: ## ## compose, simplify ## The following object is masked from &#39;package:tidyr&#39;: ## ## crossing ## The following object is masked from &#39;package:tibble&#39;: ## ## as_data_frame ## The following objects are masked from &#39;package:stats&#39;: ## ## decompose, spectrum ## The following object is masked from &#39;package:base&#39;: ## ## union ## Loading required package: gplots ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess It’s probably worth taking a look at the documentation before we dive too deeply into the analyses. ?RCA The main function is RCA() and it takes a matrix. The parameters max and min allow you to tell the function what the maximum and minimum valeus can be in the matrix, otherwise the algorithm will just use max() and min() to find them. This would be useful for example if you provided a survey with a 1-100 scale but no one rated any of the items 100. The last parameters govern the bootstrap strategy used to identify significant relationality values in the RCA algorithm. Cool! Let’s run it on our music data. If we use the full data set, it will take a really long time (RCA is pretty slow). So we will first randomly sample 500 respondents. set.seed(1) rows_to_sample &lt;- sample(1:nrow(gss_music_taste), 200) gss_music_taste_500 &lt;- gss_music_taste[rows_to_sample,] Now let’s use RCA to analyze this smaller data set. Even with the smaller data, this will probably take a couple of minutes. rca_music_taste &lt;- RCA(gss_music_taste_500) Did it run? We can take a look at the resulting RCA object. summary(rca_music_taste) ## Length Class Mode ## membership 200 -none- numeric ## modules 17 -none- list ## R 40000 -none- numeric print(rca_music_taste) ## RCA found 17 relational classes. Sizes: 1 39 1 1 1 1 1 1 1 1 1 1 1 1 61 36 51 How many groups are there? It looks like it found 5 classes, but only three are sizeable: classes 1, 4, and 5. We will focus on those in our analyses. We could plot the results as a network to see how the different groups organize the cultural space of genres. The network plotting option however isn’t very good. It is better to use the heatmap option, which plots the correlation matrix of variables within each group. 17.1.1 Group 1 plot(rca_music_taste, module = 1, heatmap = T) 17.1.2 Group 4: plot(rca_music_taste, module = 4, heatmap = T) 17.1.3 Group 5: plot(rca_music_taste, module = 5, heatmap = T) What do the results show? The membership vector will tell us which group each person belongs to. We can use this information in later analyses to analyze predictors of class membership etc. For example, we might want to know if members of one class or another are more likely to adhere to a particular way of viewing music. head(rca_music_taste$membership) ## [1] 1 2 15 2 15 15 17.2 Semantic Network Analysis In this section, we will analyze books from Project Gutenberg, an online library of more than 60,000 public domain books, using semantic network analysis.Let’s take a look at the API for Project Gutenberg. R has a package, gutenbergr for Project Gutenberg, which we can use to download and analyze many famous books. First, we use install.packages() to install the gutenbergr package. install.packages(&quot;gutenbergr&quot;) Once it is installed, we can use library() to load it in. library(gutenbergr) The package comes with the metadata of each of the texts in the Project Gutenberg library preloaded in. You can use the View() function to look at all of the data. View(gutenberg_metadata) In many analyses, you may want to filter just for English works, avoid duplicates, and include only books that have text that can be downloaded. The gutenberg_works() function does this pre-filtering for you and you can even use it to select a specific author. dickens_books &lt;- gutenberg_works(author == &quot;Dickens, Charles&quot;) austen_books &lt;- gutenberg_works(author == &quot;Austen, Jane&quot;) conrad_books &lt;- gutenberg_works(author == &quot;Conrad, Joseph&quot;) We can use the gutenberg_download() function to download a text of interest. We have to pass to it the id of the book we want to download. Let’s download one of my favorites - Dumas’ Count of Monte Cristo! First we search for the title inside of the metadata using grepl. grepl returns TRUE where it can find a given string inside of a vector, and FALSE otherwise. monte_cristo &lt;- gutenberg_works(grepl(&quot;The Count of Monte Cristo&quot;, title)) Now let’s grab its id and use the gutenberg_download() function to download it. This might take a little while… monte_cristo_text &lt;- gutenberg_download(monte_cristo$gutenberg_id, mirror = &quot;http://gutenberg.readingroo.ms/&quot;) Now use View() to read the text. It is in the form of a data.frame, where the text column holds the lines of the book. View(monte_cristo_text) We could have passed the gutenberg_download() function two ids, or more, to get more texts. Let’s download 2 random texts. First, we randomly sample two book ids from the metadata. To make sure we all randomly sample the same texts, we can set a seed, which makes random samples reproducible. set.seed(100) books_w_text &lt;- gutenberg_metadata$gutenberg_id[gutenberg_metadata$has_text] sampled_texts &lt;- sample(books_w_text, 2) random_texts &lt;- gutenberg_download(sampled_texts, meta_fields = &quot;title&quot;, mirror = &quot;http://gutenberg.readingroo.ms/&quot;) What books did we get? unique(random_texts$title) ## [1] &quot;The Secret Power&quot; ## [2] &quot;The Apology of the Church of England&quot; Marie Corelli’s The Secret Power and The Apology of the Church of England. Why did we do this again? 17.2.1 Counting Words Now that we know how to download texts, we can analyze them! First it is worth installing tidytext, a package specifically useful for analyzing text data of this sort. install.packages(&quot;tidytext&quot;) Load it into R. We’ll need the rest of the tidyverse too (specifically dplyr and stringr). library(tidytext) library(tidyverse) Let’s analyze two texts we are actually familiar with - how about the Constitution and Pride and Prejudice? I located their id in the metadata, 5 and 1342 respectively. familiar_texts &lt;- gutenberg_download(c(5, 1342), meta_fields = &quot;title&quot;, mirror = &quot;http://gutenberg.readingroo.ms/&quot;) One strategy for comparing texts is to compare the counts of the words used in them. The first step for counting words is tokenizing the texts - which means breaking sentences and paragraphs down into their respective words. We will then count how frequently each word occurs. Tidy text has a useful function called unnest_tokens() which does just that. tokenized_words &lt;- unnest_tokens(familiar_texts, word, text) head(tokenized_words) ## # A tibble: 6 × 3 ## gutenberg_id title word ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 5 The United States Constitution these ## 2 5 The United States Constitution original ## 3 5 The United States Constitution project ## 4 5 The United States Constitution gutenberg ## 5 5 The United States Constitution etexts ## 6 5 The United States Constitution will Next, we need to remove common and therefore uninformative words in the texts - like but, and, about, he, she, etc. These are called stopwords and tidy already has some predefined stopwords for us to use. We can use the data() function to read them into our R. data(stop_words) head(stop_words) ## # A tibble: 6 × 2 ## word lexicon ## &lt;chr&gt; &lt;chr&gt; ## 1 a SMART ## 2 a&#39;s SMART ## 3 able SMART ## 4 about SMART ## 5 above SMART ## 6 according SMART Let’s remove the stop words using the anti_join() function, remove numbers and punctuation from them with str_extract and mutate, and then count up how often each word appears in each of the books using count(). word_counts &lt;- anti_join(tokenized_words, stop_words, by = &quot;word&quot;) word_counts &lt;- mutate(word_counts, word = str_extract(word, &quot;[a-z&#39;]+&quot;)) word_counts &lt;- count(word_counts, title, word, sort = TRUE) head(word_counts) ## # A tibble: 6 × 3 ## title word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Pride and Prejudice elizabeth 597 ## 2 Pride and Prejudice darcy 374 ## 3 Pride and Prejudice bennet 294 ## 4 Pride and Prejudice miss 283 ## 5 Pride and Prejudice jane 264 ## 6 Pride and Prejudice bingley 257 Counts are useful when we look at just one book, but when our goal is to compare many texts, they may be biased if one of our texts is much longer than another, and therefore has higher counts across every word. A common solution to this problem is to normalize counts by the total number of words in each text. Here we group the data by title, divide each count by the sum of the counts of all the words in each title and save the result as the variable proportion, remove the count variable keeping the proportions instead, and finally dropping any row in the data which has a missing value with drop_na(). Have we used piping from dplyr before? It is a useful way to string together a bunch of function calls, in a clear and informative manner, without having to reassign them to a new object each time. word_proportions &lt;- word_counts %&gt;% group_by(title) %&gt;% mutate(proportion = n / sum(n)) %&gt;% select(-n) %&gt;% spread(title, proportion) %&gt;% drop_na Cool, now that we have the frequency with which each word occurs in each text, let’s visualize their differences. Let’s focus on Pride and Prejudice first. We use the top_n() function to select only the top 20 texts with respect to proportion. We reorder the dataset according to proportion. We then plot the results as a bar plot using geom_col(). To make the result go horizontally we also use coord_flip(), which flips the coordinates of the plot. top_words_pandp &lt;- word_proportions %&gt;% top_n(20, `Pride and Prejudice`) %&gt;% mutate(word = reorder(word, `Pride and Prejudice`)) ggplot(top_words_pandp, aes(word, `Pride and Prejudice`)) + geom_col() + xlab(NULL) + coord_flip() We do the same for The United States Constitution. top_words_const &lt;- word_proportions %&gt;% top_n(20, `The United States Constitution`) %&gt;% mutate(word = reorder(word, `The United States Constitution`)) ggplot(top_words_const, aes(word, `The United States Constitution`)) + geom_col() + xlab(NULL) + coord_flip() Awesome! 17.3 From counts to a document-to-term matrix What if we had hundreds of books and wanted to not just evaluate their individual word counts, but wanted to compare with respect to their compositions? We can build up from the individual document counts to a matrix - what we will call a document-to-term matrix - to build our semantic networks. We can compare all of Charles’ Dickens works to see how similar they are and to identify novels with similar compositions. First, download Dickens’ texts with gutenberg_download(), just like before. dickens_texts &lt;- gutenberg_download(dickens_books$gutenberg_id, meta_fields = &quot;title&quot;, mirror = &quot;http://gutenberg.readingroo.ms/&quot;) Next we tokenize them and count how often each word occurs in each text. dickens_words &lt;- dickens_texts %&gt;% unnest_tokens(word, text) %&gt;% count(title, word, sort = TRUE) Let’s evaluate the number of words in each text, add the result to the original data with left_join(), and finally, take a look at the resulting data.frame. total_words &lt;- dickens_words %&gt;% group_by(title) %&gt;% summarize(total = sum(n)) dickens_words &lt;- left_join(dickens_words, total_words) ## Joining, by = &quot;title&quot; head(dickens_words) ## # A tibble: 6 × 4 ## title word n total ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 The Pickwick Papers the 18347 305070 ## 2 Sketches by Boz, Illustrative of Every-Day Life and Every-… the 17582 258897 ## 3 Dombey and Son the 17065 359055 ## 4 Little Dorrit the 15731 341737 ## 5 Martin Chuzzlewit the 15290 340421 ## 6 Nicholas Nickleby the 15003 326369 Before running cosine on the data, it is common to convert the raw counts into tf-idfs. The idea with tf-idf is that we want to weight words in each text depending on how special or specific they are to a given text, emphasizing those words that are important to each text. Then, when we run cosine, we won’t accidentally treat two texts as similar if they just both use a bunch of common words. Instead, they will be seen as similar only if they share important words. Tf-idf stands for term frequency - inverse document frequency values. It is calculated by multiplying how frequently a word occurs in each text (term frequency) by the inverse of how many of the documents in the sample it can be found (i.e. idf = 1/document frequency). Tidy text has a convenient function, bind_tf_idf, for calculating it. We just have to tell it - the name of our word column, the column which identifies which texts we are referring to, and the word counts column. dickens_words &lt;- dickens_words %&gt;% bind_tf_idf(word, title, n) What do the results look like? dickens_words %&gt;% select(-total) %&gt;% arrange(desc(tf_idf)) %&gt;% head() ## # A tibble: 6 × 6 ## title word n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 &quot;Captain Boldheart &amp; the Latin-Grammar Ma… boldhea… 51 0.0126 3.21 0.0405 ## 2 &quot;The Magic Fishbone\\nA Holiday Romance fr… alicia 33 0.00838 3.21 0.0268 ## 3 &quot;Scenes and Characters from the Works of … chap 773 0.0384 0.666 0.0256 ## 4 &quot;Scenes and Characters from the Works of … illustr… 868 0.0431 0.566 0.0244 ## 5 &quot;A Christmas Carol&quot; scrooge 327 0.0110 2.00 0.0221 ## 6 &quot;A Christmas Carol in Prose; Being a Ghos… scrooge 314 0.0109 2.00 0.0218 Let’s plot the top fiften words, by tf-idf, for four Dickens classics dickens_words %&gt;% filter(title %in% c(&quot;A Christmas Carol&quot;, &quot;Oliver Twist&quot;, &quot;A Tale of Two Cities&quot;, &quot;David Copperfield&quot;)) %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% group_by(title) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% ggplot(aes(word, tf_idf, fill = title)) + geom_col(show.legend = FALSE) + labs(x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~title, ncol = 2, scales = &quot;free&quot;) + coord_flip() ## Selecting by tf_idf Now to turn it into a matrix all we have to do is use the cast_sparse function, specify which variables should be on the rows and columns, and how we want to fill the matrix. dickens_dtm &lt;- cast_sparse(dickens_words, row = title, col = word, value = tf_idf) 17.4 Producing a skip-gram matrix for semantic network analysis and embedding models As is the case with embedding models, you may also want to model not just the set of words that each text has, but rather, the precise sequences of words that are employed in each text. Doing so should reveal not just composition, but how words relate to each other within and across texts; or put differently, the local context in which words appear. To produce such models, we need a different kind of data from the raw co-occurence matrices we made use of before. We will use what is called a skip-gram model, which counts the number of times a given word appears near another word, with near-ness being defined as some kind of window, say, of 4 or 5 words. A skip-gram window of two, for example, would count the number of times word j appears within the two words immediately before or after word i. Rather than building a skip-gram model ourselves, we can just use the CreateTcm() function from the textmineR package of last lesson to turn a text of interest into a skip-gram. First, let’s install the textmineR package and load it in. install.packages(&quot;textmineR&quot;) library(textmineR) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## ## Attaching package: &#39;textmineR&#39; ## The following object is masked from &#39;package:Matrix&#39;: ## ## update ## The following object is masked from &#39;package:stats&#39;: ## ## update The textmineR package has a super useful function - CreateTcm() - which produces a skip-gram matrix, just like we want. All we have to do is specify the texts we want to run the skip-gram model on, the size of the skip-gram window, and how many cpus we want to use on our computer. The problem is that gutenbergr doesn’t give us texts in a single string, so we will have to do some work to prepare the data. While we are doing this, we might as well clean the data too. We can turn this data set into the classic tokenized tidytext data set like we were working on before with the unnest_tokens function. tokens &lt;- dickens_texts %&gt;% unnest_tokens(word, text) Next we want to drop words which are less than three characters in length, and drop stop words. We can drop short words with filter combined with the nchar function, and anti_join to drop stopwords. tokens &lt;- tokens %&gt;% filter(!(nchar(word) &lt; 3)) %&gt;% anti_join(stop_words) ## Joining, by = &quot;word&quot; Next we drop empty words. # filter out empty words tokens &lt;- tokens %&gt;% filter(!(word==&quot;&quot;)) The next part is a bit complicated. The basic idea is that we want to paste the texts for each book together. The unite function is good for that, but it only works on a wide form data set. So we will first group by book, produce an index for the row number (that is, what position is a given word in each text), we will then spread the data, converting our long form data into wide form, setting the key argument (which defines the columns of the new data.frame) to equal the index we created, and the value argument to word. The result is that each book is now its own row in the data.frame, with the column i+2 identifying the ith word in a given book. tokens &lt;- tokens %&gt;% group_by(title) %&gt;% mutate(ind = row_number()) %&gt;% spread(key = ind, value = word) We’ll convert NAs to \"\" and use unite to paste all of the columns in the data.frame together. We specify -title and -gutenberg_id so that those columns are preserved and not pasted with the words of each book # convert NAs to empty strings # tokens[is.na(tokens)] &lt;- &quot;&quot; # put the data.frame back together tokens &lt;- unite(tokens, text, -title, -gutenberg_id, sep =&quot; &quot; ) Great, now each book is its own row in the data and there is a column which includes all of its text. We can now use CreateTcm to create a skip_gram matrix. This might take a while. skip_gram_dickens &lt;- CreateTcm(doc_vec = tokens$text, skipgram_window = 10, verbose = FALSE, cpus = 4) The result is a word-to-word matrix, where cells are weighted by how frequently two words appear in the same window. 17.5 Comparing texts with cosine We can easily turn either of the above matrices into networks by graphing them directly with igraph ( bipartite or unipartite, respectively). It is better, however, to use cosine analysis first. To do this, we treat each text or word’s distribution of words as a vector in multidimensional space (where the number of dimensions are the number of words in the data) and then use cosine similarity to take the angle of every pair of vectors. This is an intuitive measure of similarity - where values close to 1 signify that two vectors (i.e. the word distributions of two texts) are going in precisely the same direction, and values close to 0 signal that they are going in orthagonal directions (i.e. they are not similar at all). Let’s try this with both of the matrices above. We will use the proxy package to calculate cosine similarity. install.packages(&#39;proxy&#39;) library(proxy) ## ## Attaching package: &#39;proxy&#39; ## The following object is masked from &#39;package:Matrix&#39;: ## ## as.matrix ## The following objects are masked from &#39;package:stats&#39;: ## ## as.dist, dist ## The following object is masked from &#39;package:base&#39;: ## ## as.matrix proxy has a useful function - simil() - which calculates similarity for you. All you have to do is specify your data and the method you want to use for measuring similarity. It has a lot of different methods, but we will use cosine. One thing you might notice is that these matrices are huge! We will have trouble analyzing them in our computers. There are many different strategies for extracting meaningful and useful terms (something called term extraction in computer science). Here is one simple strategy: evaluating the log-likelihood of each term in the corpus and selecting those with the highest log likelihood value. We will limit our matrices to the top 1000 terms (though a larger number is generally better as we are essentially throwing out data here…) log_likelihood_terms &lt;- function(dtm){ b = colSums(dtm) b = ifelse(b[] == 0, 1e-12, b[]) LLs = c() for(i in 1:nrow(dtm)){ a = dtm[i,] a = ifelse(a[] == 0, 1e-12, a[]) c = sum(a) d = sum(b) E1 = c*(a+b)/(c+d) E2 = d*(a+b)/(c+d) LL = 2*((a*log(a/E1))+(b*log(b/E2))) LL = sum(LL) LLs = c(LLs, LL) length(LLs)/ nrow(dtm) } names(LLs) = rownames(dtm) LLs = LLs[order(LLs, decreasing = T)] return(LLs) } dickens_dtm &lt;- as.matrix(dickens_dtm) ll_terms = log_likelihood_terms(t(dickens_dtm)) ll_terms = ll_terms[order(ll_terms, decreasing = T)] ll_terms = names(ll_terms[1:1000]) Now, we will do a couple of things. We will limit the matrices to only those terms which we deemed important using log-likelihood. We will then calculate cosine for every pair of words using the simil function from proxy. # For the document-to-term matrix cosine_dtm &lt;- simil(dickens_dtm[,ll_terms], method = &quot;cosine&quot;) cosine_dtm &lt;- as.matrix(cosine_dtm) # For the skip-gram matrix sg_ll_words &lt;- rownames(skip_gram_dickens) %in% ll_terms smaller_dickens_sg &lt;- skip_gram_dickens[sg_ll_words, sg_ll_words] smaller_dickens_sg &lt;- as.matrix(smaller_dickens_sg) cosine_sg &lt;- simil(smaller_dickens_sg, method = &quot;cosine&quot;) cosine_sg &lt;- as.matrix(cosine_sg) We could take the cosine of our cosine matrix again and again, again to CONCOR. We could therefore use this repeated cosine strategy to blockmodel the data. How do the new cosine matrices look? Notice that the dtm matrix became a book-to-book matrix. We could have first taken the transpose to produce a word-to-word matrix, but one with different values from the skip-gram cosine matrix. For each matrix, we can look at a value to see how similar two words or books are. For example, king and queen appear in very similar contexts according to the skip-gram cosine matrix. cosine_sg[&quot;queen&quot;, &quot;king&quot;] ## [1] 0.81195 This might remind you of word embeddings. Both rely on the skip-gram matrix underneath and so we should expect similarities. If, instead of taking cosine, you instead took the singular value decomposition of the skip-gram matrix, using the svd() function, you would essentially get word embeddings. Check out GloVe (https://nlp.stanford.edu/projects/glove/) from Stanford’s NLP group to learn more. Let’s plot the results as networks. To do so, we need igraph. library(igraph) We will filter the matrices so that it only contains values which are in the top 5% of the cosine distribution. For reference, here is the distribution for the document-to-term matrix. hist(cosine_dtm) We use indexing to filter the matrix. First, we identify values which are less than the 0.95 percentile of the distribution. Then we use that logical to index the matrix and set those values to zero. Finally, we set the diagonal to zero too. In many cases, it may behoove you to set an even more stringent cut-off. low_values_dtm &lt;- cosine_dtm &lt; quantile(cosine_dtm, 0.95, na.rm = T) cosine_dtm[low_values_dtm] &lt;- 0 diag(cosine_dtm) &lt;- 0 low_values_sg &lt;- cosine_sg &lt; quantile(cosine_sg, 0.95, na.rm = T) cosine_sg[low_values_sg] &lt;- 0 diag(cosine_sg) &lt;- 0 Now let’s use igraph to graph the resulting matrices as a network. We use graph.adjacency to graph the matrix, we set weighted = T, so that it takes into account the cosine values when doing so. We set the mode to undirected since the matrix is symmetrical. cosine_net_dtm &lt;- graph.adjacency(cosine_dtm, weighted = T, mode = &quot;undirected&quot;) cosine_net_sg &lt;- graph.adjacency(cosine_sg, weighted = T, mode = &quot;undirected&quot;) We drop nodes which are disconnected from the main graphs. comps &lt;- components(cosine_net_dtm)$membership largest_component &lt;- which.max(table(comps)) cosine_net_dtm &lt;- delete.vertices(cosine_net_dtm, which(comps != largest_component)) comps_sg &lt;- components(cosine_net_sg)$membership largest_component_sg &lt;- which.max(table(comps_sg)) cosine_net_sg &lt;- delete.vertices(cosine_net_sg, which(comps_sg != largest_component_sg)) And plot the result marking groups using the Louvain clustering algorithm! 17.5.0.1 DTM plot(cosine_net_dtm, vertex.size = 5, mark.groups = cluster_louvain(cosine_net_dtm), vertex.color = &quot;grey80&quot;, vertex.border.color = &quot;grey60&quot;, vertex.label.cex = 0.5, vertex.label.color = &quot;black&quot;) 17.5.0.2 SG plot(cosine_net_sg, vertex.size = 1, vertex.color = &quot;grey80&quot;, vertex.border.color = &quot;grey60&quot;, vertex.label.cex = 0.1, vertex.label.color = &quot;black&quot;) Of course, now it is just a network and you can graph and analyze it as one. sort(degree(cosine_net_dtm), decreasing = T)[1:10] ## Speeches: Literary and Social ## 27 ## Reprinted Pieces ## 22 ## Dickens&#39; Stories About Children Every Child Can Read ## 19 ## Charles Dickens&#39; Children Stories ## 14 ## American Notes ## 11 ## Oliver Twist, Vol. 3 (of 3) ## 10 ## Oliver Twist, Vol. 1 (of 3) ## 8 ## A Child&#39;s History of England ## 7 ## Oliver Twist ## 7 ## Oliver Twist, Illustrated\\nor, The Parish Boy&#39;s Progress ## 7 sort(degree(cosine_net_sg), decreasing = T)[1:10] ## dickens haredale maryon sidenote mistress dorrit pecksniff laura ## 198 192 189 182 179 178 175 172 ## assistant nupkins ## 171 171 "],["dynamics.html", "18 Dynamics 18.1 TERGMs 18.2 tergm syntax 18.3 RSiena -", " 18 Dynamics In our last week, we will cover network dynamics. In general, there are two primary methods sociologists tend to use when modeling networks over time: TERGMs (temporal ERGMs) and SAOMs (stochastic actor-oriented models). TERGMs build off of the ERGM framework to model network dynamics and therefore model network change primarily at the dyadic-level. SAOMs, on the other hand, models changes in ties as being determined by the actors in question. Each has its strengths and weaknesses, which we will discuss in the following tutorial. 18.1 TERGMs We’ll begin with TERGMs. Like ERGMs, TERGMs are part of the statnet suite of packages. The basic idea behind them is that rather than specifying a single model regarding how ties are formed in ERGM, we’ll specify two models - one governing formation and the other governing dissolution or persistence. In doing so, we specify the factors that govern tie change over time. It therefore requires data with multiple snapshots of a single network as it changed over time. Generally, this type of data comes in cross-sections, as in the case of Add Health, where network data was collected at two different time points, one year apart. As an example, we will use Sampson’s monastery data, which we mentioned in a previous class. Here networks represent monk’s social relations at a monastery at three different moments in time. library(statnet) data(samplk) statnet has a separate object for dynamic networks. We can create it from the network panels via a list and a simple function call, like so: samp_networks &lt;- list(samplk1,samplk2,samplk3) samp_dynm_net &lt;- networkDynamic(network.list = samp_networks) ## Neither start or onsets specified, assuming start=0 ## Onsets and termini not specified, assuming each network in network.list should have a discrete spell of length 1 ## Argument base.net not specified, using first element of network.list instead ## Created net.obs.period to describe network ## Network observation period info: ## Number of observation spells: 1 ## Maximal time range observed: 0 until 3 ## Temporal mode: discrete ## Time unit: step ## Suggested time increment: 1 Although they are in a single object now, you can still extract each network individually and plot them. Here is the network at time 2, for example. plot(network.extract(samp_dynm_net, at = 2)) statnet also has cool options for visualizing this kind of data. For example, one of its key features is the ability to calculate temporal paths in networks. Temporal paths trace the sequences and distances of nodes in a networkDynamic object reachable from an initial node, following paths constrained by edge timing and direction, if the network is directed. v11path &lt;- tPath(samp_dynm_net, v = 11, direction = &#39;fwd&#39;) v17path&lt;-tPath(samp_dynm_net, v=17, direction = &#39;fwd&#39;) coords&lt;-plot(samp_dynm_net, displaylabels=TRUE, label.cex=0.8, label.pos=5, vertex.col=&#39;white&#39;, vertex.cex=3, edge.label.cex=0.7 ) plotPaths(samp_dynm_net, coord=coords, list(v11path,v17path)) 18.1.1 TERGM terms and types TERGMs have very similar term names as ERGMs. For example, edges counts the number of ties in the network and nodematch captures homophily. What changes is how these terms are counted and used in the model, which will depend on the type of TERGM one specifies. This will in turn affect how the models are interpreted. While there are five different types of models one can specify within a TERGM, each governing a specific type of tie change in the network. Here, we focus on what is called a separable TERGM (STERGM). Separable models specify a process for tie dynamics using operators that are independent within time-step; the Form() + Diss() (or Persist()) model is the classic example. Here, the dyads can be partitioned into two sets at each timepoint: the empty dyads, which are subject to formation, and the tied dyads, which are subject to dissolution. A single dyad change will affect either the Form() model statistics or the Diss()/Persist() model statistics at each timestep, but not both. So the terms in the two operators are uncorrelated, and can be separately evaluated at each step for estimation or simulation. Form() tracks the formation of ties and its terms represent the factors that influence tie formation. Diss() tracks the dissolution of ties and its terms represent the factors that influence tie dissolution Note that while these separable sets are independent within timestep, they are still Markov-dependent between timesteps: the formation of a tie at time t can influence the dissolution of an adjacent tie at t+1. And within each operator set, there can still be dyad-dependence within step (if dyad-dependent terms are specified in the model). 18.2 tergm syntax For a call to tergm, you pass the formula, but this now will typically include more than one Operator (each with its own set of terms), and the estimation method, which will depend on the type of data you have. For example, for a model specified with Form() and Diss() operators: # tergm(my.dynamic.network ~ #do not run this! # Form(~ edges + gwesp(0, fixed=T)) + # Diss(~ edges + nodefactor(&#39;age&#39;)), # estimate = `insert method` # ) Let’s return to the Sampson monastery data, and consider models for the way this network changes across the 3 observed time points. This is a directed network, so it would be natural to consider mutuality as a predictor. We can also add terms for the presence of cycles and transitive triads. samp_fit &lt;- tergm( samp_dynm_net ~ Form(~edges+mutual+cyclicalties+transitiveties) + Diss(~edges+mutual+cyclicalties+transitiveties), estimate = &quot;CMLE&quot;, times = c(1:2) ) This is a small network, so we shouldn’t expect many of the terms to be significant. However, we do see that mutual ties are more likely to form and that ties which are in transitive triads are less likely to dissolve than those in intransitive triads. This may signal norms towards reciprocity and hierarchy formation. 18.3 RSiena - The SAOM is a probability model for network changes in continuous time, and the estimation theory was elaborated for network data available at discrete time points. Network ties are assumed to change one tie at a time (Holland and Leinhardt 1977), with actors choosing which of the outgoing ties to change based on a multinomial choice probability model (McFadden 1974). Dependence between ties is modeled by allowing the actors’ choices concerning tie changes to be influenced by the embedding of the potential ties in local configurations. These are akin to the local configurations in the ERGM but take the perspective of a focal actor, while there is no focal actor in the ERGM. Due to its actor-oriented formulation, the SAOM offers a direct interpretation of parameters as reflecting differential probabilities of actors’ choices with regard to their outgoing ties. install.packages(&quot;RSiena&quot;, repos=&quot;https://cloud.r-project.org&quot;) library(RSiena) First, we have to assemble the three different time points as a single three-dimensional array. This is the data format that RSiena requires. It implies that networks at each time point must have the same set of nodes. If a node is not part of your network at a certain time point, you should add them as an isolate. samp_mats &lt;- array(c(as.matrix(samplk1), as.matrix(samplk2), as.matrix(samplk3)), dim = c(18, 18, 3)) Next we will create siena data from the array using the sienaDataCreate() function. samp_siena &lt;- sienaDataCreate(sienaDependent(samp_mats)) You can get an outline of the data set with some basic descriptives from print01Report( samp_siena, modelname=&quot;sampson_monastery_siena&quot;) Now we will begin fitting the model. To do so, we first need to create an effects object. This is where the set of terms we want to include in our models will be stored. sampeff &lt;- getEffects(samp_siena) All the effects that you can possibly include in the model given your network’s structure can be seen using the effectsDocumentation() function. effectsDocumentation(sampeff) For a detailed description of all of these terms, see Chapter 12 in the RSiena manual. Let’s build a simple model. For example, we can build a basic structural effects model with reciprocity, transitivity, and 3-cycles sampeff &lt;- includeEffects(sampeff, transTrip, cycle3, recip) ## effectName include fix test initialValue parm ## 1 reciprocity TRUE FALSE FALSE 0 0 ## 2 transitive triplets TRUE FALSE FALSE 0 0 ## 3 3-cycles TRUE FALSE FALSE 0 0 Now we will initiate the algorithm. You can adjust how RSiena fits the model by changing the options in sienaAlgorithmCreate(). sampalgorithm &lt;- sienaAlgorithmCreate( projname = &#39;sampson_monastery_siena&#39; ) ## If you use this algorithm object, siena07 will create/use an output file sampson_monastery_siena.txt . Finally, we’ll run the model. samp_out &lt;- siena07( sampalgorithm, data = samp_siena, effects = sampeff) We can check the following to see if it converged properly. If the convergence criterion is less than 0.25, convergence is good. samp_out$tconv.max &lt; 0.25 ## [,1] ## [1,] TRUE Here, convergence is good. If convergence is inadequate, then you can estimate again using your previous output as the “previous answer” from which estimation continues. samp_out_continued &lt;- siena07( sampalgorithm, data = samp_siena, effects = sampeff, prevAns=samp_out) samp_out_continued$tconv.max &lt; 0.25 ## [,1] ## [1,] TRUE If convergence is good, you can look at the estimates using the summary() function. summary(samp_out) ## Estimates, standard errors and convergence t-ratios ## ## Estimate Standard Convergence ## Error t-ratio ## ## Rate parameters: ## 0.1 Rate parameter period 1 3.5408 ( 0.6619 ) ## 0.2 Rate parameter period 2 2.6203 ( 0.5250 ) ## ## Other parameters: ## 1. eval outdegree (density) -1.5187 ( 0.2018 ) -0.1029 ## 2. eval reciprocity 1.3855 ( 0.2853 ) -0.1177 ## 3. eval transitive triplets 0.3594 ( 0.1223 ) -0.1304 ## 4. eval 3-cycles -0.2229 ( 0.2123 ) -0.1064 ## ## Overall maximum convergence ratio: 0.1466 ## ## ## Total of 1907 iteration steps. ## ## Covariance matrix of estimates (correlations below diagonal) ## ## 0.041 -0.029 -0.016 0.003 ## -0.499 0.081 0.010 -0.022 ## -0.645 0.291 0.015 -0.015 ## 0.082 -0.362 -0.562 0.045 ## ## Derivative matrix of expected statistics X by parameters: ## ## 91.992 61.044 193.574 52.117 ## 31.220 42.238 70.209 23.099 ## 131.638 83.923 417.135 104.920 ## 51.912 44.267 163.402 58.824 ## ## Covariance matrix of X (correlations below diagonal): ## ## 100.620 69.180 224.305 61.143 ## 0.771 79.925 165.169 51.389 ## 0.841 0.695 706.700 188.817 ## 0.771 0.727 0.899 62.446 If you would like to update your model, you can use includeEffects() to add or remove effects. For example, the following will remove the 3-cycle term. sampeff &lt;- includeEffects(sampeff, cycle3, include = FALSE) ## [1] effectName include fix test initialValue ## [6] parm ## &lt;0 rows&gt; (or 0-length row.names) You can then refit the model. samp_out_wo_3cycle &lt;- siena07(sampalgorithm, data = samp_siena, effects = sampeff, prevAns=samp_out_continued) "]]
