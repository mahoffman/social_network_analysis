---
title: "Advanced Topics in Network Analysis 1: Centrality, Group Detection, Small Worlds and Simulation"
author: "Mark Hoffman"
output: html_document
---
# Advanced Topics in Network Analysis 1: Centrality, Group Detection, Small Worlds and Simulation

This tutorial is an expansive overview of some advanced topics in network analysis. We begin by looking at measures of network centrality - which are used to identify structurally important actors. We also discuss possible ideas for identifying important edges. We will then move to group detection and modularity, methods for endogenously identifying the boundaries of groups via their patterns of social interaction.

In social science, it is common to identify groups according to categorical membership. A non-network critique of this paradigm is Brubaker's "Ethnicity without Groups"; a network critique is Harrison White's catnet lectures. Both point to a similar pitfall with extant variable-based research - things like race, sex, occupation etc. are actors' own theories of how the world works, theories about the location of social boundaries.  How these theories act on the world, and how people use them and come to believe in them, is crucial to understanding social life; but the variables and categories themselves do not exist other in the extent that they are believed to exist by actors. 

Catnet's provide a way of 

## Centrality

Centrality originally referred to how central actors are in a network's structure. It has become abstracted as a term from its topological origins and now refers very generally to how important actors are to a network. Topological centrality has a clear definition, but many operationalizations. Network "importance" on the other hand has many definitions _and_ many operationalizations. We will explore the possible meanings and operationalizations of centrality here. There are four famous centrality measures: degree, betweenness, closeness and eigenvector - each of which has its own strengths and weaknesses. The main point we want to make is that the analytical usefulness of each depends heavily on the context of the network, the type of relation being analyzed and the underlying network morphology. We don't want to leave you with the impression that one is better than another - only that one might serve your research goals better than another.

For every node-level measure, there is a graph-level analogue. Centralization measures the extent to which the ties of a given network are concentrated on a single actor or group of actors. 

We can also look at the degree distribution.  It is a simple histogram of degree.  It can tell you whether the network is highly unequal or not.

### Loading the example network

As always, we need to load igraph.

```{r}
library(igraph)
```

The dataset we are loading is called padgett_marriage.csv 

It is a subset of a famous dataset collected by John Padgett on the relationships of prominent Florentine families in 15th century Italy. The historical puzzle is that the Medici, an upstart family, seizes political power in 1434. Padgett's goal was to explain how the Medici rose to power. 

He looked at many relations, but we only have the marriage here. Marriage was a tool in diplomacy, central to political alliances.
A tie is drawn between families if the daughter of one family was sent to marry the son of another. 

Ron Breiger has argued these edges should be directed, tracing where daughters were sent, but we will analyze with the symmetrized (undirected) version. That is, we don't differentiate between whether a son or daughter being sent from each family.

As always, we first load and prepare the dataset. 
```{r}
padgett_marriage <- read.csv("~/gh_repos/sna_teaching/Data/padgett_marriage.csv", stringsAsFactors = FALSE)
row.names(padgett_marriage) <- colnames(padgett_marriage)
padgett_marriage <- as.matrix(padgett_marriage)
```

This time the data comes in an adjacency matrix format, so we have to use graph.adjacency() to turn it into an igraph object.

```{r}
marriageNet <- graph.adjacency(padgett_marriage, mode = "undirected")
```

Let's see how it looks.

```{r}
plot(marriageNet)
```

Based on this plot, which family do you expect is most central?

### Degree Centrality

The simplest measure of centrality is degree centrality. It counts how many edges each node has - the most degree central actor is the one with the most ties. 

> **Note:** In a directed network, you will need to specify if in or out ties should be counted. These will be referred to as in or out degree respectively. If both are counted, then it is just called degree

Degree centrality is calculated using the degree function in R. It returns how many edges each node has.

```{r}
degree(marriageNet) 
```

Who is the most degree central? 

We can assign the output to a variable in the network and size the nodes according to degree.

```{r}
V(marriageNet)$degree <- degree(marriageNet) # assignment

plot(marriageNet, vertex.label.cex = .6, vertex.label.color = "black", vertex.size = V(marriageNet)$degree) # sized by degree
```

The problem is that the degree values are a little small to plot well. We can use a scalar to increase the value of the degree but maintain the ratio.

```{r}
plot(marriageNet, 
     vertex.label.cex = .6, 
     vertex.label.color = "black", 
     vertex.size = V(marriageNet)$degree*3)
```

### Betweenness Centrality

Betweenness centrality captures which nodes are important in the flow of the network. It makes use of the shortest paths in the network. A path is a series of adjacent nodes. For any two nodes we can find the shortest path between them, that is, the path with the least amount of total steps (or edges). If a node C is on a shortest path between A and B, then it means C is important to the efficient flow of goods between A and B.  Without C, flows would have to take a longer route to get from A to B. 

Thus, betweenness effectively counts how many shortest paths each node is on. The higher a node's betweenness, the more important they are for the efficient flow of goods in a network.

In igraph, betweenness() computes betweenness in the network

```{r}
betweenness(marriageNet, directed = FALSE)
```

We can again assign the output of betweenness() to a variable in the network and size the nodes according to it.

```{r}
V(marriageNet)$betweenness <- betweenness(marriageNet) # assignment

plot(marriageNet, 
     vertex.label.cex = .6, 
     vertex.label.color = "black", 
     vertex.size = V(marriageNet)$betweenness) # sized by betweenness
```

Betweenness centrality can be very large. It is often helpful to normalize it by dividing by the maximum and multiplying by some scalar when plotting. 

```{r}
plot(marriageNet,
     vertex.label.cex = .6, 
     vertex.label.color = "black", 
     vertex.size = V(marriageNet)$betweenness/max(V(marriageNet)$betweenness) * 20)
```

### Closeness Centrality

With closeness centrality we again make use of the shortest paths between nodes. We measure the distance between two nodes as the length of the shortest path between them. Farness, for a given node, is the average distance from that node to all other nodes.
Closeness is then the reciprocal of farness (1/farness).

```{r}
closeness(marriageNet)
```

We assign it to a node variable and plot the network, adjusting node size by closeness.

```{r}
V(marriageNet)$closeness <- closeness(marriageNet)
```

```{r}
plot(marriageNet,
     vertex.label.cex = .6, 
     vertex.label.color = "black", 
     vertex.size = V(marriageNet)$closeness/max(V(marriageNet)$closeness) * 20)
```

## Eigenvector Centrality

Degree centrality only takes into account the number of edges for each node, but it leaves out information about ego's alters.

However, we might think that power comes from being tied to powerful people. If A and B have the same degree centrality, but A is tied to all high degree people and B is tied to all low degree people, then intuitively we want to see A with a higher score than B. 

Eigenvector centrality takes into account alters' power. It is calculated a little bit differently in igraph. It produces a list object and we need to extract only the vector of centrality values.

```{r}
evcent(marriageNet)$vector
```

Then we can assign that vector to our network and plot it.

```{r}
V(marriageNet)$eigenvector <- evcent(marriageNet)$vector

plot(marriageNet,
     vertex.label.cex = .6, 
     vertex.label.color = "black", 
     vertex.size = V(marriageNet)$eigenvector/max(V(marriageNet)$eigenvector) * 20)
```

We can use the function bonpow() for the same purpose.  However, Bonacich cleverly notes that being connected to powerful alters can also be a disadvantageous in certain contexts.

Can you think of context where this might be the case?

He proposes a measure that can be manipulated to accomodate cases where having powerful friends is either helpful or hurtful. If the exponent is negative, alters' having higher centrality score hurts ego's score; and vise versa.

```{r}
bonpow(marriageNet, exponent = -1)
bonpow(marriageNet, exponent = 1)
```

### Measure Correlations

Most of these measures are highly correlated, meaning they don't necessarily capture unique aspects of pwoer. However, the amount of correlation depends on the network structure. Let's see how the correlations between centrality measures looks in the Florentine Family network. cor.test(x,y) performs a simple correlation test between two vectors.

```{r}
cor.test(V(marriageNet)$degree, V(marriageNet)$betweenness)
cor.test(V(marriageNet)$degree, V(marriageNet)$eigenvector) 
cor.test(V(marriageNet)$degree, V(marriageNet)$closeness)
cor.test(V(marriageNet)$betweenness, V(marriageNet)$eigenvector)
cor.test(V(marriageNet)$betweenness, V(marriageNet)$closeness)
```

What do we learn?

### Centralization and Degree Distributions

To understand the measures further, we can look at their distributions. This will tell us roughly how many nodes have centralities of a given value.

```{r}
hist(V(marriageNet)$betweenness)
```

We can see that most nodes in the marriage network have low betweenness centrality, and only one node has more than 40 betweenness. Degree distributions tend to be right-skewed; that is, only a few nodes in most networks have most of the ties. Evenly distributed degree is much rarer.

Finally centralization measures the extent to which a network is centered around a single node. The closer a network gets to looking like a star, the more centralization will be. 

```{r}
centralization.degree(marriageNet)
centralization.betweenness(marriageNet)
centralization.evcent(marriageNet)
centralization.closeness(marriageNet)
```

## Finding Groups in Networks Tutorial

Groups are one of the many tools we have for analyzing network structure. Group detection focuses on the presence of ties - in attempt to identify densly connected groups of actors. There are many different tools for doing this - we will cover four: component analysis, k-cliques, modularity, and cohesive blocking.  Each method has its own uses and theoretical underpinnings, so I put citations of famous papers that use each method at the bottom of the script. 

### Component analysis

The most basic form of network group is a component.  In a connected component, every node is reachable via some path by every other node. Most network datasets have only a single large connected component with a few isolates - however, some unique datasets might have three or four large, distinct components.

In a directed graph, components can be weakly or strongly connected. If node i can reach j via a directed path and j can reach i via a directed path, for all i and j nodes in the component, then we say the component is strongly connected. If all nodes are only reachable from a single direction, (i.e. i can reach j via a directed path, but j can't reach i), then we say the component is weakly connected.

The ?decompose.graph function in igraph will take a network and decompose it into its connected components. We can then analyze each component separately. 

```{r}
library(igraph)

g1 <- barabasi.game(100, 1, 5) # Generate a scale-free network
V(g1)$name <- as.character(1:100)

g2 <- erdos.renyi.game(100, graph.density(g1), directed = TRUE)
V(g2)$name <- as.character(101:200)

g3 <- graph.union(g1, g2, byname = TRUE) # The ?graph.union function combines two networks. 

plot(g3, vertex.label = NA, vertex.size=2, edge.arrow.size = .1) # Two distinct networks in a single plot
```

Now let's use decompose to isolate each component.  There will be two.

```{r}
component_list <- decompose.graph(g3, mode = "weak")
component_list
```

It returns a list with two graphs in it - one for each component

```{r}
plot(component_list[[1]], main = "The Scale-Free Graph", vertex.label = NA, vertex.size = 3)
plot(component_list[[2]], main = "The Random Graph", vertex.label = NA, vertex.size = 3)
```

### Cliques

Cliques are fully connected subgraphs within a network structure; they are like the caves in the caveman structure we learned about a few weeks ago. We often want to find all of the cliques in a network of various sizes.  We could have a theory for example that people will dress or behave similarly or affect those in their cliques. That is, we might imagine cliques to be meaningful for the outcomes we are interested in study. We can do this with the clique function in igraph. 

```{r}
clique_out <- cliques(g1)
length(clique_out)
```

Wow... imagine the number of cliques in a large network
If we want to look for cliques of a certain size we can use the min and max arguments.

```{r}
clique_four <- cliques(g1, min = 4, max = 4)
length(clique_four)/length(clique_out)
```

Cliques of size four make about 25% of total cliques in the network.

## Cohesive Blocking

Cohesive blocking builds on the idea of cliques.  It starts at the level of the component and identifies large substructures nested within the component. It then moves to those large substructures and identifies smaller and smaller nested substructures, until it reaches cliques. It is therefore a useful way to operationalize network embeddedness. We will run it on a small world network size 100. 

```{r}
g2 <- as.undirected(g2)
g4 <- watts.strogatz.game(1, 100 , 5, .1)
V(g4)$name <- as.character(1:100)

g5 <- graph.union(g2, g4)
```

The function for cohesive blocking is ?cohesive.blocks. It is very inefficient so it will take awhile to run. Run time depends on the number of edges in the network and the degree of nestedness. Don't even bother running it on very large networks.

```{r}
blocks <- cohesive.blocks(g5)  # The basic function for finding cohesive blocks

blocks(blocks) # This tells us what the blocks are and which nodes are in them

cohesion(blocks) # This gives a cohesion score for each block. Cohesion is the minimum number of vertices you must remove in order to make the block not strongly connected.

plotHierarchy(blocks) # Finally, plotHierarchy shows the nestedness structure of the blocks.
```

### Group Detection


We often just want to find distinct groups of people. We might not require that everyone in a group is connected - it is too stringent. Instead, we could define groups as sets of nodes who have a higher proportion of edges going inwards than outwards - that is, solidarity is strongest within the group.

There are a plethora of group detection algorithms in igraph. I will cover some of them. The full list is: edge.betweenness.community, fastgreedy.community, label.propagation.community, leading.eigenvector.community, multilevel.community, optimal.community, spinglass.community, and walktrap.community.

This webpage has a summary of their pros and cons for an older version of igraph: http://bommaritollc.com/2012/06/summary-community-detection-algorithms-igraph-0-6/

Essentially you should tailor your algorithm choice to your network. Certain algorithms are designed for directed or undirected graphs, and work better with small or large graphs. 

Each algorithm as its own igraph function. These functions produce lists with information about the algorithm results. Element 1 holds a vector that details which group each node is in, which I will refer to as the membership vector. Element 6 holds the modularity of the network given the group detection algorithm. 

For undirected graphs, you can use the optimal or multilevel algorithms.

```{r}
communityMulti <- multilevel.community(g4)
```

For directed graphs, edge betweenness is generally your best bet, though the walktrap algorithm performs well too.

```{r}
communityWalk <- walktrap.community(g2)
communityEB <- edge.betweenness.community(g2)
communityInfo <- infomap.community(g2)
```

We might wish to visualize the group structure of networks to see how the algorithms divide the network. To do so, we have to first extract the membership vector and put it in a format the plot function understands.

```{r}
communityMultiGroup <- by(seq_along(communityMulti$membership), communityMulti$membership, invisible)
communityMultiGroup
```

We can then use the mark.groups argument in plot to see the groups. 
```{r}
plot(g4, vertex.size = 3, vertex.label = NA, mark.groups = communityMultiGroup)
```

Alternatively, we can just color the nodes by group membership.  In this case, we can just use the membership vector.

```{r}
V(g4)$color <- communityMulti$membership
plot(g4, vertex.size = 3, vertex.label = NA)
```

This becomes less feasible as the number of groups increases!!


### Modularity

Modularity takes a given group structure and calculates how separated the different groups are from each other. It therefore operationalizes our notion of groups above by calculating the proportion of ties that are within groups as opposed to between them. Networks with high modularity have dense connections between the nodes within modules but sparse connections between nodes in different modules.


We can think of modularity as both a measure of how effective a given grouping algorithm is - i.e. higher modularity means the alogrithm is identifying distinct, sociall separate groups. But it can also be thought of as a measure of the saliency of groups to the network in general. The higher modularity the more that groups structure the network. Modularity measures the extent to which a network can be divided into distinct modules or groups.

Getting a network's modularity in igraph is easy! 

```{r}
communityMulti <- multilevel.community(g4)
communityMulti$modularity
```

We can either access the modularity score directly 
OR 

```{r}
modularity(communityWalk) # We can use the modularity() function on a group detection output.
```

## Affiliation Data

This portion of the tutorial focuses on affiliation data. Individuals can be directly linked to one another by affections or interactions. We have spent most of the class so far detailing and analyzing the various types of direct relations. 

However, they can also be linked through "affiliations", that is, shared associations to groups or objects.  

As an example, people might be tied by the classes they have taken together. Such data might look like:

Person,  Classes
Leo,     Biostatistics, Chemistry, Linear Algebra
Clement, Islamic Civilization, The Modern World-System, Exile and Diaspora
Paula,   Calc 1, Calc 2, Linear Algebra, 
Filippo, Linear Algebra, Social Networks, The Modern World-System

We can create a network with two types of nodes - one set of nodes will be people, the other classes. People, in this network, cannot be directly tied to each other. Rather they are co-affiliated with a class, which serves as the basis of their connection. Therefore, all ties will be between nodes of different types.

To create this network, we need to turn the above data into an edgelist, convert it to a matrix, and plot it in igraph.

Let's start with the data.

```{r}
classes_data <- data.frame(name = c("Leo", "Clement", "Palla", "Filippo"), class1 = c("Biostatistics","Islamic Civ", "Calc 1", "Linear Algebra"), class2 = c("Chemistry", "The Modern World-System", "Calc 2", "Social Networks"), class3 = c("Linear Algebra", "Exile and Diaspora", "Linear Algebra", "The Modern World-System"), stringsAsFactors = FALSE)

classes_data
```

The reshape packages will let us convert this type of data into an edgelist.

```{r}
# install.packages("reshape2")
library(reshape2)
classes_data <- melt(classes_data, measure.vars = c("class1", "class2","class3"), value.name = "classes", variable.name = "order")
```

The ?melt function turns so called "short form data" into "long form". It takes the class variables and combines them into a single variable "classes". We only need two columns, name and classes, so we use the subset function to select them. If we look at the data now, it is basically an edge list, in which people are on the left side and classes they are affiliated with on the right.

```{r}
classes_data <- subset(classes_data, select = c("name", "classes"))
```

Once we have such an edge list, we can then use the table function to turn it into an incidence matrix, which is what igraph needs to turn affiliation data into an igraph object.

```{r}
classesMatrix = table(classes_data)
class(classesMatrix) <- "matrix" # And we convert it from a table to a matrix

# View(classesMatrix)
```

In an incidence matrix, the rows are of one class of node, while columns are of another. The rows are generally people who are affiliated with groups in the columns. 

Using the get.incidence() function will turn our matrix into a bipartite network. 

```{r}
classesNet <- graph.incidence(classesMatrix, mode = c("all"))
plot(classesNet, vertex.label.cex = .6, vertex.label.color = "black")
```

We can change the shape of nodes to highlight their type.

```{r}
V(classesNet)$shape <- ifelse(V(classesNet)$type == FALSE, "circle", "square")
plot(classesNet, 
     vertex.label.cex = .6, 
     vertex.label.color = "black")
```

### Unipartite Projection

Bipartite networks can be represented (or "projected") as unipartite networks.  In this case, either people will be the only nodes, and they will be connected if they share an affiliation (i.e. they are in the same group) OR groups willbe the only nodes and they will be connected if they share an affiliation to a person. 

We can make the projection two ways - using the bipartite.projection() function in igraph, or by multiplying the incidence matrix by its transpose (or vise versa).  

The mathematical operation to make a person-to-person projection is to multiply the initial matrix by its transpose. In R that looks like:

```{r}
personMatrix = classesMatrix %*% t(classesMatrix) 
# View(personMatrix)
```

where the t() function transposes the matrix that is passed to it and %*% performs matrix multiplication. 

The diagonal of this new matrix tells us the number of groups each person is affiliated with, but we set it to 0 using the ?diag function.

```{r}
diag(personMatrix) <- 0 
# View(personMatrix)
```

To get the group-to-group matrix, we multiply the transpose by the initial matrix (reverse!)

```{r}
groupMatrix = t(classesMatrix) %*% classesMatrix
# View(groupMatrix) # The diagonal details the number of people in each class

diag(groupMatrix) <- 0 # we again set it to 0
```

Both of these operations turn our rectangular incidence matrix into a square adjacency matrix. Order matters. Now that we have adjacency matrices can use the graph.adjacency() function to turn them into network objects.

```{r}
personNet <- graph.adjacency(personMatrix, mode = "undirected")
groupNet <- graph.adjacency(groupMatrix, mode = "undirected")

plot(personNet, vertex.label.cex = .6, vertex.label.color = "black")
plot(groupNet, vertex.label.cex = .6, vertex.label.color = "black")
```

We can analyze these networks just like we would any other network with a single node type.

## The Small World Problem and the Art-Science of Simulation

Real-world social networks tend to be small worlds.  In a small world, people are clustered in groups, but despite this, are still, on average, socially proximate. For example, you might think that you are socially (and spatially) distant from a random villager in India, but find that through a series of steps, you could reach that villager. The villager lives in her own small world and you live in yours, and yet you are mutually reachable. This is referred to as "the Small-World Phenomenon".

Duncan Watts in his landmark paper explains this phenomenon. He begins with most clustered (and yet connected graph) imaginable - a "caveman" structure. There are groups of people clustered together and connected by only one or two connections to other groups.  

Sadly, igraph doesn't have a function for simulating caveman structures, so I quickly wrote one myself. In this caveman structure, all of the groups will be the same size, so the number of people must be evenly divisible by the size of groups. The basic idea is to generate a bunch of fully connected groups and then connect them by an edge or two so that they are arrayed around a circle.  

```{r}
simulate_caveman <- function(n = 25, clique_size = 5){
  # Groups are all the same size, so I check whether N is divisible by the size of groups
  if ( ((n%/%clique_size) * clique_size) != n){
    stop("n is not evenly divisible by clique_size")
  }
  
  groups = n/clique_size # this determines the number of groups
  
  el <- data.frame(PersonA = 1:n, Group = NA) # I create a dataframe which has people and the groups they are in
  # I treat it like a person to group edgelist
  
  group_vector = c()
  for (i in 1:groups){
    group_vector <- c(group_vector, rep(i, clique_size))
  }  

  el$Group <- group_vector
  
  inc <- table(el) # I use the table function to turn the person to group edgelist into an incidence matrix
  adj <- inc %*% t(inc) # And I use matrix multiplication with the transpose to turn the person to group incidence matrix
  # into a person to person adjacency matrix
  
  diag(adj) <- 0 
  
  g <- graph.adjacency(adj, mode = "undirected") # I graph this matrix

  group_connect <- seq(from = 1, to = n, by = clique_size) # I determine the points of connection using a sequence funciton
  
  for( i in 1:(length(group_connect)-1)){
    p1 <- group_connect[i] + 1
    p2 <- group_connect[i+1]
    g <- add.edges(g, c(p1,p2)) # And I connect the points of connection using add.edges
  }
    g <- add.edges(g, c(group_connect[1],(group_connect[groups]+1))) # finally I connect the ends of the structure so that it forms a circle

    return(g)    
}
```

You don't have to understand every part of this function in order to use it. All you need to do is run the function above so that it is in your R environment. You can then use it.

It has two arguments - number of nodes and the size of the groups. You could change clique_size to 4 or 10.

```{r}
caveman_net <- simulate_caveman(n = 100, clique_size = 5) 
par(mar = c(2,2,2,2))
plot(caveman_net, layout = layout.kamada.kawai(caveman_net), vertex.size = 2, vertex.label = NA, vertex.color = "grey80")
```

Now you can clearly see what a caveman structure is. Let's analyze it.

```{r}
graph.density(caveman_net)
transitivity(caveman_net) # transitivity() measures clustering coefficient, which essentially says, how clustered is the network overall
average.path.length(caveman_net)
```

It has a pretty low density since most nodes only have connections within their clique or else one tie outwards. And as I mentioned above, caveman structures are extremely clustered, since most edges are within group.

Path length is also high - basically it takes 10 steps, on average, to reach one node from a random other node. In the real world, there are way more than 100 people and more than 20 groups, so it should be even more surprising that the average degree of separation is roughly six or seven steps. It follows that the "caveman structure" is not a small-world.

We can look at the diameter of the network to see this too. The diameter is the longest shortest path.

```{r}
nodes.diameter<-get.diameter(caveman_net)
V(caveman_net)[nodes.diameter]$color<-"darkgreen" # This assigns those nodes the color darkgreen.
plot(caveman_net, layout = layout.kamada.kawai(caveman_net), vertex.size = 2, vertex.label = NA)
```

Let's reset color.

```{r}
V(caveman_net)$color <- "orange"
```

Watts wants to get from this network structure, to one in which the average path length is much lower. He performs a simple exercise to do so (and one we have already experimented with). He randomly rewires the network so that it begins, slowly to approximate a random graph. Random graphs have low average path length; so this is a good idea.

We end up with a caveman structure with some number of rewired edges that will have the tendency to cut across the network

```{r}
caveman_net_rewired <- caveman_net %>% rewire(keeping_degseq(niter = 1000))
```

We can use the rewire function to rewire the network. keeping_degseq() ensures that the degree distribution does not change and niter = 20 is the number of iterations (rewirings).

```{r}
plot(caveman_net_rewired, layout = layout.kamada.kawai(caveman_net), vertex.size = 2, vertex.label=NA)
```

Most of the rewirings cut across the network structure!

```{r}
plot(caveman_net_rewired, layout = layout.kamada.kawai(caveman_net_rewired), vertex.size = 2, vertex.label=NA) # Here it is laid out properly
plot(caveman_net_rewired, layout = layout.kamada.kawai(caveman_net_rewired), vertex.size = 2, vertex.label = NA, vertex.color = "grey80")
```

Let's compare this to the caveman network.

```{r}
graph.density(caveman_net_rewired) 
transitivity(caveman_net_rewired) 
average.path.length(caveman_net_rewired)
```
Density is unchanged. Clustering coefficient is less than before, but still relatively high. And average.path.length was cut in half. Only 20 rewirings and look at the change!

We can analyze the change as we perform more rewirings. 

```{r}
avgpathlength <- average.path.length(caveman_net) # These are the first observation
clusteringcoefficient <- transitivity(caveman_net)
caveman_net_rewired <- caveman_net

iter = 100
for ( i in 2:iter){
  caveman_net_rewired <- caveman_net_rewired %>% rewire(keeping_degseq(niter = 1))
  avgpathlength <- c(avgpathlength, average.path.length(caveman_net_rewired)) # We are just appending the result to a vector
  clusteringcoefficient <- c(clusteringcoefficient, transitivity(caveman_net_rewired))
}

plot(1:100, avgpathlength, xlab = "Numer of Rewirings", ylab = "Average Path Length", main = "Caveman", type = "l")
plot(1:100, clusteringcoefficient, xlab = "Numer of Rewirings", ylab = "Clustering Coefficient", main = "Caveman", type = "l")
```

### Other simulations

Simulations are used a lot in networks. We only talk in class about random nets and small-worlds, but there are many ideal-type networks. The third most famous is the "scale-free" network. "scale-free networks" are also known as hub-based networks, because they have a few actors with high degree and many actors with low degree. They are therefore highly centralized, and generally have low clustering. 

igraph has a built in function to generate this class of networks: barabasi.game()

```{r}
scalefree <- barabasi.game(n=100, m=5)
plot(scalefree, vertex.size = 3, layout = layout.kamada.kawai(scalefree), edge.arrow.size = .1, vertex.label=NA)

graph.density(scalefree)
transitivity(scalefree) 
average.path.length(scalefree)
```

Density, when m = 5, is roughly the same as the small-world network. Clustering coefficient is much lower. And average path length is short, because people can access almost everyone else via the hubs in the network.

Look for other games in the igraph manual... or, you can write your own like we did above.